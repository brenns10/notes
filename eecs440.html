<!DOCTYPE html>
<html>
<head>
<!-- 2016-10-10 Mon 20:36 -->
<meta  charset="utf-8">
<meta  name="viewport" content="width=device-width, initial-scale=1">
<title>EECS 440 Notes</title>
<meta  name="generator" content="Org-mode">
<meta  name="author" content="Stephen Brennan">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="assets/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="assets/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="assets/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="assets/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">EECS 440 Notes</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgheadline6">1. 2015-12-01 Tuesday</a>
<ul>
<li><a href="#orgheadline3">1.1. Embedded Methods</a>
<ul>
<li><a href="#orgheadline1">1.1.1. Adapting Overfitting Controls</a></li>
<li><a href="#orgheadline2">1.1.2. Bayesian Approaches</a></li>
</ul>
</li>
<li><a href="#orgheadline5">1.2. Dimensionality Reduction</a>
<ul>
<li><a href="#orgheadline4">1.2.1. Principal Components Analysis</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline11">2. 2015-11-24 Tuesday</a>
<ul>
<li><a href="#orgheadline7">2.1. Kalman Filtering</a></li>
<li><a href="#orgheadline10">2.2. Feature Selection</a>
<ul>
<li><a href="#orgheadline8">2.2.1. Filter Methods</a></li>
<li><a href="#orgheadline9">2.2.2. Wrapper Methods</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline25">3. 2015-11-19 Thursday</a>
<ul>
<li><a href="#orgheadline20">3.1. Hidden Markov Models</a>
<ul>
<li><a href="#orgheadline12">3.1.1. Learning Model Parameters</a></li>
<li><a href="#orgheadline13">3.1.2. Simpler Problem</a></li>
<li><a href="#orgheadline16">3.1.3. Baum-Welch Algorithm</a></li>
<li><a href="#orgheadline17">3.1.4. Handling Labeled Sequences</a></li>
<li><a href="#orgheadline18">3.1.5. Discriminative HMM</a></li>
<li><a href="#orgheadline19">3.1.6. Stochastic CFGs</a></li>
</ul>
</li>
<li><a href="#orgheadline24">3.2. Discrete Time, Continuous State</a>
<ul>
<li><a href="#orgheadline21">3.2.1. Kalman Filtering</a></li>
<li><a href="#orgheadline22">3.2.2. Online Prediction</a></li>
<li><a href="#orgheadline23">3.2.3. Filtering</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline35">4. 2015-11-17 Tuesday</a>
<ul>
<li><a href="#orgheadline34">4.1. Sequential Supervised Learning</a>
<ul>
<li><a href="#orgheadline26">4.1.1. Sequential Data</a></li>
<li><a href="#orgheadline27">4.1.2. Generative Process Model</a></li>
<li><a href="#orgheadline28">4.1.3. The -1st Approach</a></li>
<li><a href="#orgheadline29">4.1.4. The 0th Approach</a></li>
<li><a href="#orgheadline33">4.1.5. The kth Order Approach</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline36">5. 2015-11-12 Thursday</a></li>
<li><a href="#orgheadline47">6. 2015-11-03 Tuesday</a>
<ul>
<li><a href="#orgheadline46">6.1. Learning with Prior Knowledge</a>
<ul>
<li><a href="#orgheadline37">6.1.1. Pros of Prior Knowledge</a></li>
<li><a href="#orgheadline38">6.1.2. Cons of Prior Knowledge</a></li>
<li><a href="#orgheadline39">6.1.3. Inductive and Analytical Learning</a></li>
<li><a href="#orgheadline40">6.1.4. Prior Knowledge and ML</a></li>
<li><a href="#orgheadline41">6.1.5. Learning Problem Setup</a></li>
<li><a href="#orgheadline42">6.1.6. Rule-Based Prior Knowledge</a></li>
<li><a href="#orgheadline43">6.1.7. KBANN Algorithm</a></li>
<li><a href="#orgheadline45">6.1.8. KBSVM Algorithm</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline53">7. 2015-10-29 Thursday</a>
<ul>
<li><a href="#orgheadline48">7.1. Weighting</a></li>
<li><a href="#orgheadline49">7.2. Why Boosting (Adaboost) Works</a></li>
<li><a href="#orgheadline50">7.3. Another Perspective of Adaboost</a></li>
<li><a href="#orgheadline51">7.4. Other Boosting</a></li>
<li><a href="#orgheadline52">7.5. Mixture of Experts</a></li>
</ul>
</li>
<li><a href="#orgheadline57">8. 2015-10-22 Thursday</a>
<ul>
<li><a href="#orgheadline54">8.1. Active Feature Labeling</a></li>
<li><a href="#orgheadline55">8.2. DUALIST: Interactive Classification</a></li>
<li><a href="#orgheadline56">8.3. Human Behavior</a></li>
</ul>
</li>
<li><a href="#orgheadline68">9. 2015-10-13 Tuesday</a>
<ul>
<li><a href="#orgheadline67">9.1. Part 2: Issues in Machine Learning</a>
<ul>
<li><a href="#orgheadline66">9.1.1. Handling Missing Attribute Values</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline72">10. 2015-10-08 Thursday</a>
<ul>
<li><a href="#orgheadline69">10.1. Why Does Naive Bayes Work Well?</a></li>
<li><a href="#orgheadline70">10.2. Tree Augmented Naive Bayes</a></li>
<li><a href="#orgheadline71">10.3. Logistic Regression</a></li>
</ul>
</li>
<li><a href="#orgheadline80">11. 2015-09-29 Tuesday</a>
<ul>
<li><a href="#orgheadline75">11.1. Support Vector Machines</a>
<ul>
<li><a href="#orgheadline73">11.1.1. Linear Discriminants</a></li>
<li><a href="#orgheadline74">11.1.2. Margins</a></li>
</ul>
</li>
<li><a href="#orgheadline76">11.2. Why it makes sense</a></li>
<li><a href="#orgheadline77">11.3. Calculating the Margin</a></li>
<li><a href="#orgheadline78">11.4. Problem Formulation</a></li>
<li><a href="#orgheadline79">11.5. Course Project Notes</a></li>
</ul>
</li>
<li><a href="#orgheadline88">12. 2015-09-24 Thursday</a>
<ul>
<li><a href="#orgheadline81">12.1. Issue 2.1</a></li>
<li><a href="#orgheadline82">12.2. Issue 2.2</a></li>
<li><a href="#orgheadline83">12.3. One-way ANOVA</a></li>
<li><a href="#orgheadline84">12.4. Sign Test</a></li>
<li><a href="#orgheadline85">12.5. Mann-Whitney-Wolcoxon Signed-Rank Test</a></li>
<li><a href="#orgheadline86">12.6. Bootstrap</a></li>
<li><a href="#orgheadline87">12.7. Is there a best learning algorithm?</a></li>
</ul>
</li>
<li><a href="#orgheadline99">13. 2015-09-22 Tuesday</a>
<ul>
<li><a href="#orgheadline94">13.1. ANNs, Continued</a>
<ul>
<li><a href="#orgheadline89">13.1.1. Cascade Correlation (Learning the Structure of an ANN)</a></li>
<li><a href="#orgheadline90">13.1.2. Interpretation of Hidden Units</a></li>
<li><a href="#orgheadline91">13.1.3. How Many Hidden Units?</a></li>
<li><a href="#orgheadline92">13.1.4. Recurrent ANNs</a></li>
<li><a href="#orgheadline93">13.1.5. Pros/Cons of ANNs</a></li>
</ul>
</li>
<li><a href="#orgheadline98">13.2. Comparing Learning Algorithms</a>
<ul>
<li><a href="#orgheadline95">13.2.1. Issue 1</a></li>
<li><a href="#orgheadline96">13.2.2. Issue 2</a></li>
<li><a href="#orgheadline97">13.2.3. It's All Just Statistics!</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline107">14. 2015-09-17 Thursday</a>
<ul>
<li><a href="#orgheadline100">14.1. Tradeoffs of Neural Networks</a></li>
<li><a href="#orgheadline103">14.2. Training ANN</a>
<ul>
<li><a href="#orgheadline101">14.2.1. Backpropagation</a></li>
<li><a href="#orgheadline102">14.2.2. Example</a></li>
</ul>
</li>
<li><a href="#orgheadline105">14.3. Overfitting in ANNs</a>
<ul>
<li><a href="#orgheadline104">14.3.1. Weight Decay</a></li>
</ul>
</li>
<li><a href="#orgheadline106">14.4. Implementation Issues</a></li>
</ul>
</li>
<li><a href="#orgheadline114">15. 2015-09-15 Tuesday</a>
<ul>
<li><a href="#orgheadline108">15.1. Famous Dead People</a></li>
<li><a href="#orgheadline109">15.2. History</a></li>
<li><a href="#orgheadline113">15.3. Perceptron / Linear Threshold Unit</a>
<ul>
<li><a href="#orgheadline110">15.3.1. Training a Perceptron</a></li>
<li><a href="#orgheadline111">15.3.2. More on Perceptrons</a></li>
<li><a href="#orgheadline112">15.3.3. Feedforward Network Topology</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline120">16. 2015-09-10 Thursday</a>
<ul>
<li><a href="#orgheadline119">16.1. Evaluation Methods and Metrics</a>
<ul>
<li><a href="#orgheadline115">16.1.1. n-fold Cross Validation</a></li>
<li><a href="#orgheadline116">16.1.2. Metrics for Classification</a></li>
<li><a href="#orgheadline117">16.1.3. Learning Curves</a></li>
<li><a href="#orgheadline118">16.1.4. Metrics with Confidence Measures</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline125">17. 2015-09-08 Tuesday</a>
<ul>
<li><a href="#orgheadline121">17.1. Review:</a></li>
<li><a href="#orgheadline122">17.2. Generalizing ID3</a></li>
<li><a href="#orgheadline124">17.3. Overfitting</a>
<ul>
<li><a href="#orgheadline123">17.3.1. Controlling Overfitting</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline141">18. 2015-09-01 Tuesday</a>
<ul>
<li><a href="#orgheadline135">18.1. What is "Machine Learning?"</a>
<ul>
<li><a href="#orgheadline126">18.1.1. Inductive Generalization</a></li>
<li><a href="#orgheadline134">18.1.2. Learning Settings</a></li>
</ul>
</li>
<li><a href="#orgheadline136">18.2. When to use ML?</a></li>
<li><a href="#orgheadline140">18.3. Example Representations</a>
<ul>
<li><a href="#orgheadline137">18.3.1. Feature Vector Representation</a></li>
<li><a href="#orgheadline138">18.3.2. Relational Representation</a></li>
<li><a href="#orgheadline139">18.3.3. Multiple Instance Representation</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgheadline158">19. 2015-08-27 Thursday</a>
<ul>
<li><a href="#orgheadline156">19.1. Optimization</a>
<ul>
<li><a href="#orgheadline142">19.1.1. What is it?</a></li>
<li><a href="#orgheadline143">19.1.2. Types of Optimization Problems</a></li>
<li><a href="#orgheadline144">19.1.3. Unconstrained Optimization</a></li>
<li><a href="#orgheadline145">19.1.4. Gradient Ascent</a></li>
<li><a href="#orgheadline146">19.1.5. Newton-Raphson Method</a></li>
<li><a href="#orgheadline147">19.1.6. Quasi-Newton Methods</a></li>
<li><a href="#orgheadline148">19.1.7. Local and Global Optima</a></li>
<li><a href="#orgheadline149">19.1.8. Convex Sets</a></li>
<li><a href="#orgheadline150">19.1.9. Convex Functions</a></li>
<li><a href="#orgheadline151">19.1.10. Constrained Optimization</a></li>
<li><a href="#orgheadline152">19.1.11. Linear Programming</a></li>
<li><a href="#orgheadline153">19.1.12. Simplex Algorithm</a></li>
<li><a href="#orgheadline154">19.1.13. Duality in Linear Programming</a></li>
<li><a href="#orgheadline155">19.1.14. Summary of Optimization</a></li>
</ul>
</li>
<li><a href="#orgheadline157">19.2. The Simplex Algorithm</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline6" class="outline-2">
<h2 id="orgheadline6"><span class="section-number-2">1</span> 2015-12-01 Tuesday</h2>
<div class="outline-text-2" id="text-1">
<p>
"Turkey part selection."  (aka feature selection)
</p>

<p>
We had filter methods and wrapper methods.
</p>
</div>

<div id="outline-container-orgheadline3" class="outline-3">
<h3 id="orgheadline3"><span class="section-number-3">1.1</span> Embedded Methods</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>Alter the objective functions of learning algorithms to simultaneously
select features and learn the classifier.</li>
<li>Best of both worlds, but requires altering classifier.</li>
</ul>

<p>
How do we modify objectives?
</p>
<ul class="org-ul">
<li>Typically objective functions aim to minimize some loss function.</li>
<li>We need to add a criteria to encourage "sparseness" - that is, make more
weights zero (zero weights mean you can just get rid of the feature).</li>
<li>Frequently you can modify the objective by adding a "zero-norm" term.  This
is the number of weights that are non-zero (which we would like to
minimize).
<ul class="org-ul">
<li>EG: \(L'(w, x, y) = L(w, x, y) + \lambda ||w||_0\)</li>
</ul></li>
<li>Unfortunately, this term is not differentiable.  However, it is reminiscent
of other overfitting controls (like weight decay).</li>
</ul>
</div>

<div id="outline-container-orgheadline1" class="outline-4">
<h4 id="orgheadline1"><span class="section-number-4">1.1.1</span> Adapting Overfitting Controls</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
Relationship between overfitting control (OC) and feature selection (FS):
</p>
<ul class="org-ul">
<li>These two concepts have very similar objectives!</li>
<li>A classifier that is robust to overfitting will probably use a smaller and
more concise (or logical) set of features.</li>
<li>A classifier that uses fewer features will likely be more generalizable.</li>
</ul>

<p>
What if we used overfitting control techniques for feature selection?
</p>
<ul class="org-ul">
<li>For example, what we had in the ANN: \(L'(w, x, y) = L(w, x, y) +
     \gamma ||w||^2\)</li>
<li>This is good, but it's possible to do better.
<ul class="org-ul">
<li>The square function "doesn't care" too much about relatively small
values.</li>
<li>It penalizes the very high ones, but it doesn't encourage completely
eliminating features.</li>
</ul></li>
<li>The \(||w||^2\) term is called the \(L_2\) norm.  Let's use a different
penalty, the \(L_1\) norm, (aka the Lasso).</li>
<li>The \(L_1\) norm is simply the sum of the absolute values of the components of
a vector.  It's not differentiable, but we can solve this using linear
programming.
<ul class="org-ul">
<li>Instead of having unconstrained optimization using the L<sub>1</sub> norm in the
objective, we can drop the L<sub>1</sub> norm into constraints.</li>
<li>The constraints would be of the form \(||w||_1 \le B\) (for L<sub>1</sub>) and
\(||w||_2 \le B\).  If you think of the geometry of these constraints,
they're "cubes" and "spheres" respectively.</li>
<li>Imagine you're doing optimization with these two objectives.  With the L<sub>1</sub>
norm, it's far more likely that an optimal point will lie on a vertex,
which has the special property that at least one of the feature weights
is zero.</li>
<li>On the other hand, the L<sub>2</sub> norm's "sphere" typically will find solutions
where all weights are nonzero simultaneously.</li>
<li>There are some seriously good figures in the slides to explain this.</li>
</ul></li>
<li>Tragically, the Lasso/L<sub>1</sub> is not differentiable (although it has some very
desirable qualities).  However, we can take the approach of doing linear
constraints and solving with quadratic programming.  It's a bit "awkward"
but it will work.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline2" class="outline-4">
<h4 id="orgheadline2"><span class="section-number-4">1.1.2</span> Bayesian Approaches</h4>
<div class="outline-text-4" id="text-1-1-2">
<p>
Bayesian models allow you to express prior knowledge through prior
distributions.  We can use a "feature selection" prior distribution, which
is normal with mean 0 and variances to be determined for each weight.  If we
can get very small variances, that "automatically" indicates that those
features are not very important.
</p>

<p>
The approach he is talking about is called a Relevance Vector Machine, which
is an algorithm where you update hyperparameters, re-estimate the parameters
of the model, and continue until convergences.
</p>

<p>
The explanation of this stuff is going over my head right now.  I think I
really need to do some reading on the whole Bayesian Linear Regression topic
so I understand the basics (I must have missed that lecture).
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline5" class="outline-3">
<h3 id="orgheadline5"><span class="section-number-3">1.2</span> Dimensionality Reduction</h3>
<div class="outline-text-3" id="text-1-2">
<p>
This is a problem related to feature selection, but different.  The idea is
that you want to take a set of features and reduce the number of them.
However, in dimensionality reduction, your new set of features is not
necessarily a subset of the original ones.  They can be some sort of
combination, etc.  Also, dimensionality reduction as a whole is not solely
concerned with classification.
</p>

<p>
Formal definition:
</p>
<ul class="org-ul">
<li>Given: a set of examples \((\vec{x}_i, y_i)\) over \(D\) features.</li>
<li>Do: find a <i>subspace</i> of dimension \(d\), \(d<D\) &#x2026; (see slides)</li>
</ul>
</div>

<div id="outline-container-orgheadline4" class="outline-4">
<h4 id="orgheadline4"><span class="section-number-4">1.2.1</span> Principal Components Analysis</h4>
<div class="outline-text-4" id="text-1-2-1">
<ul class="org-ul">
<li>Given: a set of points \(\vec{x}_i\) centered in \(R^D\).</li>
<li><p>
Find: an orthonormal set of \(d\) (\(<<D\)) vectors \(w_j \in R^D\), and the
associated coefficients \(z_i \in R^d\) so that the projected points \(p_i =
      W z_i\) minimize the <i>reconstruction error</i>:
</p>

\begin{align*}
  J(W, Z) &= \frac{1}{N} \sum_{i=1}^N ||x_i - p_i ||^2 \\
    &= ||X - WZ^T ||_F^2
\end{align*}

<p>
(the F is the "Frobenius norm" of the matrix - a fancy way of saying
"square all the elements and add them up).
</p></li>
</ul>

<p>
Solving PCA:
</p>
<ul class="org-ul">
<li>We can solve incrementally by asking for the first projection vector.
Turns out that the first projection vector has a closed form solution,
which is the eigenvector corresponding with the largest eigenvalue of the
covariance matrix of the examples.  The coefficients \(z\) follow.</li>
<li>Then you can do this again for the "residuals".  Turns out this also has a
closed form solution, which is eigenvector corresponding to the second
largest eigenvalue.</li>
<li>This pattern continues.  Nifty.</li>
</ul>

<p>
Problems with PCA:
</p>
<ul class="org-ul">
<li>PCA doesn't take labels into account.  So you might not find the "axis of
variation" that accounts for the difference between the labels.</li>
<li>Can get around this with modifications to the process.</li>
</ul>

<p>
How to pick the number of dimensions?
</p>
<ul class="org-ul">
<li>Plot the reconstruction error as a function of the number of dimensions.</li>
</ul>

<p>
Other general PCA notes:
</p>
<ul class="org-ul">
<li>PCA needs standardized inputs.</li>
<li>Ignores class labels.</li>
<li>The components are typically not "sparse" in terms of the input features.
So it's not easy to interpret the results or understand what features
ought to be dropped.</li>
<li>Although there is a closed form solution, the decomposition is expensive
(similar to random walk with restarts&#x2026;), so there are iterative
solutions that converge much faster.</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline11" class="outline-2">
<h2 id="orgheadline11"><span class="section-number-2">2</span> 2015-11-24 Tuesday</h2>
<div class="outline-text-2" id="text-2">
</div><div id="outline-container-orgheadline7" class="outline-3">
<h3 id="orgheadline7"><span class="section-number-3">2.1</span> Kalman Filtering</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>As discussed last time, we represent state as a vector of continuous
variables.  The observations are modeled as a gaussian of a linear function
of the states.  You also have a sensor distribution.</li>
<li>Filtering: estimate the state at time \(t\).</li>
<li>The nice aspect of Kalman Filter is that it's linear Gaussian all the way
down, so there are closed form solutions.</li>
<li>Parameter estimation can happen via iterative least squares.</li>
<li>There are many variations of Kalman Filters.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline10" class="outline-3">
<h3 id="orgheadline10"><span class="section-number-3">2.2</span> Feature Selection</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>Classification algorithms need feature vectors.  If important features
aren't present, they can't do anything.  Which means we usually take the
"kitchen sink" approach - put in <b>all</b> the features!</li>
<li>However, including all the features may be bad.  Sampling artifacts may
make irrelevant features <i>look</i> relevant, and these will produce
classifiers that are less generalizable.</li>
<li>Problem statement: Given a set of examples, find a subset \(S\) of features
(\(|S| \le D\)) so that, for any other subset \(S' \ne S\), the learned concept
that uses \(S\) <i>generalizes better</i> than the learned concept that uses \(S'\).</li>
<li>Problems with this problem:
<ul class="org-ul">
<li>Optimal solution is combinatorial.</li>
<li>chicken and egg - "to find relevant features, we need the target concept,
but to find the target concept, we need relevant features."</li>
</ul></li>
</ul>
</div>

<div id="outline-container-orgheadline8" class="outline-4">
<h4 id="orgheadline8"><span class="section-number-4">2.2.1</span> Filter Methods</h4>
<div class="outline-text-4" id="text-2-2-1">
<ul class="org-ul">
<li>"Let's pretend these problems don't exist!  Maybe they'll go away."</li>
<li>Filter Methods don't involve the learning algorithm, so they avoid the
chicken &amp; egg problem.</li>
<li>Just apply a procedure before training, then train on the filtered
features.</li>
<li>Typical procedure:
<ul class="org-ul">
<li>Score each feature according to some criterion.</li>
<li>Keep the \(k\) highest scoring features.</li>
</ul></li>
<li>Common criteria:
<ul class="org-ul">
<li>Mutual information / information gain</li>
<li>Probably correlation measures such as PCC</li>
</ul></li>
<li>Issues:
<ul class="org-ul">
<li>Sometimes features aren't informative alone but they are together.</li>
<li>Sometimes features are "proxies" for each other, so they'll provide
similar, duplicate information. (relevance, not redundancy).</li>
</ul></li>
<li>Solutions: Switch to a sequential process, where you start by picking a
feature and then add features w.r.t. their adjusted score (conditioned on
the existing features).</li>
<li>Downsides:
<ul class="org-ul">
<li>More "comparisons"</li>
<li>Highly dependent on the initial feature choice &#x2013; can go "bad" quickly.</li>
<li>Computing conditional mutual information (\(I(X;Y|S)\) is difficult as
the conditioned set of variables \(S\) grows.</li>
</ul></li>
<li><p>
A more practical criterion is:
</p>
\begin{equation}
  \text{Score}(X) = \sum_{X_j \in S} I(X,X_j; Y)
\end{equation}</li>
</ul>

<p>
Pros and Cons of Filter Methods:
</p>
<ul class="org-ul">
<li>Pros:
<ul class="org-ul">
<li>Generally pretty efficient (due to their simplicity).</li>
</ul></li>
<li>Cons:
<ul class="org-ul">
<li>May produce suboptimal results, due to heuristic scoring functions,
calibration, sample sizes, etc.</li>
<li>Ignores the algorithm that will actually do classification</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline9" class="outline-4">
<h4 id="orgheadline9"><span class="section-number-4">2.2.2</span> Wrapper Methods</h4>
<div class="outline-text-4" id="text-2-2-2">
<ul class="org-ul">
<li>These methods "search" the space of possible subsets, by repeatedly
applying the algorithm and using the performance as a guide.</li>
<li>Can't search exhaustively, so heuristics, here we go!</li>
<li>Forward Feature Selection:
<ul class="org-ul">
<li>Start with empty set.</li>
<li>Add features as your search operators</li>
<li>Use classifier's generalization error as the cost function.</li>
<li>Termination condition is when the error doesn't improve.</li>
</ul></li>
<li>Backward Feature Selection is the reverse - it starts with all the
features and works backward</li>
<li>Note on implementation: feature selection needs to be done only on the
training data, not on the whole set.  So, it is normal to do feature
selection in each fold and get different feature sets.</li>
<li>We can use boosting as a wrapper to select features.  Typically this is
with decision stumps as the base learner.  This way you get a "decision"
for which feature to add to your set at each iteration.
<ul class="org-ul">
<li>Note that at each iteration, you get a feature that is best at
classifying the examples that were most misclassified with the
previously selected features.</li>
<li>Each feature provides new and complementary information (like we tried
to do with conditional mutual information before).</li>
</ul></li>
</ul>

<p>
Pros and Cons of Wrapper Methods:
</p>
<ul class="org-ul">
<li>Pros:
<ul class="org-ul">
<li>Provide very good performance because they are paired with the learner.
They solve the chicken and egg problem iteratively.</li>
<li>Robust to missing data.</li>
</ul></li>
<li>Cons:
<ul class="org-ul">
<li>Need lots of data to work well (cross validation)</li>
<li>Very computationally intensive!!</li>
<li>Need to use fast learners.</li>
<li>Boosting is a good middle ground.</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline25" class="outline-2">
<h2 id="orgheadline25"><span class="section-number-2">3</span> 2015-11-19 Thursday</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-orgheadline20" class="outline-3">
<h3 id="orgheadline20"><span class="section-number-3">3.1</span> Hidden Markov Models</h3>
<div class="outline-text-3" id="text-3-1">
<p>
Last time:
</p>
<ul class="org-ul">
<li>Hidden Markov Models</li>
<li>Forward algorithm for determining probability of an observed sequence.</li>
<li>Viterbi Algorithm for finding most likely sequence of states that could
have emitted an observed sequence.</li>
</ul>

<p>
Now:
</p>
<ul class="org-ul">
<li>Given some observations and a model structure, how to estimate parameters?</li>
</ul>
</div>

<div id="outline-container-orgheadline12" class="outline-4">
<h4 id="orgheadline12"><span class="section-number-4">3.1.1</span> Learning Model Parameters</h4>
<div class="outline-text-4" id="text-3-1-1">
<ul class="org-ul">
<li>We have a set of observation sequences.  These sequences are i.i.d., but
the elements of the sequences (obviously) are not.</li>
<li>Parameters are:
<ul class="org-ul">
<li>Emission distribution for each state: \(Pr[a | S = k]\).</li>
<li>Transition probability for each pair of states.</li>
</ul></li>
<li>How do we do this?  Maximum Likelihood Estimation!</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline13" class="outline-4">
<h4 id="orgheadline13"><span class="section-number-4">3.1.2</span> Simpler Problem</h4>
<div class="outline-text-4" id="text-3-1-2">
<ul class="org-ul">
<li>This is gonna be difficult due to the fact that we don't know the state
sequences associated with the observed emissions.
<ul class="org-ul">
<li><i>That's why it's a <b>Hidden</b> Markov Model!</i></li>
</ul></li>
<li>What if, instead we had "annotated" examples?  That is, each observation
sequence comes with a list of states.</li>
<li>Then, we can simply count up the emissions for each state, and build a
probability distribution out of it.</li>
<li>We can do something similar with the transitions.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline16" class="outline-4">
<h4 id="orgheadline16"><span class="section-number-4">3.1.3</span> Baum-Welch Algorithm</h4>
<div class="outline-text-4" id="text-3-1-3">
<ul class="org-ul">
<li>Sadly, the world is not a nice place, and it doesn't typically give us a
state sequence.</li>
<li>This is <i>missing data</i>, and for generative probabilistic models, we can
handle this with <b>expectation maximization</b>!
<ul class="org-ul">
<li>As a refresher, in the E step, we estimate values of missing values</li>
<li>Then in the M step, we do MLE</li>
<li>Then we iterate this</li>
</ul></li>
<li>The algorithm here is Baum-Welch Algorithm.</li>
</ul>
</div>

<ol class="org-ol"><li><a id="orgheadline14"></a>E Step<br ><div class="outline-text-5" id="text-3-1-3-1">
<ul class="org-ul">
<li><p>
In order to estimate the probability of emitting \(b\) in state \(k\):
</p>

\begin{align*}
  E_{k;b} &= \sum_{j \in Sequences} \sum_{i: o_i^j=b} Pr(s_k = k | \vec{o}^j) \\
  &= \sum_{j \in sequences} \frac{1}{Pr(\vec{o}^j)} \sum_{i: a_i^j=b} Pr(s_i = k, \vec{o}^j) \\
  &= \sum_{j \in sequences} \sum_{i: a_i^j=b} \frac{\alpha_{k,j}(i) \beta_{k,j}(i)}{\alpha_{END,j}(n)}
\end{align*}

<p>
In the formula above, \(\alpha\) is defined from the forward algorithm, and \(\beta\)
comes from the backward algorithm, which estimates \(\beta_{k}(i) = Pr(s_i=k |
       \{o_i, \dots, o_n\}\) in a backwards dynamic programming fashion.  It turns out
that due to the Markov property, we can factor the probability into the
product of \(\alpha\) and \(\beta\), which is pretty darn neat.
</p></li>

<li><p>
In order to estimate the probability of a transition \(k\to l\): for each
training sequence, for each time step, find the probability \(k \to l\) was
used at that point; add up all the probabilities.
</p>

\begin{align*}
  Pr(s_i = k, s_{i+1} = l | \vec{o}) = \frac{\alpha_k(i)}{} \dots
\end{align*}

<p>
Slide change!  Check slides for formula!
</p></li>
</ul>
</div></li>

<li><a id="orgheadline15"></a>M Step<br ><div class="outline-text-5" id="text-3-1-3-2">
<ul class="org-ul">
<li>Once we have the \(E\) and \(C\) values, the maximum likelihood estimation is
simply then taking these expected counts and computing probabilities,
just like we did when we had labeled sequences.</li>
</ul>
</div></li></ol>
</div>

<div id="outline-container-orgheadline17" class="outline-4">
<h4 id="orgheadline17"><span class="section-number-4">3.1.4</span> Handling Labeled Sequences</h4>
<div class="outline-text-4" id="text-3-1-4">
<ul class="org-ul">
<li>What if we have labeled sequences?  We've dealt with "emissions", but what
if the sequence as a whole is labelled?</li>
<li>You just train a model for each class.  (Need to be careful to choose your
samples for each class).</li>
<li>Then, to classify, you just get the probability out of each model, and
choose the class with the higher probability.</li>
<li>Or, you could combine your structures into a single HMM, with a START
state, a POS set of states, a NEG set of states, and then a STOP state.
Then, train just that model.  To classify, just run the Viterbi algorithm
and inspect the most likely path to see class label.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline18" class="outline-4">
<h4 id="orgheadline18"><span class="section-number-4">3.1.5</span> Discriminative HMM</h4>
<div class="outline-text-4" id="text-3-1-5">
<ul class="org-ul">
<li>Can do a similar discriminative model, if the generative HMM is overkill.</li>
<li>Called Conditional Random Fields.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline19" class="outline-4">
<h4 id="orgheadline19"><span class="section-number-4">3.1.6</span> Stochastic CFGs</h4>
<div class="outline-text-4" id="text-3-1-6">
<ul class="org-ul">
<li>What if the order of dependence cannot be bounded?</li>
<li>You may need a stochastic context free grammar.
<ul class="org-ul">
<li>you can think of HMM's as stochastic regular grammars.</li>
</ul></li>
<li>Like regular/context free languages, HMMs and Stochastic CFGs have tons of
similarities.  Algorithms exist to do similar things in both.</li>
<li>In fact, CYK algorithm can be used like the Viterbi algorithm to return
the most likely "state" sequence, which is ridiculously cool, since we
originally covered in in 343 as just a normal parsing algorithm.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline24" class="outline-3">
<h3 id="orgheadline24"><span class="section-number-3">3.2</span> Discrete Time, Continuous State</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li>Not all processes have discrete state.</li>
<li>Examples of processes with continuous state:
<ul class="org-ul">
<li>Robot that is tracking its location as it moves.</li>
<li>System that is tracking objects on radar.</li>
</ul></li>
</ul>
</div>

<div id="outline-container-orgheadline21" class="outline-4">
<h4 id="orgheadline21"><span class="section-number-4">3.2.1</span> Kalman Filtering</h4>
<div class="outline-text-4" id="text-3-2-1">
<ul class="org-ul">
<li>Specify process state using continuous variables.</li>
<li>To model the evolution of the process state, need a distribution
\(Pr(s_{t+1} | s_t)\).</li>
<li>One possibility is to use a linear function of \(s_t\) along with some
Gaussian noise.</li>
</ul>

<p>
Example:
</p>
<ul class="org-ul">
<li>Motion tracking: state has position and velocity.
<ul class="org-ul">
<li>\(x(t + \Delta t) = x(t) + v_x(t)\Delta t\)</li>
<li>\(y(t + \Delta t) = y(t) + v_y(t)\Delta t\)</li>
</ul></li>
<li><p>
Then, you model the probability distribution \(X\) like this:
</p>
\begin{align*}
  Pr(X(t + \Delta t) = x(t + \Delta t) | x(t), v_x(t)) = N(x(t+\Delta t); \mu = x(t) + v_x(t)\Delta t, \sigma)
\end{align*}</li>
<li>Above, we're modeling the next state given observations of the current
state.</li>
<li>However, there's no guarantees on our sensors.</li>
<li>So, in general, we can also model the probability we observe \(\vec{o}\)
      given that we are in state \(\vec{s}\).</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline22" class="outline-4">
<h4 id="orgheadline22"><span class="section-number-4">3.2.2</span> Online Prediction</h4>
<div class="outline-text-4" id="text-3-2-2">
<p>
With continuous state problems, we frequently would like to do some
predictions "online":
</p>
<ul class="org-ul">
<li>Filtering: what is the current state given the observations so far?
\(Pr(s(t) | o(1), \dots, o(t))\)</li>
<li>Prediction: what is the next state I am likely to be in? \(Pr(s(t+k)|o(1),
      \dots, o(t))\)</li>
<li>Smoothing: How likely was I to have been in some state in the past, given
the observations so far? \(Pr(s(t-k)|o(1), \dots, o(t))\).</li>
<li>Most likely path: \(\arg \max_s Pr(s|o(1), \dots, o(t))\).</li>
</ul>

<p>
In this class, we'll only really talk about filtering.
</p>
</div>
</div>

<div id="outline-container-orgheadline23" class="outline-4">
<h4 id="orgheadline23"><span class="section-number-4">3.2.3</span> Filtering</h4>
<div class="outline-text-4" id="text-3-2-3">
<p>
Estimate state at time \(t\) as:
</p>

\begin{align*}
  Pr(s(t) | o(1), \dots, o(t))
  &= Pr(o(t) | s(t), o(1), \dots, o(t-1)) Pr(s(t) | o(1), \dots, o(t-1)) \text{    Bayes rule}\\
  &= Pr(o(t) | s(t)) Pr(s(t) | o(1), \dots, o(t-1)) \text{     Markov property} \\
  &= Pr(o(t) | s(t)) \int_{s(t-1)} Pr(s(t), s(t-1) | o(1), \dots, o(t-1)) ds(t-1)  \text{   total probability}\\
  &= Pr(o(t) | s(t)) \int_{s(t-1)} Pr(s(t) | s(t-1), o(1), \dots, o(t-1)) Pr(s(t-1) | o(1), \dots, o(t-1)) ds(t-1) \text{   definition of cond. prob}\\
  &= Pr(o(t) | s(t)) \int_{s(t-1)} Pr(s(t) | s(t-1)) Pr(s(t-1) | o(1), \dots, o(t-1)) ds(t-1)\text{   Markov property}\\
\end{align*}

<p>
Woah.
</p>

<p>
So, the neat thing here is that the decomposition we just did got us to a
point where we had an expression containing:
</p>
<ul class="org-ul">
<li>The sensor distribution</li>
<li>The transition distribution</li>
<li>The output of filtering at the previous state</li>
</ul>

<p>
This allows everything to happen online.  Plus, if you use a linear
gaussian, then it turns out that every filtering step is also linear
gaussian.  It's linear gaussians all the way down.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline35" class="outline-2">
<h2 id="orgheadline35"><span class="section-number-2">4</span> 2015-11-17 Tuesday</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-orgheadline34" class="outline-3">
<h3 id="orgheadline34"><span class="section-number-3">4.1</span> Sequential Supervised Learning</h3>
<div class="outline-text-3" id="text-4-1">
<ul class="org-ul">
<li>So far we've looked at data we could represent as feature vectors.</li>
<li>Now we'd like to go further and look at examples that are sequences of
observations.</li>
<li>Examples:
<ul class="org-ul">
<li>NLP: Text classification, parsing, part-of-speach tagging</li>
<li>Protein family modeling</li>
<li>Speech recognition</li>
<li>Activity recognition</li>
</ul></li>
</ul>
</div>

<div id="outline-container-orgheadline26" class="outline-4">
<h4 id="orgheadline26"><span class="section-number-4">4.1.1</span> Sequential Data</h4>
<div class="outline-text-4" id="text-4-1-1">
<ul class="org-ul">
<li>Sequence of observations o<sub>1</sub>, o<sub>2</sub>, &#x2026;, o<sub>n</sub>.</li>
<li>Each observation is drawn from a background alphabet/vocabulary</li>
<li>Given this data, we have many types of prediction problems:
<ul class="org-ul">
<li>Classify the whole sequence</li>
<li>Classify each element</li>
<li>Something in between&#x2026;</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline27" class="outline-4">
<h4 id="orgheadline27"><span class="section-number-4">4.1.2</span> Generative Process Model</h4>
<div class="outline-text-4" id="text-4-1-2">
<ul class="org-ul">
<li>We hypothesize that there is an underlying generating process we can
model.</li>
<li>The process has a state that could be discrete or continuous.</li>
<li>The state evolves over time.</li>
<li>At discrete time points, we observe something about the state.
<ul class="org-ul">
<li>could be continuous time points, but less common</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline28" class="outline-4">
<h4 id="orgheadline28"><span class="section-number-4">4.1.3</span> The -1st Approach</h4>
<div class="outline-text-4" id="text-4-1-3">
<ul class="org-ul">
<li>Just ignore that the data is sequential, and try to use a normal learning
algorithm.
<ul class="org-ul">
<li>Sometimes this is a reasonable first choice.</li>
</ul></li>
<li>This approach has met with some success, especially with Naive Bayes
models of text classification.
<ul class="org-ul">
<li>Spam/ham, sport/finance</li>
<li>Use a "bag-of-words" approach - it doesn't take into account word order.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline29" class="outline-4">
<h4 id="orgheadline29"><span class="section-number-4">4.1.4</span> The 0th Approach</h4>
<div class="outline-text-4" id="text-4-1-4">
<ul class="org-ul">
<li>We'll accept that there is an underlying process, but not process the
state.</li>
<li>We'll take into account the position of each feature/item in the model.</li>
<li>Position Specific Scoring Matrix</li>
<li>Need to have a constant length sequence.</li>
<li>Doesn't account for dependence on previous inputs</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline33" class="outline-4">
<h4 id="orgheadline33"><span class="section-number-4">4.1.5</span> The kth Order Approach</h4>
<div class="outline-text-4" id="text-4-1-5">
<ul class="org-ul">
<li>Model dependence of order \(k\), with a \(k\) order hidden markov model.</li>
<li>This will take into account the \(k\) previous examples in the "decision"</li>
<li>Essentially, a HMM is a probabilistic FSM that emits output
probabilistically in each state.</li>
<li>It's a generative model!</li>
<li><p>
It works by factoring the probability distribution of \(Pr[\{s_1, s_2, \dots,
      s_n\}, \{o_1, o_2, \dots, o_n\}]\).  This is a <b>time series</b>.  A first order HMM
factors the distribution into:
</p>

\begin{align*}
  Pr[\{s_1, s_2, \dots, s_n\}, \{o_1, o_2, \dots, o_n\}] = Pr(s_1)Pr(o_1 |s_1) \prod_{r=2}^n Pr(o_r |
  s_r) Pr(s_r | s_{r-1})
\end{align*}

<p>
In the product, the first probability (\(Pr(o_r | s_r)\))is the "emission
probability" (how likely is state \(s_r\) to emit observation \(o_r\)).  The
second probability (\(Pr(s_r | s_{r-1})\))is the "transition probability" (how
likely is state \(s_{r-1}\) to transition into state \(s_r\)).
</p></li>
<li>Note that with higher order models, you condition on more than just the
previous state in the "transition probability".</li>
<li>Aside:
<ul class="org-ul">
<li>What is "markov" about this?  Only considers the previous \(k\) inputs.</li>
<li>What is "hidden"?  The states!</li>
</ul></li>
<li>Inference:
<ul class="org-ul">
<li>What is the probability of a sequence?</li>
<li>What is the most likely sequence of states to produce an observations?</li>
</ul></li>
<li>Learning:
<ul class="org-ul">
<li>How to estimate parameters? (given a model structure)</li>
<li>How to learn a structure?</li>
</ul></li>
</ul>
</div>

<ol class="org-ol"><li><a id="orgheadline30"></a>Inference in HMMs<br ><div class="outline-text-5" id="text-4-1-5-1">
<ul class="org-ul">
<li>How to estimate \(Pr(\vec{o})\)?</li>
<li>Clearly, \(Pr(\vec{o}) = \sum_{\vec{s}} Pr(\vec{o},\vec{s})\) (which we already
have above.
<ul class="org-ul">
<li>Problem: the number of potential state sequences that could generate
\(\vec{o}\) is pretty huge in general.  Exponential, actually.</li>
</ul></li>
<li>Key observation that saves us: Many \(Pr(o, s)\)  functions share
"prefixes", which we can store using dynamic programming.</li>
<li>Forward algorithm:
<ul class="org-ul">
<li>Let \(\alpha_k(i) = Pr(o_1, \dots, o_i, s_i=k)\)
<ul class="org-ul">
<li>This is the probability that the model has emitted the first \(i\)
           observations and is now in state \(k\).</li>
</ul></li>
<li>We would like to compute \(\alpha_{END}(n)\) (this assumes that we have a dummy
end state for convenience).  This is the probability of the entire
observation.</li>
<li>Construct a table of size \(n\) by \(m\), (\(n\) is length of observed
sequence, \(m\) is number of states).</li>
<li>The forward algorithm is a dynamic programming procedure that will fill
the table with &alpha; values.</li>
<li>Initialize: \(\alpha_{START}(0) = 1\), and \(\alpha_k(0) = 0\) for \(k \ne START\).</li>
<li>Recursion: \(\alpha_k(i) = Pr(o_i | s_i = k) \sum_p \alpha_p(i-1) Pr(s_i=k | s_{i-1} = p)\)</li>
</ul></li>
</ul>
</div></li>

<li><a id="orgheadline31"></a>Most Likely Path<br ><div class="outline-text-5" id="text-4-1-5-2">
<p>
Now we are trying to answer the question: which state sequence is most
likely to have generated the observations?  In terms of math, we're looking
for:
</p>

\begin{equation}
  \vec{s}^* = \arg \max_{\vec{s}} Pr(\vec{s}|\vec{o})
\end{equation}

<ul class="org-ul">
<li>The dumb way to solve it is to enumerate all possible state sequences.</li>
<li>The smart way is to use dynamic programming again, using an algorithm
called the Viterbi algorithm:</li>
</ul>

<p>
<b>Viterbi Algorithm</b>
</p>

<ul class="org-ul">
<li>Let \(\gamma_k(i) = Pr(o_1, \dots, o_i, s_i^* = k)\)
<ul class="org-ul">
<li>The probability that the most likely path is at state \(k\) after
emitting the first \(i\) observation.</li>
</ul></li>
<li>As frequently happens with DP, this table only gets us the
probabilities.  We'll need to maintain a separate table full of pointers
if we'd like to get the actual state sequence.  But that's details.</li>
<li>Initialize: \(\gamma_{START}(0) = 1\), and for the other \(k\), \(\gamma_k(0)=0\).</li>
<li>Recursion: \(\gamma_{k}(i) = Pr(o_i | s_i^* = k) \max_p \gamma_p(i-1) Pr(s_i^* = k |
       s_{i-1}^* = p)\)
<ul class="org-ul">
<li>Critical difference here from the forward algorithm is that we want to
choose the most likely path to have gotten us here, so we use a max
instead of a sum.</li>
</ul></li>
</ul>
</div></li>

<li><a id="orgheadline32"></a>Parameter Estimation<br ><div class="outline-text-5" id="text-4-1-5-3">
<p>
We ain't gonna do that this class.
</p>
</div></li></ol>
</div>
</div>
</div>
<div id="outline-container-orgheadline36" class="outline-2">
<h2 id="orgheadline36"><span class="section-number-2">5</span> 2015-11-12 Thursday</h2>
<div class="outline-text-2" id="text-5">
<p>
Examples of PAC learnable functions:
</p>
<ul class="org-ul">
<li>Boolean conjunctions over \(n\) attributes.</li>
<li>$k$-DNF formulas over \(n\) attributes, for fixed \(k\).</li>
<li>Arbitrary Boolean functions.  (PAC learnable, but not efficiently due to the
fact that it's doubly exponential)</li>
</ul>

<p>
Assumptions:
</p>
<ul class="org-ul">
<li>Target concept is in the hypothesis class.</li>
<li>Hypothesis class is finite.</li>
<li>No noise</li>
<li>Consistency</li>
</ul>

<p>
Relaxing the assumptions:
</p>
<ul class="org-ul">
<li>Consistency is tied to the hypothesis class containing the hypothesis class.
<ul class="org-ul">
<li>Instead, we should bound the difference between the output hypothesis and
the <i>best</i> hypothesis.</li>
<li>Let \(E_i = 1\) if output hypothesis makes a mistake on example \(i\), 0
otherwise.</li>
<li>\(E_i\) is a Bernoulli random variable with parameter \(err(h,f)\).</li>
<li>Our estimate of \(err(h,f)\) is \(err_S(h,f) = \frac{1}{m} \sum_i E_i\)</li>
<li>Chernoff Bound: if a rv can be expressed as a sum of independent indicator
rv's, you can use this to bound the deviation of the sum from the true
value: \(Pr(|err(h,f)-err_S(h,f)| > \epsilon) \le 2e^{-2m\epsilon^2}\)</li>
<li>When you plug a lot of things in, you get \(m \ge \frac{1}{2\epsilon^2} \log
      \left(\frac{2|H|}{\delta}\right)\), which ensures that the probability of all
hypotheses being good is greater than \(\delta\).</li>
</ul></li>
</ul>

<p>
ERM Learners
</p>
<ul class="org-ul">
<li>Empirical risk minimization algorithms output a hypothesis with the least
training error.
<ul class="org-ul">
<li>\(h_{best} = \arg \min_h (err(h,f))\)</li>
<li>\(H_{ERM} = \arg \min_h (err_S(h,f))\)</li>
</ul></li>
<li><p>
Observe:
</p>
\begin{align*}
  err(h_{ERM}) &\le err_S(h_{ERM}) + \epsilon \text{  by choice of }m\\
           &\le err_S(h_{best}) + \epsilon \text{  by ERM}\\
           &\le err(h_{best}) + 2\epsilon \text{  by choice of }m
\end{align*}</li>
<li>So the error of \(H_{ERM}\) is at most \(2\epsilon\).</li>
<li>Importance here is that even when the target concept isn't guaranteed to be
in the hypothesis class, we can still guarantee that we can learn something
suitably close to best concept.</li>
</ul>

<p>
Infinite Hypothesis Classes
</p>
<ul class="org-ul">
<li>Many hypothesis classes aren't finite.  For instance, there are infinite
choices for a line in R<sup>2</sup></li>
<li>But we have least squares regression, and in general we don't expect that we
need more than \(O(n)\) examples to fit such a model with \(n\) parameters.</li>
</ul>

<p>
Let's look at an example: Learning axis parallel rectangles in R<sup>2</sup>.  The
algorithm is to pick the rectangle with the tightest fit to the positives.
</p>
<ul class="org-ul">
<li>This strategy will always produce a rectangle that is within the target
rectangle.</li>
<li>To show this is pac learnable, we want to show that the "strip" of the
target rectangle to the learned rectangle contains a probability mass of
\(\epsilon/4\).</li>
<li><p>
We call this region of the rectangle \(T\).  The only way we have too much
strip on that edge is if none of our examples come from \(T\).
</p>
\begin{align*}
  Pr(err(R', R) > \epsilon) &= Pr(\text{mass under strips exceeds }\epsilon) \\
&\le 4 Pr(\text{mass under 1 strip exceeds }\epsilon/4) \\
&\le 4 Pr(\text{no point in }m\text{ samples lies in }T)\\
&\le 4\left(1-\frac{\epsilon}{4}\right)^m
\end{align*}
<p>
There was more to the proof, but I couldn't write it all down.  See slides
</p></li>
</ul>

<p>
This worked because there was a nice geometric shape that allowed some cool
tricks.  What about the general case of infinite hypotheses?  We can use a
clever trick called "projecting the function onto the data."  We know that
there are an infinite number of possible hypotheses, but we also know that on
a finite training sample, the behaviors of the infinite set of hypotheses is
finite.  We can define equivalence classes based on the performance on the
training set.
</p>

<p>
Growth Function
</p>
<ul class="org-ul">
<li>For a sample \(S=x_1, \dots, x_m\), let \(F(S) = \{[h(x_1), \dots, h(x_m)],
    \text{ for all }h\in H\}\)</li>
<li>Growth function: \(G(m) = \max_S |F(S)|\).</li>
<li>If \(G(m) = 2^m\), the classifier can memorize any examples you give it.  We
say that it shatters the examples.  Or shatters something.  I'm not sure
what it shatters.</li>
<li>The size of the largest set that \(H\) can shatter is the Vapnik-Chervonenkis
(VC) dimension of \(H\).</li>
<li>If \(VC(H) = d\), then there is some set of size \(d\) that can be shattered,
and</li>
<li>No set of size \(d+1\) can be shattered.</li>
<li>If we can find sets of arbitrary size that can be shattered, \(VC(H) =
    \infty\).</li>
</ul>

<p>
Examples:
</p>
<ul class="org-ul">
<li>Suppose \(H\) is the class of all intervals on the real line.  What is its VC
dimension?
<ul class="org-ul">
<li>1? we can shatter that!</li>
<li>2? we can shatter that too!</li>
<li>3? nope, no shattering that (say we had +,-,+).</li>
<li>So, the VC dimension is 2</li>
</ul></li>
<li>How about linear functions in 2D?
<ul class="org-ul">
<li>VC dimension is 3.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline47" class="outline-2">
<h2 id="orgheadline47"><span class="section-number-2">6</span> 2015-11-03 Tuesday</h2>
<div class="outline-text-2" id="text-6">
</div><div id="outline-container-orgheadline46" class="outline-3">
<h3 id="orgheadline46"><span class="section-number-3">6.1</span> Learning with Prior Knowledge</h3>
<div class="outline-text-3" id="text-6-1">
<ul class="org-ul">
<li>Humans don't just learn by examples and inductive reasoning.  If we did, we
would have a really hard time!</li>
<li>Much of how/what we learn is due to prior knowledge.</li>
<li>How to integrate that into machine learning algorithms?</li>
</ul>
</div>

<div id="outline-container-orgheadline37" class="outline-4">
<h4 id="orgheadline37"><span class="section-number-4">6.1.1</span> Pros of Prior Knowledge</h4>
<div class="outline-text-4" id="text-6-1-1">
<ul class="org-ul">
<li>Can provide accurate inductive bias.</li>
<li>Can provide good starting points for optimization.</li>
<li>Can reduce the number of examples we need to see to find a good concept.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline38" class="outline-4">
<h4 id="orgheadline38"><span class="section-number-4">6.1.2</span> Cons of Prior Knowledge</h4>
<div class="outline-text-4" id="text-6-1-2">
<ul class="org-ul">
<li>Frequently prior knowledge is <i>approximate</i>.  If we provide wrong prior
knowledge, it may be worse than no prior knowledge.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline39" class="outline-4">
<h4 id="orgheadline39"><span class="section-number-4">6.1.3</span> Inductive and Analytical Learning</h4>
<div class="outline-text-4" id="text-6-1-3">
<ul class="org-ul">
<li>Inductive Learning (what we've been doing so far): given training data,
find the hypothesis from the hypothesis space that best generalizes data.
<ul class="org-ul">
<li>focuses on generalizing examples to derive a theory</li>
</ul></li>
<li><b>Analytical Learning:</b> Given a (logical) "domain theory", find the
hypothesis <i>deduced from theory</i> that best <i>explains</i> data
<ul class="org-ul">
<li>focuses on "specializing" your domain theory to explain data</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline40" class="outline-4">
<h4 id="orgheadline40"><span class="section-number-4">6.1.4</span> Prior Knowledge and ML</h4>
<div class="outline-text-4" id="text-6-1-4">
<ul class="org-ul">
<li>An active research area.</li>
<li>Two main directions:
<ul class="org-ul">
<li>Algorithm-specific approaches for non-probabilistic algorithms.</li>
<li>Bayesian priors for probabilistic algorithms!</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline41" class="outline-4">
<h4 id="orgheadline41"><span class="section-number-4">6.1.5</span> Learning Problem Setup</h4>
<div class="outline-text-4" id="text-6-1-5">
<ul class="org-ul">
<li>Given:
<ul class="org-ul">
<li>A set of training examples and</li>
<li>A <i>domain</i> theory about the learning problem.</li>
</ul></li>
<li>Output a hypothesis that best fits both.</li>
<li>Goal: both training examples and prior knowledge are imperfect.
<ul class="org-ul">
<li>We must try to learn a concept that will do better than concepts
produced by either of them in isolation.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline42" class="outline-4">
<h4 id="orgheadline42"><span class="section-number-4">6.1.6</span> Rule-Based Prior Knowledge</h4>
<div class="outline-text-4" id="text-6-1-6">
<ul class="org-ul">
<li>Often, prior knowledge we have is expressed as rules.
<ul class="org-ul">
<li>EG: "I think any furry scary thing with long teeth is a lion."</li>
</ul></li>
<li>Can express this as a logical proposition.
<ul class="org-ul">
<li>EG: Has-fur &and; Long-Teeth &and; Scary &rarr; Lion</li>
</ul></li>
<li>Here, we only consider propositional theories, but we can also have
first-order domain theories.</li>
</ul>

<p>
Terminology for propositional logic:
</p>
<ul class="org-ul">
<li>Literal: proposition or negation</li>
<li>Clause: disjunction of literals</li>
<li>Horn clause: a clause with at most one unnegated literal
<ul class="org-ul">
<li>This is special because you can take a horn clause and write it as a
logical implication: \(x_1 \lor \lnot x_2 \lor \lnot x_3\) becomes \(x_2
        \land x_3 \to x_1 \)</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline43" class="outline-4">
<h4 id="orgheadline43"><span class="section-number-4">6.1.7</span> KBANN Algorithm</h4>
<div class="outline-text-4" id="text-6-1-7">
<ul class="org-ul">
<li>First, create a network (with sigmoid functions) that perfectly fits the
domain theory.
<ul class="org-ul">
<li>Suppose we have a Horn Clause \(x_2 \land x_3 \to C\):
<ul class="org-ul">
<li>We can create a network consisting of a single perceptron.  We give a
weight of 1 on x<sub>1</sub> and x<sub>2</sub>, and we set our activation threshold just
below two.</li>
<li>In theory, we could set the weights to be the whatever we'd like, and
we could scale the threshold accordingly.  The actual value of the
weights doesn't matter for creating a "perfect fit".</li>
<li>The weights do matter for the later backprop step.  If you have large
weights, the effect of the examples on the network will be minimal.
So you can think of the overall weight assignment as a confidence
measurement for the rule.  The more confident you are in a rule, the
larger the weights, and the less the training examples can change this
aspect of the learned concept.</li>
</ul></li>
<li>Suppose we have a more complex domain theory: \(x_2 \land x_3 \to C\),
\(x_1 \land x_2 \to C\).  Any of these clauses evaluating to true means
the example is labeled with C.
<ul class="org-ul">
<li>We can create a network with a hidden layer.</li>
<li>The hidden layer contains a perceptron for each clause as above.</li>
<li>The output layer takes the output of the two clauses and acts like an
"or gate".  In terms of its weights, this means that the activation
threshold is around \(0.5W\), where \(W\) is the weight of its inputs.
So, it activates when any of its inputs are activated.</li>
</ul></li>
<li>Suppose we have a domain theory which has variables that aren't
observable.  EG: \(x_1 \land h \to C\), \(x_3 \land x_4 \to h\)
<ul class="org-ul">
<li>In this case, we need to have more layers.  The variable \(h\) is
"defined" as the output of a perceptron in an earlier layer, and then
used in the later layers as we've discussed.</li>
<li>This is equivalent to "substituting" the definition for the variable,
but it is likely to simplify the task of the learner.</li>
</ul></li>
</ul></li>
<li>As an intermediate step, KBANN will take the structure derived above, and
it will add in <b>every</b> missing edge from the output of a previous layer to
the input of the next, with 0 weight.  This is important because our
domain theory is assumed to be imperfect, and we would like to give our
learner the flexibility to modify the concept beyond what the domain
structure initially said.
<ul class="org-ul">
<li>An interesting question is whether 0 weights give you a chance to encode
your confidence in the fact that a given input has no impact on that
proposition.  I think it's the difference between 0 and \(W\), but you
could ask KBANN explicitly not to include edges if you really don't
think they should be there.  This is just heuristic.</li>
</ul></li>
<li>Then, use backpropagation to refine this network to fit the training
examples.
<ul class="org-ul">
<li>If your prior knowledge was completely consistent with examples,
backpropagation will not change anything.  This is because the loss
function will be 0, so any backpropagation updates will be multiplied
by 0.</li>
<li>If not, this will be a good initial point to fit your data.  The weights
chosen previously will affect how "quickly" the input data will make the
network converge to the correct concept.</li>
</ul></li>
</ul>

<p>
Refining Prior Knowledge:
</p>
<ul class="org-ul">
<li>You can take the final trained network, and go back and figure out which
parts of the prior knowledge were wrong.</li>
<li>Unfortunately, you can't always interpret a network structure and weights
as a meaningful horn clause anymore.
<ul class="org-ul">
<li>There are algorithms to approximately do this (eg TREPAN).</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline45" class="outline-4">
<h4 id="orgheadline45"><span class="section-number-4">6.1.8</span> KBSVM Algorithm</h4>
<div class="outline-text-4" id="text-6-1-8">
<p>
This algorithm incorporates prior knowledge into (you guessed it) SVMs.
(Fung, Mangasarian, Shavlik 2002)
</p>

<ul class="org-ul">
<li>How do we think of prior knowledge in terms of SVMs?  Well, we could think
of them as "regions" of the input space that you know belong to one class.</li>
<li>We can define "polyhedrons", regions bounded by hyperplanes.  (Not
necessarily completely bounded).</li>
</ul>
</div>

<ol class="org-ol"><li><a id="orgheadline44"></a>Knowledge Sets<br ><div class="outline-text-5" id="text-6-1-8-1">
<p>
Each region is a knowledge set defined by \(C \cdot \vec{x} \le c\), and it
defines an implication: \(C \cdot \vec{x} \le c \to \vec{w} \cdot \vec{x} + b \ge 1\).
</p>

<p>
These can be added as constraints into the QP.
</p>
</div></li></ol>
</div>
</div>
</div>

<div id="outline-container-orgheadline53" class="outline-2">
<h2 id="orgheadline53"><span class="section-number-2">7</span> 2015-10-29 Thursday</h2>
<div class="outline-text-2" id="text-7">
</div><div id="outline-container-orgheadline48" class="outline-3">
<h3 id="orgheadline48"><span class="section-number-3">7.1</span> Weighting</h3>
<div class="outline-text-3" id="text-7-1">
<p>
Earlier we were talking about boosting, which is a way to "focus attention"
on examples that a classifier misclassifies.  In order to do this, we need te
weight each example as we train a classifier.
</p>

<p>
For pretty much any classifier, you can incorporate a weight parameter into
its formulation.  Especially for ANNs and Logistic Regression, where you then
simply take the gradient of the loss function, and the weights are baked into
the parameter update.
</p>
</div>
</div>

<div id="outline-container-orgheadline49" class="outline-3">
<h3 id="orgheadline49"><span class="section-number-3">7.2</span> Why Boosting (Adaboost) Works</h3>
<div class="outline-text-3" id="text-7-2">
<ul class="org-ul">
<li>People aren't really sure - this is an active area of research.</li>
<li>One idea: when you do boosting, you shift the decision surface to emphasize
separating certain examples.  Then, if you put multiple boosted classifiers
into an ensemble, they will create a much more complex decision surface
capable of separating all the examples.</li>
<li>Another idea: Adaboost could instead be viewed as a margin maximization
algorithm.  This means that as you continue to boost classifiers, you get
more and more generalizable ones.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline50" class="outline-3">
<h3 id="orgheadline50"><span class="section-number-3">7.3</span> Another Perspective of Adaboost</h3>
<div class="outline-text-3" id="text-7-3">
<p>
You can also look at Adaboost as a process that minimizes the exponential
loss of an ensemble classifier.
</p>

<p>
If you replace the exponential loss function with another one (say, logistic
loss), you actually get logistic regression, where each learner in the
ensemble will represent a particular feature.
</p>
</div>
</div>

<div id="outline-container-orgheadline51" class="outline-3">
<h3 id="orgheadline51"><span class="section-number-3">7.4</span> Other Boosting</h3>
<div class="outline-text-3" id="text-7-4">
<p>
Gradient boosting.  Minimizes squared loss of the gradient (not of the
classifier).  This is like cascade correlation in ANNs.
</p>
</div>
</div>

<div id="outline-container-orgheadline52" class="outline-3">
<h3 id="orgheadline52"><span class="section-number-3">7.5</span> Mixture of Experts</h3>
<div class="outline-text-3" id="text-7-5">
<ul class="org-ul">
<li>Look at each classifier as an expert in a certain area of the input.</li>
<li>Do weighted votes proportional to how likely the example is to belong to
each expert's responsible area.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline57" class="outline-2">
<h2 id="orgheadline57"><span class="section-number-2">8</span> 2015-10-22 Thursday</h2>
<div class="outline-text-2" id="text-8">
<p>
Today, Burr Settles from Duolingo will be talking about some of the stuff he
did during his PhD.  We won't be tested on this stuff, but it'll still be
interesting.
</p>

<ul class="org-ul">
<li>Text annotation example (for abstracts).</li>
<li>Wants to extract information from citations? (seems like something that
could be parsed)</li>
<li>Active learning can help reduce the size of the needed training set size.
<ul class="org-ul">
<li>Interestingly, it doesn't really reduce annotation time.</li>
<li>Probably because the examples the algorithm "asks for help" on are the
hardest to annotate.</li>
</ul></li>
<li>Interactive learning:
<ul class="org-ul">
<li>Human gives "advice" (some rules or generalizations to start from).</li>
<li>ML attempts to "self-train" on unlabeled data (semi-supervised learning)</li>
<li>ML asks for help occasionally</li>
</ul></li>
</ul>
</div>

<div id="outline-container-orgheadline54" class="outline-3">
<h3 id="orgheadline54"><span class="section-number-3">8.1</span> Active Feature Labeling</h3>
<div class="outline-text-3" id="text-8-1">
<p>
Conditional Random Fields (CRFs).  They take sequential information and learn
from them.  (Apply logistic regression to markov processes)
</p>

<p>
Each word is something like a feature.  Humans can provide rules to label
features.  The algorithm attempts to minimize the Kullback-Liebler Divergence
between the feature label distribution and the expected label distribution
(from the examples).
</p>

<p>
The system chooses features it would like labeled (active learning).  Instead
of choosing the ones that maximize some exact information theoretic quantity
that encodes the amount of information it holds, they chose to approximate
this with the "uncertainty" (i.e. entropy), multiplied by the log of the
frequency (so that we choose the common, uncertain features).
</p>
</div>
</div>

<div id="outline-container-orgheadline55" class="outline-3">
<h3 id="orgheadline55"><span class="section-number-3">8.2</span> DUALIST: Interactive Classification</h3>
<div class="outline-text-3" id="text-8-2">
<p>
Downsides of the previous algorithm were that it wasn't truly interactive.
It was slow, and the user couldn't "volunteer" information and rules they
just came up with.  DUALIST (which is pretty neat) did text classification
and allowed the user to add words to each label, and also presented words it
wanted to know that the user could label as well.  It retrained really
quickly, which made it pretty nice and interactive.
</p>

<p>
<a href="https://github.com/burrsettles/dualist">https://github.com/burrsettles/dualist</a>
</p>

<p>
Results fairly positive, but a lot of failure cases were due to human error.
</p>
</div>
</div>

<div id="outline-container-orgheadline56" class="outline-3">
<h3 id="orgheadline56"><span class="section-number-3">8.3</span> Human Behavior</h3>
<div class="outline-text-3" id="text-8-3">
<p>
Some of the problems with DUALIST were due to human errors.  They used Amazon
Mechanical Turk to get larger (n=30) samples for their examples.  They got
results that were not positive.  However, it turned out that volunteering
feature labels to the model is actually the most important part of getting a
good model.  So it takes some good interface design to encourage good input
to the model.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline68" class="outline-2">
<h2 id="orgheadline68"><span class="section-number-2">9</span> 2015-10-13 Tuesday</h2>
<div class="outline-text-2" id="text-9">
</div><div id="outline-container-orgheadline67" class="outline-3">
<h3 id="orgheadline67"><span class="section-number-3">9.1</span> Part 2: Issues in Machine Learning</h3>
<div class="outline-text-3" id="text-9-1">
<p>
We're entering the second part of the course!  We've covered algorithms,
evaluation, and comparison.  Now we cover extensions and enhancements, and
implementation issues.
</p>
</div>

<div id="outline-container-orgheadline66" class="outline-4">
<h4 id="orgheadline66"><span class="section-number-4">9.1.1</span> Handling Missing Attribute Values</h4>
<div class="outline-text-4" id="text-9-1-1">
<ul class="org-ul">
<li>Frequently you don't have complete data due to cost, noise, or just
inconsistencies on behalf of the person doing the measuring.</li>
<li>How do we handle it?</li>
</ul>

<p>
Strategies:
</p>

<ul class="org-ul">
<li>Preprocess to remove missing  values.</li>
<li>Algorithm specific strategies.</li>
<li>A general strategy for generative probabilistic models (expectation
maximization)</li>
</ul>
</div>

<ol class="org-ol"><li><a id="orgheadline61"></a>Preprocessing<br ><div class="outline-text-5" id="text-9-1-1-1">
<ul class="org-ul">
<li>Easiest strategy: remove all examples with missing examples.
<ul class="org-ul">
<li>Sometimes this is the sensible thing to do - especially if only a few
examples are missing data.</li>
</ul></li>
<li>Next easiest: replace with a new value that indicates "unknown"
<ul class="org-ul">
<li>This can cause unforeseen consequences!  (gene expression leukemia
story)</li>
</ul></li>
<li>Third strategy: fill in the missing values somehow, based on the observed
data.
<ul class="org-ul">
<li>(this is called imputation)</li>
<li>It's a learning algorithm within a learning algorithm.</li>
<li>Machine Learning-ception</li>
</ul></li>
</ul>
</div>

<ol class="org-ol"><li><a id="orgheadline58"></a>Mean Imputation<br ><div class="outline-text-6" id="text-9-1-1-1-1">
<ul class="org-ul">
<li>Suppose the \(j\)th feature is missing from the \(i\)th example is
missing.</li>
<li><p>
Substitute with the average value for the \(j\)th feature taken over all
examples with the same label for which \(j\) is observed.
</p>

\begin{equation}
  \hat{x}_{ij} = \frac{\sum_{\{k|x_{kj}\text{ is observed}\}} x_{kj} I(y_k = y_i)}{\sum_{\{k|x_{kj}\text{ is observed}\}} I(y_k=y_i)}
\end{equation}

<p>
This is useful for training.  What about prediction?
</p></li>
<li>Suppose the \(j\)th feature for new example, \(x\), is missing.</li>
<li><p>
Substitute with the average value of the \(j\)th feature taken over all
examples for which \(x_j\) is observed.
</p>

\begin{equation}
  \hat{x}_j = \frac{\sum_{\{k|x_{kj}\text{ is observed}\}} x_{kj}}{\text{# where }x_{jk}\text{ is observed}}
\end{equation}</li>
</ul>
</div></li>

<li><a id="orgheadline59"></a>Model-based Imputation<br ><div class="outline-text-6" id="text-9-1-1-1-2">
<ul class="org-ul">
<li>Suppose you have example \(i\), which is missing some attributes, in
particular \(j\).</li>
<li>Let \(O\) be the set of observed attributes for \(i\).</li>
<li>Find the set of examples \(E\) which (i) have the same label as \(j\), (ii)
have at least both \(O\) and \(x_j\) observed.</li>
<li>Learn a model to predict \(x_j\) using \(O\), trained on \(E\).</li>
<li>Use this model to impute \(x_{ij}\).</li>
<li><b>Very costly:</b> You must do this for every single cell that is missing!</li>
</ul>
</div></li>

<li><a id="orgheadline60"></a>Imputation Recap<br ><div class="outline-text-6" id="text-9-1-1-1-3">
<ul class="org-ul">
<li>If you need to do some sort of imputation, Dr. Ray recommends you start
with mean-based imputation.  It works "surprisingly well".</li>
</ul>
</div></li></ol></li>

<li><a id="orgheadline65"></a>Algorithm-Dependent Strategies<br ><div class="outline-text-5" id="text-9-1-1-2">
<p>
The above preprocessing solutions ignore which classification algorithm you
are going to use with the data.
</p>
<ul class="org-ul">
<li>"Generally not a good idea"</li>
</ul>

<p>
Some algorithms have tailored approaches to deal with missing values.
</p>
</div>

<ol class="org-ol"><li><a id="orgheadline62"></a>Decision Trees<br ><div class="outline-text-6" id="text-9-1-1-2-1">
<p>
<b>Prediction:</b>
</p>

<ul class="org-ul">
<li>When training, maintain fractions of training examples that went down
each branch, at every node.</li>
<li>When you encounter a node that asks you to test an attribute that your
example doesn't have, send it down "both" (or all) paths.
<ul class="org-ul">
<li>You'll get back a prediction with a weight.</li>
<li>Add up all the fractions for each label, and pick the label with the
biggest fraction.</li>
</ul></li>
<li>If you have lots of incomplete data, then your classification time is
like \(b^k\), where \(b\) is your branching factor and \(k\) is your number of
missing attributes.  So it's pretty inefficient.</li>
</ul>

<p>
<b>Learning:</b>
</p>
<ul class="org-ul">
<li>Key question: how to compute \(IG(X)\) if there are examples with \(X\)
        missing?</li>
<li>Idea: when partitioning the data to get \(H(Y|X)\), send "fractions" of
these examples to all buckets for \(X\).</li>
<li>For the \(i\)th bucket, the fraction is equal to the proportion of the
\(i\)th value of \(X\) in <i>other</i> examples at this node, where \(X\) is
observed.</li>
<li>On the slides, he has a table with an example, that really helps the
understanding.  I'd check it out if this is confusing.</li>
<li>When you go down a node, you need to remember what proportion of each
example is "present" in this node.</li>
<li>His slides example:
<ul class="org-ul">
<li>You have an example that is missing "color".  \(\frac{3}{8}\) of the
other examples have color=red, so when you split a node on color, you
say that there are 3 and \(\frac{3}{8}\) examples where color=red.</li>
<li>When you go down to the next node, and that examples is present, you
need to remember that there is only \(\frac{3}{8}\) of that example
present.</li>
<li>If that example is missing another attribute shape, and
\(\frac{1}{3}\) of the other examples in that node have shape=circle,
then you compute that \(1 + (1/3) * (3/8) = 1 \frac{1}{8}\).</li>
</ul></li>
</ul>
</div></li>

<li><a id="orgheadline63"></a>Neural Networks<br ><div class="outline-text-6" id="text-9-1-1-2-2">
<p>
People typically just use preprocessing, because modifying ANN's is too
difficult.
</p>
</div></li>

<li><a id="orgheadline64"></a>Generative Models<br ><div class="outline-text-6" id="text-9-1-1-2-3">
<p>
Expectation maximization.
</p>

<ul class="org-ul">
<li><p>
For a missing feature in an example, create a random variable \(S_i\).
</p>

\begin{align*}
  \hat{\theta} &= \arg \max_{\theta} \sum_{i} \ln p(\vec{x}_i, y_i; \theta) \\
  &= \arg \max_{\theta} \sum_{i \in NoMissing} \ln p(\vec{x_i}, y_i; \theta) + \sum_{i \in Missing} \ln \sum_{s_i} p(\vec{x}_i, s_i, y_i; \theta) \\
  &= MLE_{\text{NoMissing}} + EM
\end{align*}</li>
</ul>

<p>
Steps associated with EM:
</p>

<ul class="org-ul">
<li>Initialize the parameters of the probabilistic model randomly.</li>
<li>EM is a two stage optimization process:
<ul class="org-ul">
<li>E(xpectation): estimate the values of the hidden variables given the
data and the current model.</li>
<li>M(aximization): find the MLE estimates of the parameters, given the
now complete data.</li>
</ul></li>
<li>Iterate until convergence.
<ul class="org-ul">
<li>Fortunately, this procedure will not cycle, and it will reach an
optimal value.  But, there's no guarantee that it will be a global
optimum.</li>
</ul></li>
</ul>
</div></li></ol></li></ol>
</div>
</div>
</div>

<div id="outline-container-orgheadline72" class="outline-2">
<h2 id="orgheadline72"><span class="section-number-2">10</span> 2015-10-08 Thursday</h2>
<div class="outline-text-2" id="text-10">
<p>
Naive Bayes Classifier
</p>

<p>
The naive bayes classifier implements a linear decision boundary.
</p>
</div>

<div id="outline-container-orgheadline69" class="outline-3">
<h3 id="orgheadline69"><span class="section-number-3">10.1</span> Why Does Naive Bayes Work Well?</h3>
<div class="outline-text-3" id="text-10-1">
<ul class="org-ul">
<li>Theoretically, it should not.</li>
<li>The independence assumptions made by naive bayes are nearly always wrong.</li>
<li>Strangely, it still works well.</li>
<li>Turns out, it works quite well for <b>classification</b>, but the probability
estimates are usually bad.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline70" class="outline-3">
<h3 id="orgheadline70"><span class="section-number-3">10.2</span> Tree Augmented Naive Bayes</h3>
<div class="outline-text-3" id="text-10-2">
<p>
Augments Naive Bayes so that there is a tree structure over the attributes,
which is unknown.
</p>

<ul class="org-ul">
<li>Instead of each \(X_i\) node having \(p(X_i |Y)\), you have \(p(X_i | X_j and
     Y)\).</li>
<li>Makes fewer independence assumptions -&gt; better performance.</li>
<li>Has optimal algorithm to learn the structure:</li>
</ul>

<p>
<b>ALGORITM</b>
</p>

<ul class="org-ul">
<li>Create a complete graph over the attributes.
<ul class="org-ul">
<li>Edges are weighted by \(I(U, V| Y)\) (conditional mutual information).</li>
</ul></li>
<li>Find the maximal weighted spanning tree of this graph.
<ul class="org-ul">
<li>Just negate edge weights and apply Prim's Algorithm.</li>
</ul></li>
<li>Set the parameters with maximum likelihood estimation.</li>
</ul>

<p>
It turns out that this is the tree structure that maximizes the likelihood of
the data.  (ask for paper)
</p>
</div>
</div>

<div id="outline-container-orgheadline71" class="outline-3">
<h3 id="orgheadline71"><span class="section-number-3">10.3</span> Logistic Regression</h3>
<div class="outline-text-3" id="text-10-3">
<p>
The simplest discriminative model.  Models "log odds" as a linear function:
</p>

\begin{equation}
\log \frac{p(Y = 1 | \vec{x})}{p(Y = -1| \vec{x})} = \vec{w} \cdot \vec{x} + b
\end{equation}

<p>
If you manimulate this equation (in slides) you get the following:
</p>

\begin{equation}
  p(Y=1|\vec{x}) = \frac{1}{1 + e^{-(\vec{w} \cdot \vec{x} + b)}}
\end{equation}

<p>
To do classification, you simply compute the above probability and classify
as positive if it's greater than 0.5, otherwise negative.
</p>

<p>
To do learning, you can use maximum likelihood estimation.  In the previous
example, we used the likelihood of the data.  In this example, we maximize
the conditional likelihood of the data.
</p>

\begin{align*}
  \vec{w}, b &= \arg \max \Pi_i p(Y_i = p_i | \vec{x}_i) \\
  &= \arg \max \sum_{i\in pos} \log p(Y_i = 1 | \vec{x}_i) \sum_{i\in neg} \log p(Y_i = -1 | \vec{x}_i)
\end{align*}

<p>
You can also add a term to control for overfitting.  Typically
\(\frac{1}{2} ||\vec{w}||^2\).  Then you have to switch the signs of the rest
of the function and do a minimization instead.  You can do this min/max
problem using gradient descent or many other optimization methods.  It's very
robust, works well in many practical settings, and is easy to code.
</p>

<p>
Geometry - classify as positive iff \(\vec{w} \cdot \vec{x} + b > 0\).  So it's
linear again.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline80" class="outline-2">
<h2 id="orgheadline80"><span class="section-number-2">11</span> 2015-09-29 Tuesday</h2>
<div class="outline-text-2" id="text-11">
</div><div id="outline-container-orgheadline75" class="outline-3">
<h3 id="orgheadline75"><span class="section-number-3">11.1</span> Support Vector Machines</h3>
<div class="outline-text-3" id="text-11-1">
<ul class="org-ul">
<li>SVM's are a rather new method in machine learning.</li>
<li>Produced by multiple groups of people in ML, Statistics, and Operations
Research who basically converged on this idea.</li>
<li>Three fundamental ideas:
<ul class="org-ul">
<li>Linear discriminants (we saw this with a perceptron)</li>
<li>Margins</li>
<li>Kernels</li>
</ul></li>
</ul>
</div>

<div id="outline-container-orgheadline73" class="outline-4">
<h4 id="orgheadline73"><span class="section-number-4">11.1.1</span> Linear Discriminants</h4>
<div class="outline-text-4" id="text-11-1-1">
<p>
What is?
</p>
<ul class="org-ul">
<li>\(sign(5x_1 + 3x_2 - 4)\)</li>
<li>\(sign(x_2 - 4x_1^2)\)</li>
<li>\(sign(x_2 - e^{-x^2})\)</li>
</ul>

<p>
This was a trick question.  They all are.  In machine learning, we aren't
particularly interested in the \(x\) variables &#x2013; they are given to us.  We are
interested in the coefficients \(w\).  So when we talk about linear
discriminants, we mean linear in terms of \(w\), not \(x\).
</p>
<ul class="org-ul">
<li>In general, we talk about a linear discriminant in this form:</li>
<li>\(\vec{w} \cdot \phi(\vec{x}) + b = 0\)</li>
</ul>

<p>
When we talked about perceptrons not being able to discriminate XOR, it was
not entirely true.  If you make a transformation \(\phi\) of the variables, you
can define a line that discriminates XOR perfectly.  This transformation is
\(\phi(x_1, x_2) = x_1 + x_1 x_2 \).  You can see the diagram in the slides.
</p>
</div>
</div>

<div id="outline-container-orgheadline74" class="outline-4">
<h4 id="orgheadline74"><span class="section-number-4">11.1.2</span> Margins</h4>
<div class="outline-text-4" id="text-11-1-2">
<p>
Given a training sample, you can frequently define tons of linear
discriminants that cleanly separate the training sample.
</p>

<p>
What is the best one?  Probably the one that is mostly in the middle of the
training positives and negatives.
</p>

<p>
We define <b>margins</b> as the amount you could slide the discriminant in until
you reach a point.  With this, we can say that we would define our best SVM
classifier as the one with the <i>maximum margin</i>.
</p>

<p>
When you are in the "<i>input feature space</i>", (i.e. \(\phi(\vec{x})=\vec{x}\)),
this is called a "linear SVM" (which is a bit confusing, but this means
linear in the feature space, as opposed to the coefficient space &#x2013; in
essence, it's a linear, linear SVM).
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline76" class="outline-3">
<h3 id="orgheadline76"><span class="section-number-3">11.2</span> Why it makes sense</h3>
<div class="outline-text-3" id="text-11-2">
<ul class="org-ul">
<li>Computationally easy to come up with.</li>
<li>Requires relatively few data points.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline77" class="outline-3">
<h3 id="orgheadline77"><span class="section-number-3">11.3</span> Calculating the Margin</h3>
<div class="outline-text-3" id="text-11-3">
<p>
If you have a classifier \(\vec{w}\cdot\vec{x} + b = 0\), and your first positive
datapoint is at \(\vec{w}\cdot\vec{x} + b = 1\), and your first negative point is
\(\vec{w}\cdot\vec{x} + b = -1\).  Cool.
</p>

<p>
Note that \(\vec{w}\) is perpendicular to \(\vec{w}\cdot\vec{x} + b = 0\),
</p>
<ul class="org-ul">
<li>Why? Pick any \(\vec{u}\), \(\vec{v}\).</li>
<li>\(\vec{w}\cdot(\vec{u}-\vec{v}) = \vec{w}\cdot\vec{u} - \vec{w}\cdot\vec{v} = (-b) -
     (-b) = 0\)</li>
<li>Also, \(\vec{w}\) is perpendicular to the plus and minus planes.</li>
</ul>

<p>
There's some stuff here that I didn't follow.  Essentially, he got to the
point where maximizing the margin is equivalent to maximizing
\(\frac{2}{||\vec{w}||}\), which is equivalent to \(\frac{||\vec{w}||^2}{2}\).
</p>
</div>
</div>

<div id="outline-container-orgheadline78" class="outline-3">
<h3 id="orgheadline78"><span class="section-number-3">11.4</span> Problem Formulation</h3>
<div class="outline-text-3" id="text-11-4">
<p>
What we want to do:
</p>
<ul class="org-ul">
<li>While respecting the labels of training examples:
<ul class="org-ul">
<li>\(\vec{w}\cdot\vec{x_i} + b \ge 1 \text{ if } y_i = 1\)</li>
<li>\(\vec{w}\cdot\vec{x_i} + b \le -1 \text{ if } y_i = -1\)</li>
</ul></li>
<li>Or, more compactly:
<ul class="org-ul">
<li>\(y_i(\vec{w}\cdot\vec{x_i} + b) \ge 1\)</li>
</ul></li>
</ul>

<p>
Problem formulation:
</p>
<ul class="org-ul">
<li>\(\min \frac{1}{2} ||\vec{w}||^2\), s.t.</li>
<li>\(y_i(\vec{w}\cdot\vec{x}_i + b) \ge 1\)</li>
</ul>

<p>
Whoa, it's quadratic programming!  There are algorithms to do this, and since
the constraints are linear, the feasible region is convex, and therefore we
have a global minimum!
</p>

<p>
Unfortunately, if the data are not linearly separable, then our quadratic
programming algorithm will come back saying that the problem is not feasible.
So, we need to allow for misclassification by adding slack variables:
</p>

<ul class="org-ul">
<li>\(\min \frac{1}{2} ||\vec{w}||^2\), s.t.</li>
<li>\(y_i(\vec{w}\cdot\vec{x}_i + b) + \xi_i \ge 1\)</li>
<li>\(\xi_i \ge 0\)</li>
</ul>

<p>
Sadly, this doesn't solve the problem.  A trivial solution that always
exists: \(\vec{w}=\vec{0}\), and \(\xi_i=0\)!  We need to add into the objective
function a term to simultaneously minimize the number of errors in the
classifier.  Unfortunately, the number of errors is not a differentiable
quantity, but \(\sum_i \xi_i\) is a good proxy and is differentiable.  Hooray!
</p>

<ul class="org-ul">
<li>\(\min \frac{1}{2} ||\vec{w}||^2 + C\sum_i \xi_i \), s.t.</li>
<li>\(y_i(\vec{w}\cdot\vec{x}_i + b) + \xi_i \ge 1\)</li>
<li>\(\xi_i \ge 0\)</li>
</ul>

<p>
In this program, \(C\) is a constant that quantifies the "tradeoff" between the
generalization and the error.  The value tends to be problem and dataset
specific, so you need to determine it via cross validation in your code.
</p>

<p>
Notice that in an optimal solution \(\xi_i = \max(0,1-y_i(\vec{w}\cdot\vec{x}_i +
   b))\).  We can lift this into the objective function to remove the
constraints:
</p>

\begin{equation}
  \min \frac{1}{2} ||\vec{w}||^2 + C\sum_i [(1 - y_i(\vec{w}\cdot\vec{x}_i + b))]^2
\end{equation}

<p>
We don't need to use quadratic programming now, we can just solve an
unconstrained problem, which is much simpler and more efficient.  Yay us.
</p>
</div>
</div>

<div id="outline-container-orgheadline79" class="outline-3">
<h3 id="orgheadline79"><span class="section-number-3">11.5</span> Course Project Notes</h3>
<div class="outline-text-3" id="text-11-5">
<p>
Sign up sheet: "I want to do project", "I have an idea for a project."
</p>

<p>
Email with idea for project by 10/9.  Official project start date 10/23.
</p>

<p>
Significant work - treat like a real research project.  Start with a
hypothesis of what you'd like to show.  You may be algorithm-specific, or
application specific:
</p>
<ul class="org-ul">
<li>I have a better version of algorithm X.</li>
<li>My algorithm X is better for application Y.</li>
</ul>

<p>
Preferably, you should integrate the project with your own research.  You may
also be able to integrate the project with other course projects as well.
</p>

<p>
In terms of grading, the project is 35% of your grade:
</p>
<ul class="org-ul">
<li>25% is a writeup, in a conference format, of your problem, experiments,
observations, and interesting directions.</li>
<li>10% is a presentation to the class on what you did, during finals week.</li>
</ul>

<p>
Website will have example projects, and a document with requirements for the
writeup document.
</p>

<p>
<b>The writeup is due Dec 10!  <i>No extensions!!</i></b>
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline88" class="outline-2">
<h2 id="orgheadline88"><span class="section-number-2">12</span> 2015-09-24 Thursday</h2>
<div class="outline-text-2" id="text-12">
<p>
Recall: we want to compare ML algorithms.  Our steps for comparing test data:
</p>
<ol class="org-ol">
<li>Determine the sampling distribution.</li>
<li>Estimate parameters using MLE.</li>
<li>Use an approximation distribution if necessary.</li>
<li>Come up with a C% confidence interval.</li>
</ol>

<p>
Our "Issue #1" was "what do we know about the true performance of a
classification algorithm, given that we've tested it on a test set?"
</p>
<ul class="org-ul">
<li>We determined that we could come up with a confidence interval for an
observed test statistic.</li>
</ul>
</div>

<div id="outline-container-orgheadline81" class="outline-3">
<h3 id="orgheadline81"><span class="section-number-3">12.1</span> Issue 2.1</h3>
<div class="outline-text-3" id="text-12-1">
<p>
Our next issue, #2.1, is that we have a conjecture "classifier A is better
than algorithm B on a certain type of data."  We would like to evaluate
whether this conjecture is true.  We can do this with statistical hypothesis
testing.
</p>

<p>
We want to look at the random variable \(F = Err_{C_1} - Err_{C_2}\).  What can we
say about the sampling distribution of \(F\)?  Assuming that the error
distributions are Gaussian, the distribution of \(F\) is going to be Gaussian
also.  The MLE parameter estimates:
</p>
<ul class="org-ul">
<li>\(E(F) = e_{S,C_1} - e_{S,C_2} = \left(\frac{r_1}{n_1} - \frac{r_2}{n_2}\right)\)</li>
<li>\(V(F) = V(Err_{C_1}) + V(Err_{C_2}) = \frac{e_{S,C_1}(1-e_{S,C_1})}{n_1} + \frac{e_{S,C_2}(1-e_{S,C_2})}{n_2}\)</li>
</ul>

<p>
Comparing classifiers: hypothesis testing:
</p>
<ul class="org-ul">
<li>Establish your "null hypothesis."
<ul class="org-ul">
<li>You will reject this hypothesis with high probability.</li>
<li>You presume it is true until the test shows otherwise.</li>
</ul></li>
</ul>

<p>
This test assumes that the two classifiers were evaluated on independent test
data.
</p>

<p>
Example:
</p>
<ul class="org-ul">
<li>On a test set with (*0 examples a decision tree misclassifies 20 examples.
on the same test set, a neural network misclassifies 25 examples. Are these
two classifiers actually different on this problem?</li>
<li>\(F=\frac{r_1}{n_1} - \frac{r_2}{n_2} = 0.05\)</li>
<li>\(V(F) = 0.2(1-0.2)/100 + 0.25 (1-0.25) / 100 = 0.0016 + 0.001875 = 0.0034\)</li>
<li>Standard deviation is about 0.05.</li>
<li>0 is definitely within the 95% confidence interval, so we cannot reject the
null hypothesis (that they are the same)</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline82" class="outline-3">
<h3 id="orgheadline82"><span class="section-number-3">12.2</span> Issue 2.2</h3>
<div class="outline-text-3" id="text-12-2">
<p>
This issue is critically different from 2.1.  In 2.1, we had two classifiers
&#x2013; who knows where they came from, but we want to compare those two
classifiers' performances.  In this case, we have two learning algorithms,
and we want to compare the performance of the <b>algorithms</b>, not a particular
<b>classifier</b> produced by an algorithm.
</p>

<p>
In order to do so, we must find the expected value of an algorithm's error
rate.  To do this, we must take the average over all classifiers, produced by
all possible training sets.  We usually estimate this by doing $n$-fold
validation instead of actually finding all possible training sets from the
population.
</p>

<p>
We can do <i>pair testing</i>, where we evaluate the algorithms on the same folds,
and then compare the difference between their error rates.  Or, we can do it
independently, on separate folds, and compare their error rates.  But this
method gives you a bigger variance.
</p>

<p>
When you compare the difference of error rates, you want to know what the
sampling distribution is.  The sampling distribution looks Gaussian, but not
quite.  Instead, it's a \(t\) distribution.  If \(k\) (the number of folds) was
very large, we could use the Gaussian, but instead we have to use \(t\)
distribution, with \(k-1\) degrees of freedom.  Here are parameter estimates:
</p>
<ul class="org-ul">
<li>Mean (&delta;): the average difference of error rates across \(k\) folds.</li>
<li><p>
Standard Deviation:
</p>
\begin{equation}
  s = \sqrt{\frac{\sum_{i=1}^k (\delta_i - \delta)^2}{k(k-1)}}
\end{equation}</li>
<li>The standard deviation is adjusted to make the distribution narrower, and
put more mass in the tails!</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline83" class="outline-3">
<h3 id="orgheadline83"><span class="section-number-3">12.3</span> One-way ANOVA</h3>
<div class="outline-text-3" id="text-12-3">
<ul class="org-ul">
<li>For comparing &gt;2 algorithms.</li>
<li>Why not just do pairwise hypothesis tests?
<ul class="org-ul">
<li>The results may not be consistent (i.e. transitive)</li>
<li>Multiple hypotheses result in lost confidence, so you'd need to correct
your P-value/confidence interval.
<ul class="org-ul">
<li>EG: with 10 95% CI's, you only have 60% confidence that the true values
of all 10 parameters are within the range.</li>
</ul></li>
</ul></li>
<li>ANOVA looks at the "between means" variance.</li>
<li>Essentially, it seems like a generalized \(t\) test (gives the same result as
a \(t\) test for two distributions).</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline84" class="outline-3">
<h3 id="orgheadline84"><span class="section-number-3">12.4</span> Sign Test</h3>
<div class="outline-text-3" id="text-12-4">
<ul class="org-ul">
<li>Simpler than \(t\) test with fewer assumptions.</li>
<li>For each fold, note which algorithm had better performance.</li>
<li>Use binomial null hypothesis, where p=0.5.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline85" class="outline-3">
<h3 id="orgheadline85"><span class="section-number-3">12.5</span> Mann-Whitney-Wolcoxon Signed-Rank Test</h3>
<div class="outline-text-3" id="text-12-5">
<ul class="org-ul">
<li>What if a classifier produces confidence estimates?</li>
<li><p>
If we can rank the predictions, we can calculate a \(U\) statistic based on
the ranks:
</p>
\begin{equation}
  U_1 = \sum_i R_{1,i} - \frac{n_1(n_1 + 1)}{2}
\end{equation}</li>
<li>For large enough samples, you can approximate \(U\) with a normal
distribution.</li>
<li>AUC is actually a normalized version of \(U\)!</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline86" class="outline-3">
<h3 id="orgheadline86"><span class="section-number-3">12.6</span> Bootstrap</h3>
<div class="outline-text-3" id="text-12-6">
<ul class="org-ul">
<li>All previous methods relied on knowing the sampling distribution of the
statistic we are interested in.</li>
<li>The bootstrap is a procedure where we get the properties of the statistic
using <i>empirical resampling</i> from the observations.</li>
</ul>

<p>
Example:
</p>
<ul class="org-ul">
<li>Suppose we have a set of iid examples and we want to get a C1 for F1 score.</li>
<li>Repeatedly draw an equal sized sample (with replacement) from our test
examples, and measure F1.</li>
<li>This creates an empirical sampling dsitribution.</li>
<li>Then, go back to the original data, measure F1, and ask how unusual that is
in the empirical distribution.</li>
</ul>

<p>
Weird&#x2026; &ensp;()_/
</p>

<p>
Pros: very easy, few assumptions, good for complex things.
</p>

<p>
Cons: finite sample behavior is not very well understood.
</p>
</div>
</div>

<div id="outline-container-orgheadline87" class="outline-3">
<h3 id="orgheadline87"><span class="section-number-3">12.7</span> Is there a best learning algorithm?</h3>
<div class="outline-text-3" id="text-12-7">
<ul class="org-ul">
<li>No</li>
<li>No Free Lunch theorem!
<ul class="org-ul">
<li>In the expectation over all learning algorithms, they will perform
equally.</li>
<li>Wolpert 1996: "The lack of a priori distinctions between learning
algorithms."</li>
<li>For any specific application, you can have a "best" algorithm.  But
overall, no.</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline99" class="outline-2">
<h2 id="orgheadline99"><span class="section-number-2">13</span> 2015-09-22 Tuesday</h2>
<div class="outline-text-2" id="text-13">
</div><div id="outline-container-orgheadline94" class="outline-3">
<h3 id="orgheadline94"><span class="section-number-3">13.1</span> ANNs, Continued</h3>
<div class="outline-text-3" id="text-13-1">
</div><div id="outline-container-orgheadline89" class="outline-4">
<h4 id="orgheadline89"><span class="section-number-4">13.1.1</span> Cascade Correlation (Learning the Structure of an ANN)</h4>
<div class="outline-text-4" id="text-13-1-1">
<ul class="org-ul">
<li>No textbook sections on this, ask for paper.</li>
<li>Start with a single perceptron.  Train and find "residuals".</li>
<li>Now, add a new perceptron that feeds into the original one.</li>
<li>Train it to feed the "residuals" into the original perceptron.
<ul class="org-ul">
<li>Hold the original perceptron's training constant.</li>
</ul></li>
<li>Continue adding perceptrons that correct for the "residual" of the
previous iteration.</li>
<li>This essentially does a Taylor series approximation of the underlying
function.</li>
<li>It is an instantiation of a more general technique called "gradient
boosting".</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline90" class="outline-4">
<h4 id="orgheadline90"><span class="section-number-4">13.1.2</span> Interpretation of Hidden Units</h4>
<div class="outline-text-4" id="text-13-1-2">
<ul class="org-ul">
<li>Unlike Decision trees, the ANN structure is very opaque.</li>
<li>Difficult to interpret what it is doing.</li>
<li>One way is to look at the last layer of the ANN (the last perceptron) and
see what it's doing.  You could even assign labels to each of the inputs
for whatever concept you may believe they represent.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline91" class="outline-4">
<h4 id="orgheadline91"><span class="section-number-4">13.1.3</span> How Many Hidden Units?</h4>
<div class="outline-text-4" id="text-13-1-3">
<ul class="org-ul">
<li>Some work shows that it is better to start with a network that is too big.
<ul class="org-ul">
<li>Train until the error on the validation set grows.</li>
<li>Then look at the weights associated with edges, and prune the hidden
units that don't actually contribute.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline92" class="outline-4">
<h4 id="orgheadline92"><span class="section-number-4">13.1.4</span> Recurrent ANNs</h4>
<div class="outline-text-4" id="text-13-1-4">
<ul class="org-ul">
<li>So far, we've looked at ANNs that feed forward.</li>
<li>There are also networks with loops, called "recurrent neural networks"
<ul class="org-ul">
<li>This gives ANNs a "memory" of previous inputs.</li>
<li>They are much more of a dynamic structure</li>
</ul></li>
<li>A recurrent ANN architecture with <i>rational weights</i> has computational
power equivalent to a Universal Turing Machine!!!!!
<ul class="org-ul">
<li>However, this is ridiculously hard to train (ya don't say&#x2026;)</li>
<li>Very prone to overfitting.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline93" class="outline-4">
<h4 id="orgheadline93"><span class="section-number-4">13.1.5</span> Pros/Cons of ANNs</h4>
<div class="outline-text-4" id="text-13-1-5">
<p>
Pros:
</p>
<ul class="org-ul">
<li>Very expressive hypothesis space</li>
<li>Very useful for classification, regression, density estimation</li>
<li>Builds useful representations "automatically"</li>
</ul>

<p>
Cons:
</p>
<ul class="org-ul">
<li>Easy to overfit.</li>
<li>Slow to train, require many examples.</li>
<li>Doesn't easily handle nominal data.</li>
<li>Opaque</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline98" class="outline-3">
<h3 id="orgheadline98"><span class="section-number-3">13.2</span> Comparing Learning Algorithms</h3>
<div class="outline-text-3" id="text-13-2">
</div><div id="outline-container-orgheadline95" class="outline-4">
<h4 id="orgheadline95"><span class="section-number-4">13.2.1</span> Issue 1</h4>
<div class="outline-text-4" id="text-13-2-1">
<ul class="org-ul">
<li>Suppose we collect test data and evaluate a classifier.  Accuracy=\(x\).</li>
<li>Then, someone repeats the experiment with another set of test data from the
same problem, independent of the first set.
<ul class="org-ul">
<li>What can we say about the accuracy here?</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline96" class="outline-4">
<h4 id="orgheadline96"><span class="section-number-4">13.2.2</span> Issue 2</h4>
<div class="outline-text-4" id="text-13-2-2">
<ul class="org-ul">
<li>Suppose now we have two different classifiers \(A\) and \(B\).  We measure
their accuracies on a test set, and get \(x\) and \(y\), and \(x > y\).  Does
this mean \(A\) is better than \(B\) in this problem?</li>
<li>Or how about doing this with completely different algorithms?
<ul class="org-ul">
<li>If we repeated this experiment, we would get new \(x'\) and \(y'\).  Would
we find that \(x' > y'\) again?</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline97" class="outline-4">
<h4 id="orgheadline97"><span class="section-number-4">13.2.3</span> It's All Just Statistics!</h4>
<div class="outline-text-4" id="text-13-2-3">
<ul class="org-ul">
<li>We're just looking at estimating the "true value" of a metric on the basis
of a small sample.</li>
<li>Just like statistics!</li>
<li><b>Definition:</b> Data Distribution: assume there is an unknown, underlying
probability distribution, \(D\), from which <i>unlabeled</i> examples (\(x\)) are
being sampled without replacement.
<ul class="org-ul">
<li>I.I.D.</li>
</ul></li>
<li><p>
<b>Definition:</b> Sample Error Rate: The fraction of examples in our test
sample on which the learned classifier disagrees with the target concept.
</p>
\begin{equation}
  e_s = \frac{1}{n} \sum_x \delta(y_x, \hat{y}_x)
\end{equation}</li>
<li><p>
<b>Definition:</b> True Error Rate: The probability that the learned classifier
will make a mistake on a random example drawn from \(D\).:
</p>
\begin{equation}
  e_D = Pr_{x~D}(y_x \ne \hat{y}_x)
\end{equation}</li>
<li>For problem #1, we want to know how are \(e_S\) and \(e_D\) related.</li>
<li><b>Definition:</b> Sampling Distribution:
<ul class="org-ul">
<li>Suppose we perform a random experiment lots of times and record the
outcome.</li>
<li>Call the random variable associated with the outcome \(O\).</li>
<li>Suppose we then plot a frequency histogram of \(O\).</li>
<li>something something something (see slides)</li>
</ul></li>
<li>We'd like to get at the sampling distribution of the "error rate" r.v.,
but we'll start with something easier.</li>
<li>Let \(R\) be an rv denoting the number of errors in an evaluation
experiment: (he changed slides too quick)
<ul class="org-ul">
<li>Sampling Distribution of \(R\)
<ul class="org-ul">
<li>Suppose we run \(k\) experiment with test samples of size \(n\)</li>
<li>In the \(i\)th experiment our learned classifier makes \(R=r_i\) errors.</li>
<li>We'll pot a frequency histogram of \(R\).</li>
<li>What will it look like for \(k\) large?</li>
<li>We have a Binomial distribution.  In the limit, this actually
converges to a normal distribution.</li>
<li>This means we can infer the error rate \(e_D\) (since \(\mu=np\), \(\sigma =
          ne_D(1-e_d)\))</li>
</ul></li>
<li>If we do one trial and find that there are \(r\) errors on \(n\) examples, a
good parameter estimate for \(e_D\) is \(\frac{r}{n}\).  Why?</li>
<li>This is a maximum likelihood estimation.  It is the parameter that
maximizes the probability of the data.</li>
</ul></li>
<li><b>Definition:</b> Estimation Bias: Estimation bias of an estimator \(Y\) for
parameter \(p\) is \(E(Y)-p\).
<ul class="org-ul">
<li>If it has 0 bias, it converges asymptotically to the true value.</li>
<li>MLE has 0 estimation bias.</li>
</ul></li>
<li>This is getting to some good math, but I can't summarize it in my notes
right now if I want to understand it.  See slides.</li>
<li>Summary for Issue 1:
<ul class="org-ul">
<li>Determine sampling distribution of measure.</li>
<li>Estimate sampling distribution parameters using MLE on test set.
<ul class="org-ul">
<li>If necessary, approximate using standard distribution such as
Gaussian.</li>
</ul></li>
<li>Use tables to determine C% CI.
<ul class="org-ul">
<li>Usually use C=95</li>
<li>The true measure will lie in that interval with C% probability.</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline107" class="outline-2">
<h2 id="orgheadline107"><span class="section-number-2">14</span> 2015-09-17 Thursday</h2>
<div class="outline-text-2" id="text-14">
</div><div id="outline-container-orgheadline100" class="outline-3">
<h3 id="orgheadline100"><span class="section-number-3">14.1</span> Tradeoffs of Neural Networks</h3>
<div class="outline-text-3" id="text-14-1">
<ul class="org-ul">
<li>Lots of DoF!
<ul class="org-ul">
<li>Topology</li>
<li>Parameters</li>
</ul></li>
<li>Easy to overfit.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline103" class="outline-3">
<h3 id="orgheadline103"><span class="section-number-3">14.2</span> Training ANN</h3>
<div class="outline-text-3" id="text-14-2">
<p>
We'll pretend that the network topology is already decided.  Here is the
setup:
</p>

\begin{equation}
  D = \left( \begin{array}{ccccc}
        x_{11} & \cdots & x_{1n} & -1 & y_1 \\
        \vdots & & \vdots & \vdots & \vdots \\
        x_{m1} & \cdots & x_{mn} & -1 & y_m
      \end{array} \right)
\end{equation}

<ul class="org-ul">
<li>Want to find parameters \(\vec{w} = (w_1, w_2, \cdots, \sigma)\).</li>
<li>Such that we minimize the "loss" function \(L(\vec{w})\).</li>
<li>We can't use the sign function because it's not differentiable.</li>
<li>We can't use the dot product approximation.</li>
<li>Instead we use a sigmoid function \(y = (1 - e^{x})\) I think.</li>
</ul>

<p>
To train, we use Backpropagation!  This is gonna be fun.
</p>
<ul class="org-ul">
<li>Feed examples forward through the network.</li>
<li>Do layer-wise gradient descent starting at the output layer.</li>
</ul>
</div>

<div id="outline-container-orgheadline101" class="outline-4">
<h4 id="orgheadline101"><span class="section-number-4">14.2.1</span> Backpropagation</h4>
<div class="outline-text-4" id="text-14-2-1">
<ul class="org-ul">
<li>Let \(x_{ji}\) be the ith input to unit j.</li>
<li>Let \(w_{ji}\) be the parameter associated with \(x_{ji}\).</li>
<li>Let \(n_j = \sum_i something\)</li>
<li>Next up is the derivation of the derivative of the loss function for the
output layer.  It's easy to follow, and I can't keep up with typing the
math.  Check the slides!</li>
</ul>

<p>
Backpropagation for hidden layers.
</p>
<ul class="org-ul">
<li>A perceptron \(j\) only can affect the output from its downstream
perceptrons, which we denote as \(Downstream(j)\).</li>
<li>We can compute the derivative of the loss function with respect to the
inputs of this perceptron, \(\frac{dL}{dn_j}\), by computing the sum of
\(\frac{dL}{dn_k} \frac{dn_k}{dn_j}\) for all the \(k\in{}Downstream(j)\).
Excitingly, we already have \(\frac{dL}{dn_k}\), since \(k\) is dowstream of
\(j\),</li>
<li>The math is on the slides again, cause I'm not typing this stuff.  Still
pretty easy to follow.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline102" class="outline-4">
<h4 id="orgheadline102"><span class="section-number-4">14.2.2</span> Example</h4>
<div class="outline-text-4" id="text-14-2-2">
<p>
Consider a neural network with 2 input units, 2 hidden units, and 1 output
unit, and all weights initialized to 1, with the bias set to zero.  Using
squared loss, show the weights after the first backpropagation update with
these examples.
</p>

<p>
We have the inputs labelled 1 and 2, and then the two internal nodes labeled
3 and 4, and the output node labeled 5.  Weights and x's are labeled
accordingly.
</p>

<table>


<colgroup>
<col  class="org-right">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">
</colgroup>
<tbody>
<tr>
<td class="org-right">x<sub>1</sub></td>
<td class="org-right">x<sub>2</sub></td>
<td class="org-right">f</td>
<td class="org-right">\(\hat{f}\)</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0.731</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0.812</td>
</tr>
</tbody>
</table>

<p>
Now that we have the initial outputs of the network, we can compute the
derivatives for each example, and once we have all the derivatives we add up
all the derivatives and compute the next step.
</p>

<p>
Example 1:
</p>
<ul class="org-ul">
<li>Output layer: \(\frac{dL}{dw_{53}} = (0.731) (1 - 0.731) (0.5) (0.731 - 0) = 0.0719\)</li>
</ul>

<p>
Example 2:
</p>
<ul class="org-ul">
<li>Output Layer: \(\frac{dL}{dw_{53}} = (0.812) (1 - 0.812) (0.731) (0.812 - 1) = -0.021\)</li>
</ul>

<p>
Update:
</p>
<ul class="org-ul">
<li>\(w_{53}' = 1 - \eta (0.0719 - 0.021) = 0.949\) (assuming \(\eta = 1\) for example).</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline105" class="outline-3">
<h3 id="orgheadline105"><span class="section-number-3">14.3</span> Overfitting in ANNs</h3>
<div class="outline-text-3" id="text-14-3">
<ul class="org-ul">
<li>They are very prone to overfitting, due to the large amount of parameters.</li>
<li>Can create very nonlinear decision surfaces.</li>
<li>You can impose a simple structure on the network, but then the network may
not be capable of representing the true decision boundary.</li>
<li>Some strategies for controlling overfitting:</li>
</ul>
</div>

<div id="outline-container-orgheadline104" class="outline-4">
<h4 id="orgheadline104"><span class="section-number-4">14.3.1</span> Weight Decay</h4>
<div class="outline-text-4" id="text-14-3-1">
<p>
Add a "weight decay term" to keep the weights from growing:
</p>

<p>
\(L_{OC}(\vec{y}, \hat{\vec{y}}, \vec{w}) = L(\vec{y}, \hat{\vec{y}}, \vec{w}) + \gamma \sum_i \sum_j w_{ji}^2\)
</p>

<p>
If you have a large \(\gamma\), your solution will tend to $w<sub>ji</sub>$'s will tend toward
zero, to minimize the effect of \(\gamma\).  So it seems careful choice of \(\gamma\) is
pretty important.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline106" class="outline-3">
<h3 id="orgheadline106"><span class="section-number-3">14.4</span> Implementation Issues</h3>
<div class="outline-text-3" id="text-14-4">
<p>
You should standardize your inputs to zero mean, unit variance, so that your
units don't have a massive effect on the network.
</p>

<p>
Nominal features: you need to re-encode it.  You could do 1 of N input units.
Or you could do logarithmic encoding, where each input is a binary code.
</p>
</div>
</div>
</div>

<div id="outline-container-orgheadline114" class="outline-2">
<h2 id="orgheadline114"><span class="section-number-2">15</span> 2015-09-15 Tuesday</h2>
<div class="outline-text-2" id="text-15">
</div><div id="outline-container-orgheadline108" class="outline-3">
<h3 id="orgheadline108"><span class="section-number-3">15.1</span> Famous Dead People</h3>
<div class="outline-text-3" id="text-15-1">
<ul class="org-ul">
<li>George Boole - father of Boolean algebra.</li>
<li>Someone else - neuroscience.</li>
<li>Frank Rosenblatt (may not be dead) - artificial neurons.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline109" class="outline-3">
<h3 id="orgheadline109"><span class="section-number-3">15.2</span> History</h3>
<div class="outline-text-3" id="text-15-2">
<ul class="org-ul">
<li>We want "artificial intelligence."</li>
<li>Human brain is intelligent.</li>
<li>Try to simulate the structure of the brain to achieve intelligence</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline113" class="outline-3">
<h3 id="orgheadline113"><span class="section-number-3">15.3</span> Perceptron / Linear Threshold Unit</h3>
<div class="outline-text-3" id="text-15-3">
<ul class="org-ul">
<li>Has weighted (\(w_i\)) inputs (\(x_i\)).</li>
<li>Has Activation Threshold \(\sigma\)</li>
<li><p>
Activation function is:
</p>

\begin{equation}
  h(\vec{x}; \vec{w}, \sigma) = \left\{
  \begin{array}{ll}
    +1 & \text{if } \vec{w} \cdot \vec{x} \ge \sigma \\
    -1 & \text{else} \\
  \end{array}
  \right.
\end{equation}</li>

<li>The parameters of the perceptron are \(\vec{w}\) and \(\sigma\).
<ul class="org-ul">
<li>There aren't really parameters of the decision tree algorithm, just the
structure of the tree.</li>
</ul></li>

<li><p>
Example evaluation for perceptron \(\vec{w}=(1,2)\), \(\sigma=0.5\):
</p>

<table>


<colgroup>
<col  class="org-right">

<col  class="org-right">

<col  class="org-right">
</colgroup>
<tbody>
<tr>
<td class="org-right">\(x_1\)</td>
<td class="org-right">\(x_2\)</td>
<td class="org-right">h</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">-1</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table></li>

<li>So, the question remains, how do we train them?</li>
</ul>
</div>

<div id="outline-container-orgheadline110" class="outline-4">
<h4 id="orgheadline110"><span class="section-number-4">15.3.1</span> Training a Perceptron</h4>
<div class="outline-text-4" id="text-15-3-1">
<ul class="org-ul">
<li>Loss function: \(L(\vec{w},\sigma)\)</li>
<li>Measures the difference between the current estimates of \(y\) (\(\hat{y}\)),
and the true \(y\) (which is known), over all training examples.</li>
<li>Our goal is to minimize the loss function with respect to \((\vec{w}, \sigma)\).</li>
<li>Notations:
<ul class="org-ul">
<li>Training data: (he changed the slide too quick)</li>
</ul></li>
<li><p>
Common loss function is "squared loss":
</p>
\begin{equation}
  L(\vec{w}) = \frac{1}{2} \sum_{i=1}^m (y_i - \hat{y}_i)^2
             = \frac{1}{2} \sum_{i=1}^m (y_i - sign(\vec{w}\cdot\vec{x}_i))^2
\end{equation}</li>
<li><p>
Sign function is not differentiable, so we'll replace it by dot product.
</p>
\begin{equation}
  L(\vec{w}) = \frac{1}{2} \sum_{i=1}^m (y_i - \vec{w}\cdot\vec{x}_i)^2
\end{equation}</li>
<li><p>
Calculate gradient wrt \(\vec{w}\)
</p>
\begin{equation}
  \frac{dL}{d\vec{w}} = \sum_{i=1}^m (y_i - \vec{w} \cdot \vec{x}_i)(-\vec{x}_i)
\end{equation}</li>
<li><p>
Parameter Update:
</p>
\begin{equation}
  \vec{w} \gets \vec{w} - \eta \frac{dL}{d\vec{w}}
\end{equation}</li>
<li>We can use gradient descent
<ul class="org-ul">
<li>Loss function is differentiable.</li>
<li>Loss function is bounded below by 0.</li>
<li>Loss function is convex (proof???)</li>
<li>This means there is a well-defined minimum for the loss function.</li>
<li>And, gradient descent will find it!</li>
</ul></li>
<li>However, just cause the gradient descent converges, doesn't mean that it
will converge to 0, since the true concept is not necessarily linear.</li>
<li><p>
Stochastic G.D:
</p>
\begin{array}{l}
  \frac{dL}{d\vec{w}} = (y_i - \vec{w} \cdot \vec{x}_i)(-\vec{x}_i) \\
  \vec{w} \gets \vec{w} - \eta \frac{dL}{d\vec{w}}
\end{array}
<ul class="org-ul">
<li>This is done for each example instead of as a group.</li>
<li>Since the loss function is convex, it will converge to the same thing in
the limit.</li>
<li>But the stochastic procedure will procede differently and maybe converge
at a different speed.</li>
<li>Stochastic seems to give initial examples more "weight" in the direction
of the search.</li>
<li>Stochastic is better for "online" learning, and for very large datasets.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline111" class="outline-4">
<h4 id="orgheadline111"><span class="section-number-4">15.3.2</span> More on Perceptrons</h4>
<div class="outline-text-4" id="text-15-3-2">
<ul class="org-ul">
<li>Geometry of the perceptron:
<ul class="org-ul">
<li>In one dimension, it is a step function.</li>
<li>In two dimensions, the separating surface is a line.</li>
<li>In three dimensions, the separating surface is a plane.</li>
<li>So, in general, the decision surface is a hyperplane.</li>
</ul></li>
<li>Loss function is 0 when the surface completely separates the examples with
no errors.  It is non-0 when there are some wrong ones.</li>
<li>Linear separability is whether or not a dataset can be separated by a
linear function without error.</li>
<li>The perceptron is not nearly as powerful as a decision tree (can't
separate things like exclusive or).</li>
<li>So, it is more resistant to overfitting.  (which we will quantify later)</li>
<li>It can do some logic:
<ul class="org-ul">
<li><p>
Conjunctions:
</p>
\begin{array}{l}
  x_1 \land x_2 \land x_3 \leftrightarrow y \\
  1 \cdot x_1 + 1 \cdot x_2 + 1 \cdot x_3 \ge 3
\end{array}</li>
<li><p>
At least $m$-of-\(n\):
</p>
\begin{array}{l}
  (x_1 \land x_2) \lor (x_1 \land x_3) \lor (x_2 \land x_3) \leftrightarrow y \\
  1 \cdot x_1 + 1 \cdot x_2 + 1 \cdot x_3 \ge 2
\end{array}</li>
</ul></li>
<li>But not all:
<ul class="org-ul">
<li>Complex disjunctions</li>
<li>Exclusive or!!</li>
</ul></li>
<li>Can fix this by using more perceptrons hooked up to each other.</li>
<li>The neural network for exclusive or looks remarkably similar to the logic
gate circuit for XOR :D</li>
<li>It involves a "hidden" layer that isn't part of the output.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline112" class="outline-4">
<h4 id="orgheadline112"><span class="section-number-4">15.3.3</span> Feedforward Network Topology</h4>
<div class="outline-text-4" id="text-15-3-3">
<ul class="org-ul">
<li>Essentially, a directed acyclic graph of perceptrons.</li>
<li>But, it may be that you have to follow the layer structure.</li>
<li>Representation ability
<ul class="org-ul">
<li>Every boolean function can be represented by a network w/ one hidden
layer.</li>
<li>Every bounded continuous function can be represented by a network with
one hidden layer.</li>
<li>Every function in R<sup>n</sup> can be represented by a network with two hidden
layers.</li>
<li>Woah.</li>
</ul></li>
<li>This gives you a tradeoff&#x2026;
<ul class="org-ul">
<li>You end up with the possibility for a lot of overfitting (many degrees
of freedom and high representation ability).</li>
<li>It also takes a long time to train these networks if they are complex.</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-orgheadline120" class="outline-2">
<h2 id="orgheadline120"><span class="section-number-2">16</span> 2015-09-10 Thursday</h2>
<div class="outline-text-2" id="text-16">
</div><div id="outline-container-orgheadline119" class="outline-3">
<h3 id="orgheadline119"><span class="section-number-3">16.1</span> Evaluation Methods and Metrics</h3>
<div class="outline-text-3" id="text-16-1">
<p>
How do you figure out if your algorithm is "good"?
</p>

<p>
Goal: find a measure <b>expected future performance</b> of the learning algorithm
for some problem.  How?
</p>

<p>
Idea:
</p>
<ul class="org-ul">
<li>Separate available data into sets for training and evaluation.</li>
<li>The examples for evaluation will be new to the learned classifier.</li>
<li>Do this lots of times to get reliable estimates.</li>
<li>The sets should be "separate" at least in the sense of independently
chosen, if not disjoint examples.</li>
</ul>
</div>

<div id="outline-container-orgheadline115" class="outline-4">
<h4 id="orgheadline115"><span class="section-number-4">16.1.1</span> n-fold Cross Validation</h4>
<div class="outline-text-4" id="text-16-1-1">
<ul class="org-ul">
<li>Generally, the number of examples is limited.</li>
<li>Want to train on sets that are as large as possible.</li>
<li>Divide set into \(n\) separate sets.
<ul class="org-ul">
<li>For each set, withhold it for testing, and train on the remaining sets.</li>
<li>Then evaluate the classifier on the testing sets.</li>
</ul></li>
<li>Special case of $n$-fold cross validation: Leave-one-out
<ul class="org-ul">
<li>\(n\) examples, \(n\) folds.</li>
<li>Only really useful if you have a few examples.</li>
<li>Called "jackknife" in statistics literature.</li>
</ul></li>
<li>Stratified cross validation
<ul class="org-ul">
<li>Same as $n$-fold cross validation, but you sample folds such that the
proportions of class labels is preserved in each fold.</li>
<li>More stable performance estimates.</li>
<li>Implementation:
<ul class="org-ul">
<li>Put \(pos\) positive examples in one list, and \(neg\) negative examples
in another.</li>
<li>Randomly shuffle the lists.</li>
<li>Put the first \(pos/n\) positives in fold 1, the next into fold 2, etc.</li>
<li>Repeat for negatives.</li>
<li>Assign leftover examples randomly.</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline116" class="outline-4">
<h4 id="orgheadline116"><span class="section-number-4">16.1.2</span> Metrics for Classification</h4>
<div class="outline-text-4" id="text-16-1-2">
<p>
Contingency Table
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">Positive (TC)</td>
<td class="org-left">Negative (TC)</td>
</tr>

<tr>
<td class="org-left">Positive (C)</td>
<td class="org-left">True Positive (TP)</td>
<td class="org-left">False Positives (FP, Type I)</td>
</tr>

<tr>
<td class="org-left">Negative (C)</td>
<td class="org-left">False Negative (FN, Type II)</td>
<td class="org-left">True Negative (TN)</td>
</tr>
</tbody>
</table>

<p>
Can compute all metrics from the contingency table.
</p>

<ul class="org-ul">
<li><p>
Accuracy: most commonly used measure for comparing algorithms.
</p>
\begin{equation}
  \text{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN}
\end{equation}
<ul class="org-ul">
<li>Simply the fraction of examples that are correctly classified.</li>
<li>There are many problems with accuracy.
<ul class="org-ul">
<li>Skewed class distribution: eg, if 99% animals aren't lions, a
classifier with 99% accuracy would just predict "not lion".  And it
would kill you next time you see a lion.</li>
<li>Differential misclassification costs: some types of errors (FP or FN)
are more serious for an application than others (eg screening for a
disease).  Accuracy treats them equally.</li>
</ul></li>
</ul></li>
<li><p>
Weighted Accuracy
</p>
\begin{equation}
  \text{WAcc} = \frac{1}{2}\left(\frac{TP}{Allpos} + \frac{TN}{Allneg}\right)
              = \frac{1}{2}\left(\frac{TP}{TP + FN} + \frac{TN}{TN + FP}\right)
\end{equation}
<ul class="org-ul">
<li>First part is the "true positive rate" (how many positives are correctly
identified)</li>
<li>Second part is the "true negative rate" (how many negatives are
correctly identified)</li>
</ul></li>
<li><p>
Precision
</p>
\begin{equation}
  \text{Precision} = \frac{TP}{TP + FP}
\end{equation}
<ul class="org-ul">
<li>Sometimes, the "positive" case is all you're interested in.</li>
<li>This measures "of all the examples classified positive, how many were
actually positive?"</li>
</ul></li>
<li><p>
Recall / True Positive Rate / Sensitivity
</p>
\begin{equation}
  \text{Recall} = \frac{TP}{TP + FN}
\end{equation}
<ul class="org-ul">
<li>This quantifies "of all the positive examples, how many were correctly
classified?"</li>
</ul></li>
<li><p>
Specificity
</p>
\begin{equation}
  \text{Specificity} = \frac{TN}{TN + FP}
\end{equation}
<ul class="org-ul">
<li>Conterpart of recall for the negative class.</li>
</ul></li>
<li><p>
F1
</p>
\begin{equation}
  \frac{1}{F1} = \frac{1}{2} \left( \frac{1}{Precision} + \frac{1}{Recall}\right)
\end{equation}
\begin{equation}
  F1 = \frac{2}{\left( \frac{1}{Precision} + \frac{1}{Recall}\right)}
\end{equation}
<ul class="org-ul">
<li>Combines precision and recall into single measure.</li>
<li>Not necessarily a good idea, but widely used.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline117" class="outline-4">
<h4 id="orgheadline117"><span class="section-number-4">16.1.3</span> Learning Curves</h4>
<div class="outline-text-4" id="text-16-1-3">
<ul class="org-ul">
<li>Frequently it's useful to plot metrics as a function of sample size.</li>
<li>Provides insight into how many examples the algorithm needs to be
effective.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline118" class="outline-4">
<h4 id="orgheadline118"><span class="section-number-4">16.1.4</span> Metrics with Confidence Measures</h4>
<div class="outline-text-4" id="text-16-1-4">
<ul class="org-ul">
<li>Many learning algorithms produce classifiers or models that provide
estimates of how confident they are.</li>
<li>Can use this to create Precision/Recall curves or Receiver Operator
Characteristic curves.</li>
<li>Precision/Recall curves:
<ul class="org-ul">
<li>plot precision, recall as you change threshold.</li>
</ul></li>
<li>ROC graphs
<ul class="org-ul">
<li>plot FPR x , TPR y as you change threshold.</li>
<li>Random guessing is a diagonal line.
<ul class="org-ul">
<li>Also majority class classifier.</li>
<li>Good classifier mst be above the diagonal.</li>
</ul></li>
<li>Monotonically increasing.</li>
<li>Can be misleading if class distribution is too skewed.
<ul class="org-ul">
<li>Use PR instead.</li>
</ul></li>
<li>Frequently use AUC as statistic.</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orgheadline125" class="outline-2">
<h2 id="orgheadline125"><span class="section-number-2">17</span> 2015-09-08 Tuesday</h2>
<div class="outline-text-2" id="text-17">
</div><div id="outline-container-orgheadline121" class="outline-3">
<h3 id="orgheadline121"><span class="section-number-3">17.1</span> Review:</h3>
<div class="outline-text-3" id="text-17-1">
<ul class="org-ul">
<li>Decision trees: trees where internal nodes are tests on attributes, and
leaves are class labels.</li>
<li>Construct them by choosing attributes which give the most information.</li>
<li>Measure this information with entropy, mutual information ("information
gain").</li>
<li>ID3 algorithm is the formal algorithm for applying mutual information to
constructing decision trees.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline122" class="outline-3">
<h3 id="orgheadline122"><span class="section-number-3">17.2</span> Generalizing ID3</h3>
<div class="outline-text-3" id="text-17-2">
<ul class="org-ul">
<li>What about multiple valued attributes (more than 2-valued)?
<ul class="org-ul">
<li>Mutual information still applies to $v$-valued finite, discrete
variables.</li>
<li>You simply have the internal node for that attribute have \(v\) children
instead of 2.</li>
<li>However, the maximum mutual information for a \(k\) valued variable is
\(\log{k}\), so the IG function is biased towards attributes with many
values.</li>
<li>Can normalize by dividing by \(H(X)\), the entropy of the attribute itself.
<ul class="org-ul">
<li><b>Question:</b> why is this better than dividing by \(\log{|X|}\), e.g., the
maximum overall entropy of \(H(X)\)?</li>
<li>In essence, this division gives you a quantity that answers the
question "what fraction of this variable's entropy contributes
information about the class label?"</li>
</ul></li>
</ul></li>
<li>Continuous Attributes
<ul class="org-ul">
<li>Continuous variables have entropy defined on them, but it's useless for
making a decision in a tree.</li>
<li>Need to "bin" the attribute (\(X \le v\) or \(X \ge v\)).</li>
<li>You only need to consider values for \(v\) that separate different class
labels in the training set.
<ul class="org-ul">
<li>This is still problematic for large training sets, as we'll see on our
programming assignment.</li>
</ul></li>
</ul></li>
</ul>

<p>
Example
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-right">

<col  class="org-left">

<col  class="org-right">
</colgroup>
<tbody>
<tr>
<td class="org-left">Color</td>
<td class="org-right">Area</td>
<td class="org-left">Shape</td>
<td class="org-right">Class Label</td>
</tr>

<tr>
<td class="org-left">red</td>
<td class="org-right">0.1</td>
<td class="org-left">circle</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">red</td>
<td class="org-right">0.7</td>
<td class="org-left">square</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">red</td>
<td class="org-right">0.4</td>
<td class="org-left">triangle</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">blue</td>
<td class="org-right">0.2</td>
<td class="org-left">triangle</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">blue</td>
<td class="org-right">0.6</td>
<td class="org-left">circle</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">blue</td>
<td class="org-right">0.8</td>
<td class="org-left">square</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">green</td>
<td class="org-right">0.4</td>
<td class="org-left">square</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">green</td>
<td class="org-right">0.3</td>
<td class="org-left">triangle</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">green</td>
<td class="org-right">0.3</td>
<td class="org-left">circle</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>

<ol class="org-ol">
<li>First, compute H(Y), which is \(H(\frac{1}{3})\) (as a shorthand).</li>
<li><p>
Then, compute H(Y|Color):
</p>

\begin{equation}
H(Y|Color) = p(Color=red)H(Y|Color=red) + p(Color=blue)H(Y|color=blue) + p(Color=green)H(Y|Color=green)
\end{equation}

\begin{equation}
H(Y|Color) = \frac{1}{3}H(\frac{1}{3}) + \frac{1}{3}H(\frac{1}{3}) + \frac{1}{3}\times 0
\end{equation}

\begin{equation}
H(Y|Color) = \frac{2}{3}H(\frac{1}{3})
\end{equation}</li>

<li><p>
We can use this to compute the information gain of Color.
</p>

\begin{equation}
IG(Color) = H(Y) - H(Y|Color) = \frac{1}{3} H(\frac{1}{3})
\end{equation}</li>

<li>Conveniently, this is the same as the information gain of Shape.</li>

<li><p>
For area, if we sort the training set by Area, we find the cutoffs 0.25,
0.35, and 0.5.  Then we can compute H(Y|Area,v) for each cutoff v.
</p>

<p>
\(H(Y|Area\le0.25) = \frac{2}{9}\times 0 + \frac{7}{9} H(\frac{1}{7})\), so IG(Area&le; 0.25) = 0.4583
</p>

<p>
etc for each cutoff
</p></li>

<li>You choose the best IG, and use that for the root node.  Then continue to
do this for each child node.</li>
</ol>
</div>
</div>

<div id="outline-container-orgheadline124" class="outline-3">
<h3 id="orgheadline124"><span class="section-number-3">17.3</span> Overfitting</h3>
<div class="outline-text-3" id="text-17-3">
<ul class="org-ul">
<li>Given enough features, ID3 will create a tree that fits your data perfectly.
<ul class="org-ul">
<li>Enough features = enough that there are no contradictory examples.</li>
</ul></li>
<li>Overfitting is an issue.</li>

<li>What is overfitting?  Making your model too specific to your training
examples, and not general enough to be applied well to new data.</li>

<li>Strictly, if a concept \(h\) has:

<ul class="org-ul">
<li>Higher performance on the training examples, but</li>
<li>Lower performance on the whole dataset</li>
</ul></li>

<li>Than some other concept \(h'\), then we say that \(h\) has overfit the training
data.</li>
</ul>
</div>

<div id="outline-container-orgheadline123" class="outline-4">
<h4 id="orgheadline123"><span class="section-number-4">17.3.1</span> Controlling Overfitting</h4>
<div class="outline-text-4" id="text-17-3-1">
<ul class="org-ul">
<li>Can introduce a restriction on the hypothesis space, to prevent overly
complex hypotheses from being learned.</li>
<li>Early Stopping
<ul class="org-ul">
<li>Standard ID3 algorithm stops when IG(X)=0 for all X.</li>
<li>Instead, stop when IG(X) &le; &epsilon;, for some chosen &epsilon;.</li>
<li>This is sensitive to your parameter choice for &epsilon;.</li>
<li>It's easy to implement, but doesn't work well in practice.</li>
</ul></li>
<li>Greedy post-pruning
<ul class="org-ul">
<li>Hold aside some training examples at the start.</li>
<li>Do your training procedure on the remainder (allowing it to overfit if
it wants).</li>
<li>Then, do a <i>greedy pruning</i> algorithm on your model.</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orgheadline141" class="outline-2">
<h2 id="orgheadline141"><span class="section-number-2">18</span> 2015-09-01 Tuesday</h2>
<div class="outline-text-2" id="text-18">
<p>
HW1 due tonight at midnight.  HW 2 out today.  Read Ch. 3 in Mitchell.
</p>
</div>

<div id="outline-container-orgheadline135" class="outline-3">
<h3 id="orgheadline135"><span class="section-number-3">18.1</span> What is "Machine Learning?"</h3>
<div class="outline-text-3" id="text-18-1">
<ul class="org-ul">
<li>Machine = autonomous system, with no (or limited) human intervention.</li>
<li>Learning?
<ul class="org-ul">
<li>System changes after an experience, so that it can work more effectively
next time it does the task.</li>
<li>We want the system to learn how to do <i>related</i> tasks better too.</li>
</ul></li>
<li>Specification for a learning system:
<ul class="org-ul">
<li>Given: Task goal, performance measure P, and examples E</li>
<li>Produce a <b>concept</b> that is good wih respect to P on <i>all</i> examples of
the task.</li>
</ul></li>
<li>Example: learn to play chess
<ul class="org-ul">
<li>Perforance measure = games won/lost</li>
<li>Examples = games played</li>
<li>Concept?  Probably a function mapping a current board state to a move to
play next.</li>
</ul></li>
<li>Two phases: learning/training, and evaluation/testing
<ul class="org-ul">
<li>(In the evaluation phase, you want to evaluate on new examples that you
haven't trained on).</li>
</ul></li>
<li>Batch learning: one learning phase, with a large set of examples, followed
by a testing phase.</li>
<li>Online learning: examples arrive one at a time (or in small groups);
learning and evaluation phases iterate.</li>
<li>Learning systems need to have some sort of constraint.  Memorizing all the
examples is probably the best strategy, but we know that this doesn't
represent learning the underlying concept.</li>
</ul>
</div>

<div id="outline-container-orgheadline126" class="outline-4">
<h4 id="orgheadline126"><span class="section-number-4">18.1.1</span> Inductive Generalization</h4>
<div class="outline-text-4" id="text-18-1-1">
<ul class="org-ul">
<li>In all learning problems, need to reason from specific examples to a
general case.</li>
<li>(this is the reverse of deductive reasoning, where you reason from the
general case to the specific case)</li>
<li>Target concept = the underlying concept that the system is trying to
learn.  EG, Gary kasparov's head.</li>
<li>Typically, the performance measure quantifies the difference between
current and target concepts.</li>
<li>Hypothesis space - all concepts the learning system will consider
(e.g. all possible combinations of animal properties)</li>
<li>Hopefully, target concept is in the hypothesis space.
<ul class="org-ul">
<li>But can't include every possible hypothesis in your space.</li>
<li>The size would be huge.</li>
<li>You would end up memorizing, not learning.</li>
</ul></li>
<li>This is the idea behind "No Tabula Rasa" (blank slate) learning.  There
has to be some sort of restriction on hypothesis spaces.</li>
<li>Inductive Bias
<ul class="org-ul">
<li>Assumptions used to limit the hypothesis space are the inductive bias.</li>
<li>The more assumptions, the stronger the bias.</li>
<li>It can even be quantified (later)</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline134" class="outline-4">
<h4 id="orgheadline134"><span class="section-number-4">18.1.2</span> Learning Settings</h4>
<div class="outline-text-4" id="text-18-1-2">
</div><ol class="org-ol"><li><a id="orgheadline127"></a>Supervised Learning<br ><div class="outline-text-5" id="text-18-1-2-1">
<ul class="org-ul">
<li>Examples are annotated by a teacheer or oracle.</li>
<li>Learning system just finds the concept to match the annotations.</li>
</ul>
</div></li>

<li><a id="orgheadline128"></a>Unsupervised Learning<br ><div class="outline-text-5" id="text-18-1-2-2">
<ul class="org-ul">
<li>No annotations</li>
<li>Goal is to find interesting patterns in the examples</li>
<li>System defines what is interesting.</li>
<li>Example: grouping images by content.</li>
</ul>
</div></li>

<li><a id="orgheadline129"></a>Semi-Supervised Learning<br ><div class="outline-text-5" id="text-18-1-2-3">
<ul class="org-ul">
<li>"<b>normal learning</b>" is really a combination of the two</li>
<li>You do unsupervised learning, and you occasionally get your
"parent"/oracle to come in and teach you some labels.</li>
<li>You use those new concepts to help you organize your thoughts better.</li>
</ul>
</div></li>

<li><a id="orgheadline130"></a>Active Learning<br ><div class="outline-text-5" id="text-18-1-2-4">
<ul class="org-ul">
<li>A few examples are annotated with the target concept.</li>
<li>Learning system can "ask" the oracle to label something.</li>
<li>There is a cost of labelling that the system must optimize.</li>
</ul>
</div></li>

<li><a id="orgheadline131"></a>Transductive Learning<br ><div class="outline-text-5" id="text-18-1-2-5">
<ul class="org-ul">
<li>Learning system has some knowledge of possible examples it will be
evaluated on.</li>
<li>Adjusts the system to do better on those examples.</li>
<li>EG - learn to play chess against Kasparov.</li>
</ul>
</div></li>

<li><a id="orgheadline132"></a>Reinforcement Learning<br ><div class="outline-text-5" id="text-18-1-2-6">
<ul class="org-ul">
<li>This is "sequential" learning.</li>
<li>Your environment provides feedback.</li>
<li>You take actions and use the consequences to learn.</li>
</ul>
</div></li>

<li><a id="orgheadline133"></a>Transfer Learning<br ><div class="outline-text-5" id="text-18-1-2-7">
<ul class="org-ul">
<li>Human learning is cumulative.
<ul class="org-ul">
<li>When we encounter a new problem, we don't just start from scratch.</li>
<li>We use prior knowledge and reasoning.</li>
</ul></li>
<li>Transfer learning attempts to apply concepts learned in other problems to
bias your search.</li>
</ul>
</div></li></ol>
</div>
</div>

<div id="outline-container-orgheadline136" class="outline-3">
<h3 id="orgheadline136"><span class="section-number-3">18.2</span> When to use ML?</h3>
<div class="outline-text-3" id="text-18-2">
<ul class="org-ul">
<li>Shouldn't use ML to recognize geometric shapes.</li>
<li>In general, you don't need to learn if you have these things:
<ul class="org-ul">
<li>The concept is already accurately known.</li>
<li>It can be easily (and compactly) described</li>
<li>Unlikely to change</li>
</ul></li>
<li>Learning is not free, requires computation and storage, and real world
effort in labeling, etc.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline140" class="outline-3">
<h3 id="orgheadline140"><span class="section-number-3">18.3</span> Example Representations</h3>
<div class="outline-text-3" id="text-18-3">
<ul class="org-ul">
<li>Internal representation of examples effects how you learn.</li>
<li>EG: When you recognize objects, you don't do it at the level of signals on
your optic nerve.  You do it at the level of smaller parts that you've
learned.  A chair has four legs, a flat surface, and usually a back.</li>
<li>In the same way, pixels aren't useful in object recognition.</li>
<li>This is an open area of research: we don't always know the best
representation of examples.</li>
</ul>
</div>

<div id="outline-container-orgheadline137" class="outline-4">
<h4 id="orgheadline137"><span class="section-number-4">18.3.1</span> Feature Vector Representation</h4>
<div class="outline-text-4" id="text-18-3-1">
<ul class="org-ul">
<li>Examples are vectors of values for a set of attributes.</li>
<li><p>
Can be an n-by-m matrix
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-left">

<col  class="org-left">

<col  class="org-left">
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">Attr 1</td>
<td class="org-left">Attr 2</td>
<td class="org-left">Attr 3</td>
</tr>

<tr>
<td class="org-left">EG 1</td>
<td class="org-left">v<sub>11</sub></td>
<td class="org-left">v<sub>12</sub></td>
<td class="org-left">v<sub>13</sub></td>
</tr>

<tr>
<td class="org-left">EG 2</td>
<td class="org-left">V<sub>21</sub></td>
<td class="org-left">V<sub>22</sub></td>
<td class="org-left">v<sub>23</sub></td>
</tr>

<tr>
<td class="org-left">EG 3</td>
<td class="org-left">v<sub>31</sub></td>
<td class="org-left">v<sub>32</sub></td>
<td class="org-left">v<sub>33</sub></td>
</tr>
</tbody>
</table></li>

<li>This is also called "propositional representation", because each example
can be a logical conjunction.</li>
<li>Can represent all the examples as logic formula.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline138" class="outline-4">
<h4 id="orgheadline138"><span class="section-number-4">18.3.2</span> Relational Representation</h4>
<div class="outline-text-4" id="text-18-3-2">
<ul class="org-ul">
<li>Can use first order logic.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline139" class="outline-4">
<h4 id="orgheadline139"><span class="section-number-4">18.3.3</span> Multiple Instance Representation</h4>
<div class="outline-text-4" id="text-18-3-3">
<ul class="org-ul">
<li>Examples are represented by arbitrary sized sets of attribute-value pairs.</li>
</ul>
</div>
</div>
</div>
</div>
<div id="outline-container-orgheadline158" class="outline-2">
<h2 id="orgheadline158"><span class="section-number-2">19</span> 2015-08-27 Thursday</h2>
<div class="outline-text-2" id="text-19">
</div><div id="outline-container-orgheadline156" class="outline-3">
<h3 id="orgheadline156"><span class="section-number-3">19.1</span> Optimization</h3>
<div class="outline-text-3" id="text-19-1">
</div><div id="outline-container-orgheadline142" class="outline-4">
<h4 id="orgheadline142"><span class="section-number-4">19.1.1</span> What is it?</h4>
<div class="outline-text-4" id="text-19-1-1">
<p>
Find the extreme points of an objective function.
</p>
</div>
</div>

<div id="outline-container-orgheadline143" class="outline-4">
<h4 id="orgheadline143"><span class="section-number-4">19.1.2</span> Types of Optimization Problems</h4>
<div class="outline-text-4" id="text-19-1-2">
<ul class="org-ul">
<li>Discrete vs Continuous - objective function is defined on discrete or
continuous space.</li>
<li>Unconstrained vs constrained - whether there are additional constraints
defining the feasible region.</li>
<li>In this class, we are interested in continuous problems, constrained and
unconstrained.  We use tools from calculus and linear algebra.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline144" class="outline-4">
<h4 id="orgheadline144"><span class="section-number-4">19.1.3</span> Unconstrained Optimization</h4>
<div class="outline-text-4" id="text-19-1-3">
<ul class="org-ul">
<li>Function of one variable, eg minimum of x<sup>2</sup>.  Typical method for solving
this is to compute first and second derivative, find zeros of first
derivative where second derivative is positive.</li>
<li>Fuctions of two variables, you find the same things, but in matrix form:
<ul class="org-ul">
<li>Jacobian \(J = (\frac{\delta{}f}{\delta{}x_i}) = 0\)</li>
<li>Hessian \(H = [\frac{\delta^{2}f}{\delta{}x_{i}\delta{}x_j}] > 0\) must be
positive definite.</li>
</ul></li>
<li>Can't always do this, due to computational constrains, and due to weird or
unknown function.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline145" class="outline-4">
<h4 id="orgheadline145"><span class="section-number-4">19.1.4</span> Gradient Ascent</h4>
<div class="outline-text-4" id="text-19-1-4">
<p>
A way of maximizing/minimizing a function.  From your current position
\(\vec{x}\), go in the direction that maximizes the increase.
</p>

<p>
\(\vec{x}_{new} = \vec{x}_{old} - \alpha \Delta f_{\vec{x}_old}(\vec{x})\)
</p>

<p>
Here, &alpha; is the step size, and &Delta; f is the function gradient
evaluated at x<sub>old</sub>.
</p>

<p>
Downside of this is that the convergence rate is not very good.  Also, this
procedure assumes linearity, where a quadratic function may be a better
approximation.
</p>
</div>
</div>

<div id="outline-container-orgheadline146" class="outline-4">
<h4 id="orgheadline146"><span class="section-number-4">19.1.5</span> Newton-Raphson Method</h4>
<div class="outline-text-4" id="text-19-1-5">
<p>
In this, we use a quadratic approximation of f.  Then, instead of taking a
linear step, we take a "Newton step".
</p>

<p>
\(f(\vec{x}_{old} + u) = f(\vec{x}_{old}) + u^T \Delta f_{\vec{x}_{old}}(\vec{x}) + \frac{1}{2} u^T \Delta^2f_{\vec{x}_{old}}(\vec{x})u = g(u)\)
</p>

<p>
More math, see slides.
</p>

<p>
Properties:
</p>
<ul class="org-ul">
<li>Fast convergence close to solution.</li>
<li>Not guaranteed to converge if started far from solution, may cycle or
diverge in this case.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline147" class="outline-4">
<h4 id="orgheadline147"><span class="section-number-4">19.1.6</span> Quasi-Newton Methods</h4>
<div class="outline-text-4" id="text-19-1-6">
<ul class="org-ul">
<li>Often, constructing the Hessian for a multivariate function is
computationally difficult, because it takes O(n<sup>2</sup>) space and time and has
to be done over and over.</li>
<li>So, a number of methods exist that approximate the Hessian by using the
Jacobian at nearby points.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline148" class="outline-4">
<h4 id="orgheadline148"><span class="section-number-4">19.1.7</span> Local and Global Optima</h4>
<div class="outline-text-4" id="text-19-1-7">
<ul class="org-ul">
<li>A <b>global minimum</b> for a function is a point x where f(x) &le; f(x+u) for
all u.</li>
<li>A <b>local minimum</b> is an x where f(x) &le; f(x+u) for all |u|&lt;&epsilon;, for
some positive &epsilon;.</li>
<li>Every global minimum is a local min, but not the other way around.</li>
<li>There is no algorithm that is guaranteed to find the global maximum of an
arbitrary function.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline149" class="outline-4">
<h4 id="orgheadline149"><span class="section-number-4">19.1.8</span> Convex Sets</h4>
<div class="outline-text-4" id="text-19-1-8">
<p>
Take two points x<sub>1</sub> and x<sub>2</sub>.  A point on the line segment between them is
defined by &lambda; x<sub>1</sub> + (1-&lambda;) x<sub>2</sub>, for 0 &le; &lambda; &le; 1.
</p>

<p>
A Convex Set is a set of points such that for any two points in the set,
&lambda; x<sub>1</sub> + (1-&lambda;) x<sub>2</sub> is also in the set (for 0 &le; &lambda; &le;
1).  Basically, you can visualize these sets on the plane as "shapes that
don't have holes in them".
</p>
</div>
</div>

<div id="outline-container-orgheadline150" class="outline-4">
<h4 id="orgheadline150"><span class="section-number-4">19.1.9</span> Convex Functions</h4>
<div class="outline-text-4" id="text-19-1-9">
<p>
If you look at all the points that are "above" a function - {(x,y)|y &ge;
f(x)}, if that set is convex, then f is a convex function.
</p>

<p>
JENSEN'S INEQUALITY (yaaaaaaas)!
</p>

<p>
f(&lambda; x<sub>1</sub> + (1-&lambda;) x<sub>2</sub>) &le; &lambda; f(x<sub>1</sub>) + (1-&lambda;) f(x<sub>2</sub>)
</p>

<p>
Jensen's inequality seems to apply for any convex function.  It just says
that the points on the segment between f(x<sub>1</sub>) and f(x<sub>2</sub>) have to be above
the the function itself.  Pretty cool.
</p>

<p>
For a convex function, every local optimum is also a global optimum!  That's
a pretty nice property to have.
</p>
</div>
</div>

<div id="outline-container-orgheadline151" class="outline-4">
<h4 id="orgheadline151"><span class="section-number-4">19.1.10</span> Constrained Optimization</h4>
<div class="outline-text-4" id="text-19-1-10">
<ul class="org-ul">
<li>Minimize a function of x such that some constraints on x are satisfied.
The constraints define a feasible region on of in which the solution must
lie.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline152" class="outline-4">
<h4 id="orgheadline152"><span class="section-number-4">19.1.11</span> Linear Programming</h4>
<div class="outline-text-4" id="text-19-1-11">
<p>
Linear Programming is a <b>special case</b> of <b>constrained optimization</b>, in
which both the objective function and the constraints are linear!
Typically, we write all the constraints and objective function as functions
of matrices and vectors, for compactness.
</p>

<p>
When you apply all these linear constraints, you have a feasible region that
is a "polyhedron" (because it is bounded by a bunch of "hyperplanes").  It's
possible that one side of the feasible region is open, (so not completely
bounded).
</p>

<p>
If you have a linear objective function, you can say for certain that an
optimal point is on one of the vertices.
</p>
</div>
</div>

<div id="outline-container-orgheadline153" class="outline-4">
<h4 id="orgheadline153"><span class="section-number-4">19.1.12</span> Simplex Algorithm</h4>
<div class="outline-text-4" id="text-19-1-12">
<ul class="org-ul">
<li>Around the polyhedron we go.</li>
<li>From any feasible vertex, walk along the edges of the polyhedron,
following the vertices.</li>
<li>Once you are at a vertex where the neighboring vertices have higher f
values, stop.</li>
<li>You've found a local optimum, which happens to be a global optimum since
the linear function is convex.</li>
</ul>

<p>
Properties of this algorithm:
</p>

<ul class="org-ul">
<li>Very simple, and easy to implement, and works well in practice.</li>
<li>It works by traversing vertices, and there may be exponentially many
vertices for n constraints.  So, in the worst case, runtime is
exponential.
<ul class="org-ul">
<li>Average case under various distributions has been shown to be
polynomial, which is useful.</li>
</ul></li>
<li>Other algorithms exist, such as "interior point methods", which have
polynomial bounds*</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline154" class="outline-4">
<h4 id="orgheadline154"><span class="section-number-4">19.1.13</span> Duality in Linear Programming</h4>
<div class="outline-text-4" id="text-19-1-13">
<p>
From any "primal" LP, we can derive a "dual" LP.  Say we have a primal LP:
</p>

<ul class="org-ul">
<li>min<sub>x</sub> c<sup>T</sup> x, such that</li>
<li>A x &ge; b</li>
<li>x &ge; 0</li>
</ul>

<p>
We could create a dual like this:
</p>

<ul class="org-ul">
<li>max<sub>u</sub> b<sup>T</sup> u, such that</li>
<li>A<sup>T</sup> u &le; c</li>
<li>u &ge; 0</li>
</ul>

<p>
The nice properties of this are:
</p>

<ul class="org-ul">
<li>The primal has a solution iff the dual has a solution.</li>
<li>Further, the dual LP is a lower bound on the primal LP.
<ul class="org-ul">
<li>That is, if we pick any feasible x and any feasible u, we always havve
c<sup>T</sup> x &ge; b<sup>T</sup> u.</li>
</ul></li>
<li>From the relationship between primal and dual LPs, we can derive a set of
conditions that characterize the solutions for a primal/dual pair, called
the Karush-Kuhn-Tucker conditions.</li>
<li>Essentially, the conditions are that at the optimal solution, x and u are
feasible and the objective functions c<sup>T</sup> x and b<sup>T</sup> u are equal (and some
other stuff).</li>
<li>Soumya says if this doesn't make sense now, that's ok.  Which is good,
because he lost me at the dual being a lower bound on the primal.</li>
</ul>
</div>
</div>

<div id="outline-container-orgheadline155" class="outline-4">
<h4 id="orgheadline155"><span class="section-number-4">19.1.14</span> Summary of Optimization</h4>
<div class="outline-text-4" id="text-19-1-14">
<ul class="org-ul">
<li>Types of optimization problems.</li>
<li>Unconstrained optimization - gradient ascent/descent, Newton Raphson
methods.</li>
<li>Convex sets and functions</li>
<li>Constrained optimization:
<ul class="org-ul">
<li>Linear programming</li>
<li>Simplex method</li>
<li>Duality</li>
<li>KKT conditions</li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgheadline157" class="outline-3">
<h3 id="orgheadline157"><span class="section-number-3">19.2</span> The Simplex Algorithm</h3>
<div class="outline-text-3" id="text-19-2">
<p>
He says we should know how it works.
</p>

<p>
Let us consider the following linear program:
</p>

<ul class="org-ul">
<li>minimize (with respect to x<sub>1</sub>, x<sub>2</sub>) f(x) = 3x<sub>1</sub> - 6x<sub>2</sub>, such that</li>
<li>x<sub>1</sub> + 2x<sub>2</sub> &ge; -1</li>
<li>2x<sub>1</sub> + x<sub>2</sub> &ge; 0</li>
<li>-x<sub>2</sub> + x<sub>1</sub> &ge; -1</li>
<li>-4x<sub>2</sub> + x<sub>1</sub> &ge; -15</li>
<li>-4x<sub>1</sub> + x<sub>2</sub> &ge; -23</li>
<li>x1, x<sub>2</sub> &ge; 0</li>
</ul>

<p>
Steps:
</p>
<ol class="org-ol">
<li>Standardize so everything is in [variables] &ge; [constant] form.</li>
<li>Introduce "slack variables".  Essentially, these are the gap in the
conditions.  These have to be greater than or equal to 0:
<ol class="org-ol">
<li>x<sub>3</sub> = x<sub>1</sub> + 2x<sub>2</sub> + 1</li>
<li>x<sub>4</sub> = 2x<sub>1</sub> + x<sub>2</sub></li>
<li>x<sub>5</sub> = -x2 + x<sub>1</sub> + 1</li>
<li>x<sub>6</sub> = -4x<sub>2</sub> + x<sub>1</sub> + 15</li>
<li>x<sub>7</sub> = -4x<sub>1</sub> + x<sub>2</sub> + 23</li>
</ol></li>
<li><p>
We can put this stuff into tableu form:
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-right">x<sub>1</sub></td>
<td class="org-right">x<sub>2</sub></td>
<td class="org-right">&#xa0;</td>
</tr>

<tr>
<td class="org-left">x<sub>3</sub></td>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">x<sub>4</sub></td>
<td class="org-right">2</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">x<sub>5</sub></td>
<td class="org-right">1</td>
<td class="org-right">-1</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">x<sub>6</sub></td>
<td class="org-right">1</td>
<td class="org-right">-4</td>
<td class="org-right">13</td>
</tr>

<tr>
<td class="org-left">x<sub>7</sub></td>
<td class="org-right">-4</td>
<td class="org-right">1</td>
<td class="org-right">23</td>
</tr>

<tr>
<td class="org-left">2</td>
<td class="org-right">3</td>
<td class="org-right">-6</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table></li>

<li><p>
Assume that zero is feasible.  Pick the variable that will decrease the
objective function (the most?), and change it accordingly.  In this case,
we choose x<sub>2</sub>.  Then, we write out the constraints, holding x<sub>1</sub> to be 0.
We find the smallest positive constraint value for x<sub>2</sub>, and choose that.
Whatever variable caused that constraint, we swap it with x<sub>2</sub>, and make a
new tableau.
</p>

<p>
In this case, x<sub>5</sub> is the blocking constraint, so we pick it.
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-right">x<sub>1</sub></td>
<td class="org-right">x<sub>5</sub></td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">x<sub>3</sub></td>
<td class="org-right">3</td>
<td class="org-right">-2</td>
<td class="org-right">3</td>
</tr>

<tr>
<td class="org-left">x<sub>4</sub></td>
<td class="org-right">3</td>
<td class="org-right">-1</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">x<sub>2</sub></td>
<td class="org-right">1</td>
<td class="org-right">-1</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">x<sub>6</sub></td>
<td class="org-right">-3</td>
<td class="org-right">4</td>
<td class="org-right">9</td>
</tr>

<tr>
<td class="org-left">x<sub>7</sub></td>
<td class="org-right">-3</td>
<td class="org-right">1</td>
<td class="org-right">24</td>
</tr>

<tr>
<td class="org-left">z</td>
<td class="org-right">-3</td>
<td class="org-right">6</td>
<td class="org-right">-6</td>
</tr>
</tbody>
</table></li>

<li><p>
The value of the function is now -6.  We can see that the right variable
to decrease now is x<sub>1</sub>.  So, we do the constraints again.  Here, the
blocking constraint is x<sub>6</sub>, so then we get this tableau:
</p>

<table>


<colgroup>
<col  class="org-left">

<col  class="org-right">

<col  class="org-right">

<col  class="org-right">
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-right">x<sub>6</sub></td>
<td class="org-right">x<sub>5</sub></td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">x<sub>3</sub></td>
<td class="org-right">-1</td>
<td class="org-right">2</td>
<td class="org-right">12</td>
</tr>

<tr>
<td class="org-left">x<sub>4</sub></td>
<td class="org-right">-1</td>
<td class="org-right">3</td>
<td class="org-right">10</td>
</tr>

<tr>
<td class="org-left">x<sub>2</sub></td>
<td class="org-right">1/3</td>
<td class="org-right">1/3</td>
<td class="org-right">4</td>
</tr>

<tr>
<td class="org-left">x<sub>1</sub></td>
<td class="org-right">-1/3</td>
<td class="org-right">1/3</td>
<td class="org-right">3</td>
</tr>

<tr>
<td class="org-left">x<sub>7</sub></td>
<td class="org-right">1</td>
<td class="org-right">-5</td>
<td class="org-right">15</td>
</tr>

<tr>
<td class="org-left">z</td>
<td class="org-right">2</td>
<td class="org-right">1</td>
<td class="org-right">-15</td>
</tr>
</tbody>
</table>

<p>
The stopping condition is when both variables on top of the columns have
coefficients that are positive, so you can't improve the function value.
</p></li>
</ol>

<p>
If you have more than one variable that will decrease the function, you can
choose any variable to decrease, and you will always get to the correct
solution.  However, some choices will be faster than others.
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Stephen Brennan</p>
<p class="date">Created: 2016-10-10 Mon 20:36</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
