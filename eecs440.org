#+TITLE: EECS 440 Notes
#+AUTHOR: Stephen Brennan
#+SETUPFILE: config.setup

* 2015-09-29 Tuesday

** Support Vector Machines

   - SVM's are a rather new method in machine learning.
   - Produced by multiple groups of people in ML, Statistics, and Operations
     Research who basically converged on this idea.
   - Three fundamental ideas:
     - Linear discriminants (we saw this with a perceptron)
     - Margins
     - Kernels

*** Linear Discriminants

    What is?
    - \(sign(5x_1 + 3x_2 - 4)\)
    - \(sign(x_2 - 4x_1^2)\)
    - \(sign(x_2 - e^{-x^2})\)

    This was a trick question.  They all are.  In machine learning, we aren't
    particularly interested in the $x$ variables -- they are given to us.  We are
    interested in the coefficients $w$.  So when we talk about linear
    discriminants, we mean linear in terms of $w$, not $x$.
    - In general, we talk about a linear discriminant in this form:
    - \(\vec{w} \cdot \phi(\vec{x}) + b = 0\)

    When we talked about perceptrons not being able to discriminate XOR, it was
    not entirely true.  If you make a transformation $\phi$ of the variables, you
    can define a line that discriminates XOR perfectly.  This transformation is
    \(\phi(x_1, x_2) = x_1 + x_1 x_2 \).  You can see the diagram in the slides.

*** Margins

    Given a training sample, you can frequently define tons of linear
    discriminants that cleanly separate the training sample.

    What is the best one?  Probably the one that is mostly in the middle of the
    training positives and negatives.

    We define *margins* as the amount you could slide the discriminant in until
    you reach a point.  With this, we can say that we would define our best SVM
    classifier as the one with the /maximum margin/.

    When you are in the "/input feature space/", (i.e. \(\phi(\vec{x})=\vec{x}\)),
    this is called a "linear SVM" (which is a bit confusing, but this means
    linear in the feature space, as opposed to the coefficient space -- in
    essence, it's a linear, linear SVM).

** Why it makes sense

   - Computationally easy to come up with.
   - Requires relatively few data points.

** Calculating the Margin

   If you have a classifier \(\vec{w}\cdot\vec{x} + b = 0\), and your first positive
   datapoint is at \(\vec{w}\cdot\vec{x} + b = 1\), and your first negative point is
   \(\vec{w}\cdot\vec{x} + b = -1\).  Cool.

   Note that $\vec{w}$ is perpendicular to \(\vec{w}\cdot\vec{x} + b = 0\),
   - Why? Pick any $\vec{u}$, $\vec{v}$.
   - \(\vec{w}\cdot(\vec{u}-\vec{v}) = \vec{w}\cdot\vec{u} - \vec{w}\cdot\vec{v} = (-b) -
     (-b) = 0\)
   - Also, $\vec{w}$ is perpendicular to the plus and minus planes.

   There's some stuff here that I didn't follow.  Essentially, he got to the
   point where maximizing the margin is equivalent to maximizing
   \(\frac{2}{||\vec{w}||}\), which is equivalent to $\frac{||\vec{w}||^2}{2}$.

** Problem Formulation

   What we want to do:
   - While respecting the labels of training examples:
     - \(\vec{w}\cdot\vec{x_i} + b \ge 1 \text{ if } y_i = 1\)
     - \(\vec{w}\cdot\vec{x_i} + b \le -1 \text{ if } y_i = -1\)
   - Or, more compactly:
     - \(y_i(\vec{w}\cdot\vec{x_i} + b) \ge 1\)

   Problem formulation:
   - \(\min \frac{1}{2} ||\vec{w}||^2\), s.t.
   - \(y_i(\vec{w}\cdot\vec{x}_i + b) \ge 1\)

   Whoa, it's quadratic programming!  There are algorithms to do this, and since
   the constraints are linear, the feasible region is convex, and therefore we
   have a global minimum!

   Unfortunately, if the data are not linearly separable, then our quadratic
   programming algorithm will come back saying that the problem is not feasible.
   So, we need to allow for misclassification by adding slack variables:

   - \(\min \frac{1}{2} ||\vec{w}||^2\), s.t.
   - \(y_i(\vec{w}\cdot\vec{x}_i + b) + \xi_i \ge 1\)
   - \(\xi_i \ge 0\)

   Sadly, this doesn't solve the problem.  A trivial solution that always
   exists: \(\vec{w}=\vec{0}\), and \(\xi_i=0\)!  We need to add into the objective
   function a term to simultaneously minimize the number of errors in the
   classifier.  Unfortunately, the number of errors is not a differentiable
   quantity, but \(\sum_i \xi_i\) is a good proxy and is differentiable.  Hooray!

   - \(\min \frac{1}{2} ||\vec{w}||^2 + C\sum_i \xi_i \), s.t.
   - \(y_i(\vec{w}\cdot\vec{x}_i + b) + \xi_i \ge 1\)
   - \(\xi_i \ge 0\)

   In this program, $C$ is a constant that quantifies the "tradeoff" between the
   generalization and the error.  The value tends to be problem and dataset
   specific, so you need to determine it via cross validation in your code.

   Notice that in an optimal solution \(\xi_i = \max(0,1-y_i(\vec{w}\cdot\vec{x}_i +
   b))\).  We can lift this into the objective function to remove the
   constraints:

   \begin{equation}
     \min \frac{1}{2} ||\vec{w}||^2 + C\sum_i [(1 - y_i(\vec{w}\cdot\vec{x}_i + b))]^2
   \end{equation}

   We don't need to use quadratic programming now, we can just solve an
   unconstrained problem, which is much simpler and more efficient.  Yay us.

** Course Project Notes

   Sign up sheet: "I want to do project", "I have an idea for a project."

   Email with idea for project by 10/9.  Official project start date 10/23.

   Significant work - treat like a real research project.  Start with a
   hypothesis of what you'd like to show.  You may be algorithm-specific, or
   application specific:
   - I have a better version of algorithm X.
   - My algorithm X is better for application Y.

   Preferably, you should integrate the project with your own research.  You may
   also be able to integrate the project with other course projects as well.

   In terms of grading, the project is 35% of your grade:
   - 25% is a writeup, in a conference format, of your problem, experiments,
     observations, and interesting directions.
   - 10% is a presentation to the class on what you did, during finals week.

   Website will have example projects, and a document with requirements for the
   writeup document.

   *The writeup is due Dec 10!  /No extensions!!/*

* 2015-09-24 Thursday

  Recall: we want to compare ML algorithms.  Our steps for comparing test data:
  1. Determine the sampling distribution.
  2. Estimate parameters using MLE.
  3. Use an approximation distribution if necessary.
  4. Come up with a C% confidence interval.

  Our "Issue #1" was "what do we know about the true performance of a
  classification algorithm, given that we've tested it on a test set?"
  - We determined that we could come up with a confidence interval for an
    observed test statistic.

** Issue 2.1

  Our next issue, #2.1, is that we have a conjecture "classifier A is better
  than algorithm B on a certain type of data."  We would like to evaluate
  whether this conjecture is true.  We can do this with statistical hypothesis
  testing.

  We want to look at the random variable \(F = Err_{C_1} - Err_{C_2}\).  What can we
  say about the sampling distribution of $F$?  Assuming that the error
  distributions are Gaussian, the distribution of $F$ is going to be Gaussian
  also.  The MLE parameter estimates:
  - \(E(F) = e_{S,C_1} - e_{S,C_2} = \left(\frac{r_1}{n_1} - \frac{r_2}{n_2}\right)\)
  - \(V(F) = V(Err_{C_1}) + V(Err_{C_2}) = \frac{e_{S,C_1}(1-e_{S,C_1})}{n_1} + \frac{e_{S,C_2}(1-e_{S,C_2})}{n_2}\)

  Comparing classifiers: hypothesis testing:
  - Establish your "null hypothesis."
    - You will reject this hypothesis with high probability.
    - You presume it is true until the test shows otherwise.

  This test assumes that the two classifiers were evaluated on independent test
  data.

  Example:
  - On a test set with (*0 examples a decision tree misclassifies 20 examples.
    on the same test set, a neural network misclassifies 25 examples. Are these
    two classifiers actually different on this problem?
  - \(F=\frac{r_1}{n_1} - \frac{r_2}{n_2} = 0.05\)
  - \(V(F) = 0.2(1-0.2)/100 + 0.25 (1-0.25) / 100 = 0.0016 + 0.001875 = 0.0034\)
  - Standard deviation is about 0.05.
  - 0 is definitely within the 95% confidence interval, so we cannot reject the
    null hypothesis (that they are the same)

** Issue 2.2

   This issue is critically different from 2.1.  In 2.1, we had two classifiers
   -- who knows where they came from, but we want to compare those two
   classifiers' performances.  In this case, we have two learning algorithms,
   and we want to compare the performance of the *algorithms*, not a particular
   *classifier* produced by an algorithm.

   In order to do so, we must find the expected value of an algorithm's error
   rate.  To do this, we must take the average over all classifiers, produced by
   all possible training sets.  We usually estimate this by doing $n$-fold
   validation instead of actually finding all possible training sets from the
   population.

   We can do /pair testing/, where we evaluate the algorithms on the same folds,
   and then compare the difference between their error rates.  Or, we can do it
   independently, on separate folds, and compare their error rates.  But this
   method gives you a bigger variance.

   When you compare the difference of error rates, you want to know what the
   sampling distribution is.  The sampling distribution looks Gaussian, but not
   quite.  Instead, it's a $t$ distribution.  If $k$ (the number of folds) was
   very large, we could use the Gaussian, but instead we have to use $t$
   distribution, with $k-1$ degrees of freedom.  Here are parameter estimates:
   - Mean (\delta): the average difference of error rates across $k$ folds.
   - Standard Deviation:
     \begin{equation}
       s = \sqrt{\frac{\sum_{i=1}^k (\delta_i - \delta)^2}{k(k-1)}}
     \end{equation}
   - The standard deviation is adjusted to make the distribution narrower, and
     put more mass in the tails!

** One-way ANOVA

   - For comparing >2 algorithms.
   - Why not just do pairwise hypothesis tests?
     - The results may not be consistent (i.e. transitive)
     - Multiple hypotheses result in lost confidence, so you'd need to correct
       your P-value/confidence interval.
       - EG: with 10 95% CI's, you only have 60% confidence that the true values
         of all 10 parameters are within the range.
   - ANOVA looks at the "between means" variance.
   - Essentially, it seems like a generalized $t$ test (gives the same result as
     a $t$ test for two distributions).

** Sign Test

   - Simpler than $t$ test with fewer assumptions.
   - For each fold, note which algorithm had better performance.
   - Use binomial null hypothesis, where p=0.5.

** Mann-Whitney-Wolcoxon Signed-Rank Test

   - What if a classifier produces confidence estimates?
   - If we can rank the predictions, we can calculate a $U$ statistic based on
     the ranks:
     \begin{equation}
       U_1 = \sum_i R_{1,i} - \frac{n_1(n_1 + 1)}{2}
     \end{equation}
   - For large enough samples, you can approximate $U$ with a normal
     distribution.
   - AUC is actually a normalized version of $U$!

** Bootstrap

   - All previous methods relied on knowing the sampling distribution of the
     statistic we are interested in.
   - The bootstrap is a procedure where we get the properties of the statistic
     using /empirical resampling/ from the observations.

   Example:
   - Suppose we have a set of iid examples and we want to get a C1 for F1 score.
   - Repeatedly draw an equal sized sample (with replacement) from our test
     examples, and measure F1.
   - This creates an empirical sampling dsitribution.
   - Then, go back to the original data, measure F1, and ask how unusual that is
     in the empirical distribution.

   Weird... ¯\_ (ツ)_/¯

   Pros: very easy, few assumptions, good for complex things.

   Cons: finite sample behavior is not very well understood.

** Is there a best learning algorithm?

   - No
   - No Free Lunch theorem!
     - In the expectation over all learning algorithms, they will perform
       equally.
     - Wolpert 1996: "The lack of a priori distinctions between learning
       algorithms."
     - For any specific application, you can have a "best" algorithm.  But
       overall, no.

* 2015-09-22 Tuesday

** ANNs, Continued

*** Cascade Correlation (Learning the Structure of an ANN)

    - No textbook sections on this, ask for paper.
    - Start with a single perceptron.  Train and find "residuals".
    - Now, add a new perceptron that feeds into the original one.
    - Train it to feed the "residuals" into the original perceptron.
      - Hold the original perceptron's training constant.
    - Continue adding perceptrons that correct for the "residual" of the
      previous iteration.
    - This essentially does a Taylor series approximation of the underlying
      function.
    - It is an instantiation of a more general technique called "gradient
      boosting".

*** Interpretation of Hidden Units

    - Unlike Decision trees, the ANN structure is very opaque.
    - Difficult to interpret what it is doing.
    - One way is to look at the last layer of the ANN (the last perceptron) and
      see what it's doing.  You could even assign labels to each of the inputs
      for whatever concept you may believe they represent.

*** How Many Hidden Units?

    - Some work shows that it is better to start with a network that is too big.
      - Train until the error on the validation set grows.
      - Then look at the weights associated with edges, and prune the hidden
        units that don't actually contribute.

*** Recurrent ANNs

    - So far, we've looked at ANNs that feed forward.
    - There are also networks with loops, called "recurrent neural networks"
      - This gives ANNs a "memory" of previous inputs.
      - They are much more of a dynamic structure
    - A recurrent ANN architecture with /rational weights/ has computational
      power equivalent to a Universal Turing Machine!!!!!
      - However, this is ridiculously hard to train (ya don't say...)
      - Very prone to overfitting.

*** Pros/Cons of ANNs

    Pros:
    - Very expressive hypothesis space
    - Very useful for classification, regression, density estimation
    - Builds useful representations "automatically"

    Cons:
    - Easy to overfit.
    - Slow to train, require many examples.
    - Doesn't easily handle nominal data.
    - Opaque

** Comparing Learning Algorithms

*** Issue 1
    - Suppose we collect test data and evaluate a classifier.  Accuracy=$x$.
    - Then, someone repeats the experiment with another set of test data from the
      same problem, independent of the first set.
      - What can we say about the accuracy here?

*** Issue 2
    - Suppose now we have two different classifiers $A$ and $B$.  We measure
      their accuracies on a test set, and get $x$ and $y$, and $x > y$.  Does
      this mean $A$ is better than $B$ in this problem?
    - Or how about doing this with completely different algorithms?
      - If we repeated this experiment, we would get new $x'$ and $y'$.  Would
        we find that $x' > y'$ again?

*** It's All Just Statistics!

    - We're just looking at estimating the "true value" of a metric on the basis
      of a small sample.
    - Just like statistics!
    - *Definition:* Data Distribution: assume there is an unknown, underlying
      probability distribution, $D$, from which /unlabeled/ examples ($x$) are
      being sampled without replacement.
      - I.I.D.
    - *Definition:* Sample Error Rate: The fraction of examples in our test
      sample on which the learned classifier disagrees with the target concept.
      \begin{equation}
        e_s = \frac{1}{n} \sum_x \delta(y_x, \hat{y}_x)
      \end{equation}
    - *Definition:* True Error Rate: The probability that the learned classifier
      will make a mistake on a random example drawn from $D$.:
      \begin{equation}
        e_D = Pr_{x~D}(y_x \ne \hat{y}_x)
      \end{equation}
    - For problem #1, we want to know how are $e_S$ and $e_D$ related.
    - *Definition:* Sampling Distribution:
      - Suppose we perform a random experiment lots of times and record the
        outcome.
      - Call the random variable associated with the outcome $O$.
      - Suppose we then plot a frequency histogram of $O$.
      - something something something (see slides)
    - We'd like to get at the sampling distribution of the "error rate" r.v.,
      but we'll start with something easier.
    - Let $R$ be an rv denoting the number of errors in an evaluation
      experiment: (he changed slides too quick)
      - Sampling Distribution of $R$
        - Suppose we run $k$ experiment with test samples of size $n$
        - In the $i$th experiment our learned classifier makes $R=r_i$ errors.
        - We'll pot a frequency histogram of $R$.
        - What will it look like for $k$ large?
        - We have a Binomial distribution.  In the limit, this actually
          converges to a normal distribution.
        - This means we can infer the error rate $e_D$ (since $\mu=np$, $\sigma =
          ne_D(1-e_d)$)
      - If we do one trial and find that there are $r$ errors on $n$ examples, a
        good parameter estimate for $e_D$ is $\frac{r}{n}$.  Why?
      - This is a maximum likelihood estimation.  It is the parameter that
        maximizes the probability of the data.
    - *Definition:* Estimation Bias: Estimation bias of an estimator $Y$ for
      parameter $p$ is $E(Y)-p$.
      - If it has 0 bias, it converges asymptotically to the true value.
      - MLE has 0 estimation bias.
    - This is getting to some good math, but I can't summarize it in my notes
      right now if I want to understand it.  See slides.
    - Summary for Issue 1:
      - Determine sampling distribution of measure.
      - Estimate sampling distribution parameters using MLE on test set.
        - If necessary, approximate using standard distribution such as
          Gaussian.
      - Use tables to determine C% CI.
        - Usually use C=95
        - The true measure will lie in that interval with C% probability.

* 2015-09-17 Thursday

** Tradeoffs of Neural Networks

   - Lots of DoF!
     - Topology
     - Parameters
   - Easy to overfit.

** Training ANN

   We'll pretend that the network topology is already decided.  Here is the
   setup:

   \begin{equation}
     D = \left( \begin{array}{ccccc}
           x_{11} & \cdots & x_{1n} & -1 & y_1 \\
           \vdots & & \vdots & \vdots & \vdots \\
           x_{m1} & \cdots & x_{mn} & -1 & y_m
         \end{array} \right)
   \end{equation}

   - Want to find parameters $\vec{w} = (w_1, w_2, \cdots, \sigma)$.
   - Such that we minimize the "loss" function $L(\vec{w})$.
   - We can't use the sign function because it's not differentiable.
   - We can't use the dot product approximation.
   - Instead we use a sigmoid function $y = (1 - e^{x})$ I think.

   To train, we use Backpropagation!  This is gonna be fun.
   - Feed examples forward through the network.
   - Do layer-wise gradient descent starting at the output layer.

*** Backpropagation

    - Let $x_{ji}$ be the ith input to unit j.
    - Let $w_{ji}$ be the parameter associated with $x_{ji}$.
    - Let $n_j = \sum_i something$
    - Next up is the derivation of the derivative of the loss function for the
      output layer.  It's easy to follow, and I can't keep up with typing the
      math.  Check the slides!

    Backpropagation for hidden layers.
    - A perceptron $j$ only can affect the output from its downstream
      perceptrons, which we denote as $Downstream(j)$.
    - We can compute the derivative of the loss function with respect to the
      inputs of this perceptron, $\frac{dL}{dn_j}$, by computing the sum of
      $\frac{dL}{dn_k} \frac{dn_k}{dn_j}$ for all the $k\in{}Downstream(j)$.
      Excitingly, we already have $\frac{dL}{dn_k}$, since $k$ is dowstream of
      $j$,
    - The math is on the slides again, cause I'm not typing this stuff.  Still
      pretty easy to follow.

*** Example

    Consider a neural network with 2 input units, 2 hidden units, and 1 output
    unit, and all weights initialized to 1, with the bias set to zero.  Using
    squared loss, show the weights after the first backpropagation update with
    these examples.

    We have the inputs labelled 1 and 2, and then the two internal nodes labeled
    3 and 4, and the output node labeled 5.  Weights and x's are labeled
    accordingly.

    | x_1 | x_2 | f | $\hat{f}$ |
    | 0  | 0  | 0 | 0.731     |
    | 0  | 1  | 1 | 0.812     |

    Now that we have the initial outputs of the network, we can compute the
    derivatives for each example, and once we have all the derivatives we add up
    all the derivatives and compute the next step.

    Example 1:
    - Output layer: \(\frac{dL}{dw_{53}} = (0.731) (1 - 0.731) (0.5) (0.731 - 0) = 0.0719\)

    Example 2:
    - Output Layer: \(\frac{dL}{dw_{53}} = (0.812) (1 - 0.812) (0.731) (0.812 - 1) = -0.021\)

    Update:
    - \(w_{53}' = 1 - \eta (0.0719 - 0.021) = 0.949\) (assuming $\eta = 1$ for example).

** Overfitting in ANNs

   - They are very prone to overfitting, due to the large amount of parameters.
   - Can create very nonlinear decision surfaces.
   - You can impose a simple structure on the network, but then the network may
     not be capable of representing the true decision boundary.
   - Some strategies for controlling overfitting:

*** Weight Decay

    Add a "weight decay term" to keep the weights from growing:

    \(L_{OC}(\vec{y}, \hat{\vec{y}}, \vec{w}) = L(\vec{y}, \hat{\vec{y}}, \vec{w}) + \gamma \sum_i \sum_j w_{ji}^2\)

    If you have a large $\gamma$, your solution will tend to $w_{ji}$'s will tend toward
    zero, to minimize the effect of $\gamma$.  So it seems careful choice of $\gamma$ is
    pretty important.

** Implementation Issues

   You should standardize your inputs to zero mean, unit variance, so that your
   units don't have a massive effect on the network.

   Nominal features: you need to re-encode it.  You could do 1 of N input units.
   Or you could do logarithmic encoding, where each input is a binary code.

* 2015-09-15 Tuesday

** Famous Dead People

   - George Boole - father of Boolean algebra.
   - Someone else - neuroscience.
   - Frank Rosenblatt (may not be dead) - artificial neurons.

** History

   - We want "artificial intelligence."
   - Human brain is intelligent.
   - Try to simulate the structure of the brain to achieve intelligence

** Perceptron / Linear Threshold Unit

   - Has weighted ($w_i$) inputs ($x_i$).
   - Has Activation Threshold $\sigma$
   - Activation function is:

     \begin{equation}
       h(\vec{x}; \vec{w}, \sigma) = \left\{
       \begin{array}{ll}
         +1 & \text{if } \vec{w} \cdot \vec{x} \ge \sigma \\
         -1 & \text{else} \\
       \end{array}
       \right.
     \end{equation}

   - The parameters of the perceptron are $\vec{w}$ and $\sigma$.
     - There aren't really parameters of the decision tree algorithm, just the
       structure of the tree.

   - Example evaluation for perceptron $\vec{w}=(1,2)$, $\sigma=0.5$:

     | $x_1$ | $x_2$ |  h |
     |    0 |    0 | -1 |
     |    0 |    1 |  1 |

   - So, the question remains, how do we train them?

*** Training a Perceptron

    - Loss function: $L(\vec{w},\sigma)$
    - Measures the difference between the current estimates of $y$ ($\hat{y}$),
      and the true $y$ (which is known), over all training examples.
    - Our goal is to minimize the loss function with respect to $(\vec{w}, \sigma)$.
    - Notations:
      - Training data: (he changed the slide too quick)
    - Common loss function is "squared loss":
      \begin{equation}
        L(\vec{w}) = \frac{1}{2} \sum_{i=1}^m (y_i - \hat{y}_i)^2
                   = \frac{1}{2} \sum_{i=1}^m (y_i - sign(\vec{w}\cdot\vec{x}_i))^2
      \end{equation}
    - Sign function is not differentiable, so we'll replace it by dot product.
      \begin{equation}
        L(\vec{w}) = \frac{1}{2} \sum_{i=1}^m (y_i - \vec{w}\cdot\vec{x}_i)^2
      \end{equation}
    - Calculate gradient wrt $\vec{w}$
      \begin{equation}
        \frac{dL}{d\vec{w}} = \sum_{i=1}^m (y_i - \vec{w} \cdot \vec{x}_i)(-\vec{x}_i)
      \end{equation}
    - Parameter Update:
      \begin{equation}
        \vec{w} \gets \vec{w} - \eta \frac{dL}{d\vec{w}}
      \end{equation}
    - We can use gradient descent
      - Loss function is differentiable.
      - Loss function is bounded below by 0.
      - Loss function is convex (proof???)
      - This means there is a well-defined minimum for the loss function.
      - And, gradient descent will find it!
    - However, just cause the gradient descent converges, doesn't mean that it
      will converge to 0, since the true concept is not necessarily linear.
    - Stochastic G.D:
      \begin{array}{l}
        \frac{dL}{d\vec{w}} = (y_i - \vec{w} \cdot \vec{x}_i)(-\vec{x}_i) \\
        \vec{w} \gets \vec{w} - \eta \frac{dL}{d\vec{w}}
      \end{array}
      - This is done for each example instead of as a group.
      - Since the loss function is convex, it will converge to the same thing in
        the limit.
      - But the stochastic procedure will procede differently and maybe converge
        at a different speed.
      - Stochastic seems to give initial examples more "weight" in the direction
        of the search.
      - Stochastic is better for "online" learning, and for very large datasets.

*** More on Perceptrons

    - Geometry of the perceptron:
      - In one dimension, it is a step function.
      - In two dimensions, the separating surface is a line.
      - In three dimensions, the separating surface is a plane.
      - So, in general, the decision surface is a hyperplane.
    - Loss function is 0 when the surface completely separates the examples with
      no errors.  It is non-0 when there are some wrong ones.
    - Linear separability is whether or not a dataset can be separated by a
      linear function without error.
    - The perceptron is not nearly as powerful as a decision tree (can't
      separate things like exclusive or).
    - So, it is more resistant to overfitting.  (which we will quantify later)
    - It can do some logic:
      - Conjunctions:
        \begin{array}{l}
          x_1 \land x_2 \land x_3 \leftrightarrow y \\
          1 \cdot x_1 + 1 \cdot x_2 + 1 \cdot x_3 \ge 3
        \end{array}
      - At least $m$-of-$n$:
        \begin{array}{l}
          (x_1 \land x_2) \lor (x_1 \land x_3) \lor (x_2 \land x_3) \leftrightarrow y \\
          1 \cdot x_1 + 1 \cdot x_2 + 1 \cdot x_3 \ge 2
        \end{array}
    - But not all:
      - Complex disjunctions
      - Exclusive or!!
    - Can fix this by using more perceptrons hooked up to each other.
    - The neural network for exclusive or looks remarkably similar to the logic
      gate circuit for XOR :D
    - It involves a "hidden" layer that isn't part of the output.

*** Feedforward Network Topology

    - Essentially, a directed acyclic graph of perceptrons.
    - But, it may be that you have to follow the layer structure.
    - Representation ability
      - Every boolean function can be represented by a network w/ one hidden
        layer.
      - Every bounded continuous function can be represented by a network with
        one hidden layer.
      - Every function in R^n can be represented by a network with two hidden
        layers.
      - Woah.
    - This gives you a tradeoff...
      - You end up with the possibility for a lot of overfitting (many degrees
        of freedom and high representation ability).
      - It also takes a long time to train these networks if they are complex.

* 2015-09-10 Thursday

** Evaluation Methods and Metrics

   How do you figure out if your algorithm is "good"?

   Goal: find a measure *expected future performance* of the learning algorithm
   for some problem.  How?

   Idea:
   - Separate available data into sets for training and evaluation.
   - The examples for evaluation will be new to the learned classifier.
   - Do this lots of times to get reliable estimates.
   - The sets should be "separate" at least in the sense of independently
     chosen, if not disjoint examples.

*** n-fold Cross Validation

    - Generally, the number of examples is limited.
    - Want to train on sets that are as large as possible.
    - Divide set into $n$ separate sets.
      - For each set, withhold it for testing, and train on the remaining sets.
      - Then evaluate the classifier on the testing sets.
    - Special case of $n$-fold cross validation: Leave-one-out
      - $n$ examples, $n$ folds.
      - Only really useful if you have a few examples.
      - Called "jackknife" in statistics literature.
    - Stratified cross validation
      - Same as $n$-fold cross validation, but you sample folds such that the
        proportions of class labels is preserved in each fold.
      - More stable performance estimates.
      - Implementation:
        - Put $pos$ positive examples in one list, and $neg$ negative examples
          in another.
        - Randomly shuffle the lists.
        - Put the first $pos/n$ positives in fold 1, the next into fold 2, etc.
        - Repeat for negatives.
        - Assign leftover examples randomly.

*** Metrics for Classification

    Contingency Table

    |              | Positive (TC)                | Negative (TC)                |
    | Positive (C) | True Positive (TP)           | False Positives (FP, Type I) |
    | Negative (C) | False Negative (FN, Type II) | True Negative (TN)           |

    Can compute all metrics from the contingency table.

    - Accuracy: most commonly used measure for comparing algorithms.
      \begin{equation}
        \text{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN}
      \end{equation}
      - Simply the fraction of examples that are correctly classified.
      - There are many problems with accuracy.
        - Skewed class distribution: eg, if 99% animals aren't lions, a
          classifier with 99% accuracy would just predict "not lion".  And it
          would kill you next time you see a lion.
        - Differential misclassification costs: some types of errors (FP or FN)
          are more serious for an application than others (eg screening for a
          disease).  Accuracy treats them equally.
    - Weighted Accuracy
      \begin{equation}
        \text{WAcc} = \frac{1}{2}\left(\frac{TP}{Allpos} + \frac{TN}{Allneg}\right)
                    = \frac{1}{2}\left(\frac{TP}{TP + FN} + \frac{TN}{TN + FP}\right)
      \end{equation}
      - First part is the "true positive rate" (how many positives are correctly
        identified)
      - Second part is the "true negative rate" (how many negatives are
        correctly identified)
    - Precision
      \begin{equation}
        \text{Precision} = \frac{TP}{TP + FP}
      \end{equation}
      - Sometimes, the "positive" case is all you're interested in.
      - This measures "of all the examples classified positive, how many were
        actually positive?"
    - Recall / True Positive Rate / Sensitivity
      \begin{equation}
        \text{Recall} = \frac{TP}{TP + FN}
      \end{equation}
      - This quantifies "of all the positive examples, how many were correctly
        classified?"
    - Specificity
      \begin{equation}
        \text{Specificity} = \frac{TN}{TN + FP}
      \end{equation}
      - Conterpart of recall for the negative class.
    - F1
      \begin{equation}
        \frac{1}{F1} = \frac{1}{2} \left( \frac{1}{Precision} + \frac{1}{Recall}\right)
      \end{equation}
      \begin{equation}
        F1 = \frac{2}{\left( \frac{1}{Precision} + \frac{1}{Recall}\right)}
      \end{equation}
      - Combines precision and recall into single measure.
      - Not necessarily a good idea, but widely used.

*** Learning Curves

    - Frequently it's useful to plot metrics as a function of sample size.
    - Provides insight into how many examples the algorithm needs to be
      effective.

*** Metrics with Confidence Measures

    - Many learning algorithms produce classifiers or models that provide
      estimates of how confident they are.
    - Can use this to create Precision/Recall curves or Receiver Operator
      Characteristic curves.
    - Precision/Recall curves:
      - plot precision, recall as you change threshold.
    - ROC graphs
      - plot FPR x , TPR y as you change threshold.
      - Random guessing is a diagonal line.
        - Also majority class classifier.
        - Good classifier mst be above the diagonal.
      - Monotonically increasing.
      - Can be misleading if class distribution is too skewed.
        - Use PR instead.
      - Frequently use AUC as statistic.
* 2015-09-08 Tuesday

** Review:

   - Decision trees: trees where internal nodes are tests on attributes, and
     leaves are class labels.
   - Construct them by choosing attributes which give the most information.
   - Measure this information with entropy, mutual information ("information
     gain").
   - ID3 algorithm is the formal algorithm for applying mutual information to
     constructing decision trees.

** Generalizing ID3

   - What about multiple valued attributes (more than 2-valued)?
     - Mutual information still applies to $v$-valued finite, discrete
       variables.
     - You simply have the internal node for that attribute have $v$ children
       instead of 2.
     - However, the maximum mutual information for a $k$ valued variable is
       $\log{k}$, so the IG function is biased towards attributes with many
       values.
     - Can normalize by dividing by $H(X)$, the entropy of the attribute itself.
       - *Question:* why is this better than dividing by $\log{|X|}$, e.g., the
         maximum overall entropy of $H(X)$?
       - In essence, this division gives you a quantity that answers the
         question "what fraction of this variable's entropy contributes
         information about the class label?"
   - Continuous Attributes
     - Continuous variables have entropy defined on them, but it's useless for
       making a decision in a tree.
     - Need to "bin" the attribute ($X \le v$ or $X \ge v$).
     - You only need to consider values for $v$ that separate different class
       labels in the training set.
       - This is still problematic for large training sets, as we'll see on our
         programming assignment.

   Example

   | Color | Area | Shape    | Class Label |
   | red   |  0.1 | circle   |           1 |
   | red   |  0.7 | square   |           0 |
   | red   |  0.4 | triangle |           1 |
   | blue  |  0.2 | triangle |           1 |
   | blue  |  0.6 | circle   |           0 |
   | blue  |  0.8 | square   |           0 |
   | green |  0.4 | square   |           0 |
   | green |  0.3 | triangle |           0 |
   | green |  0.3 | circle   |           0 |

   1. First, compute H(Y), which is $H(\frac{1}{3})$ (as a shorthand).
   2. Then, compute H(Y|Color):

      \begin{equation}
      H(Y|Color) = p(Color=red)H(Y|Color=red) + p(Color=blue)H(Y|color=blue) + p(Color=green)H(Y|Color=green)
      \end{equation}

      \begin{equation}
      H(Y|Color) = \frac{1}{3}H(\frac{1}{3}) + \frac{1}{3}H(\frac{1}{3}) + \frac{1}{3}\times 0
      \end{equation}

      \begin{equation}
      H(Y|Color) = \frac{2}{3}H(\frac{1}{3})
      \end{equation}

   3. We can use this to compute the information gain of Color.

      \begin{equation}
      IG(Color) = H(Y) - H(Y|Color) = \frac{1}{3} H(\frac{1}{3})
      \end{equation}

   4. Conveniently, this is the same as the information gain of Shape.

   5. For area, if we sort the training set by Area, we find the cutoffs 0.25,
      0.35, and 0.5.  Then we can compute H(Y|Area,v) for each cutoff v.

      $H(Y|Area\le0.25) = \frac{2}{9}\times 0 + \frac{7}{9} H(\frac{1}{7})$, so IG(Area\leq 0.25) = 0.4583

      etc for each cutoff

   6. You choose the best IG, and use that for the root node.  Then continue to
      do this for each child node.

** Overfitting

   - Given enough features, ID3 will create a tree that fits your data perfectly.
     - Enough features = enough that there are no contradictory examples.
   - Overfitting is an issue.

   - What is overfitting?  Making your model too specific to your training
     examples, and not general enough to be applied well to new data.

   - Strictly, if a concept $h$ has:

     - Higher performance on the training examples, but
     - Lower performance on the whole dataset

   - Than some other concept $h'$, then we say that $h$ has overfit the training
     data.

*** Controlling Overfitting

    - Can introduce a restriction on the hypothesis space, to prevent overly
      complex hypotheses from being learned.
    - Early Stopping
      - Standard ID3 algorithm stops when IG(X)=0 for all X.
      - Instead, stop when IG(X) \leq \epsilon, for some chosen \epsilon.
      - This is sensitive to your parameter choice for \epsilon.
      - It's easy to implement, but doesn't work well in practice.
    - Greedy post-pruning
      - Hold aside some training examples at the start.
      - Do your training procedure on the remainder (allowing it to overfit if
        it wants).
      - Then, do a /greedy pruning/ algorithm on your model.
* 2015-09-01 Tuesday

  HW1 due tonight at midnight.  HW 2 out today.  Read Ch. 3 in Mitchell.

** What is "Machine Learning?"

   - Machine = autonomous system, with no (or limited) human intervention.
   - Learning?
     - System changes after an experience, so that it can work more effectively
       next time it does the task.
     - We want the system to learn how to do /related/ tasks better too.
   - Specification for a learning system:
     - Given: Task goal, performance measure P, and examples E
     - Produce a *concept* that is good wih respect to P on /all/ examples of
       the task.
   - Example: learn to play chess
     - Perforance measure = games won/lost
     - Examples = games played
     - Concept?  Probably a function mapping a current board state to a move to
       play next.
   - Two phases: learning/training, and evaluation/testing
     - (In the evaluation phase, you want to evaluate on new examples that you
       haven't trained on).
   - Batch learning: one learning phase, with a large set of examples, followed
     by a testing phase.
   - Online learning: examples arrive one at a time (or in small groups);
     learning and evaluation phases iterate.
   - Learning systems need to have some sort of constraint.  Memorizing all the
     examples is probably the best strategy, but we know that this doesn't
     represent learning the underlying concept.

*** Inductive Generalization

    - In all learning problems, need to reason from specific examples to a
      general case.
    - (this is the reverse of deductive reasoning, where you reason from the
      general case to the specific case)
    - Target concept = the underlying concept that the system is trying to
      learn.  EG, Gary kasparov's head.
    - Typically, the performance measure quantifies the difference between
      current and target concepts.
    - Hypothesis space - all concepts the learning system will consider
      (e.g. all possible combinations of animal properties)
    - Hopefully, target concept is in the hypothesis space.
      - But can't include every possible hypothesis in your space.
      - The size would be huge.
      - You would end up memorizing, not learning.
    - This is the idea behind "No Tabula Rasa" (blank slate) learning.  There
      has to be some sort of restriction on hypothesis spaces.
    - Inductive Bias
      - Assumptions used to limit the hypothesis space are the inductive bias.
      - The more assumptions, the stronger the bias.
      - It can even be quantified (later)

*** Learning Settings

**** Supervised Learning

     - Examples are annotated by a teacheer or oracle.
     - Learning system just finds the concept to match the annotations.

**** Unsupervised Learning

     - No annotations
     - Goal is to find interesting patterns in the examples
     - System defines what is interesting.
     - Example: grouping images by content.

**** Semi-Supervised Learning

     - "*normal learning*" is really a combination of the two
     - You do unsupervised learning, and you occasionally get your
       "parent"/oracle to come in and teach you some labels.
     - You use those new concepts to help you organize your thoughts better.

**** Active Learning

     - A few examples are annotated with the target concept.
     - Learning system can "ask" the oracle to label something.
     - There is a cost of labelling that the system must optimize.

**** Transductive Learning

     - Learning system has some knowledge of possible examples it will be
       evaluated on.
     - Adjusts the system to do better on those examples.
     - EG - learn to play chess against Kasparov.

**** Reinforcement Learning

     - This is "sequential" learning.
     - Your environment provides feedback.
     - You take actions and use the consequences to learn.

**** Transfer Learning

     - Human learning is cumulative.
       - When we encounter a new problem, we don't just start from scratch.
       - We use prior knowledge and reasoning.
     - Transfer learning attempts to apply concepts learned in other problems to
       bias your search.

** When to use ML?

   - Shouldn't use ML to recognize geometric shapes.
   - In general, you don't need to learn if you have these things:
     - The concept is already accurately known.
     - It can be easily (and compactly) described
     - Unlikely to change
   - Learning is not free, requires computation and storage, and real world
     effort in labeling, etc.

** Example Representations

   - Internal representation of examples effects how you learn.
   - EG: When you recognize objects, you don't do it at the level of signals on
     your optic nerve.  You do it at the level of smaller parts that you've
     learned.  A chair has four legs, a flat surface, and usually a back.
   - In the same way, pixels aren't useful in object recognition.
   - This is an open area of research: we don't always know the best
     representation of examples.

*** Feature Vector Representation

    - Examples are vectors of values for a set of attributes.
    - Can be an n-by-m matrix

      |      | Attr 1 | Attr 2 | Attr 3 |
      | EG 1 | v_11    | v_12    | v_13    |
      | EG 2 | V_21    | V_22    | v_23    |
      | EG 3 | v_31    | v_32    | v_33    |

    - This is also called "propositional representation", because each example
      can be a logical conjunction.
    - Can represent all the examples as logic formula.

*** Relational Representation
    - Can use first order logic.

*** Multiple Instance Representation
    - Examples are represented by arbitrary sized sets of attribute-value pairs.
* 2015-08-27 Thursday

** Optimization

*** What is it?

    Find the extreme points of an objective function.

*** Types of Optimization Problems

    - Discrete vs Continuous - objective function is defined on discrete or
      continuous space.
    - Unconstrained vs constrained - whether there are additional constraints
      defining the feasible region.
    - In this class, we are interested in continuous problems, constrained and
      unconstrained.  We use tools from calculus and linear algebra.

*** Unconstrained Optimization

    - Function of one variable, eg minimum of x^2.  Typical method for solving
      this is to compute first and second derivative, find zeros of first
      derivative where second derivative is positive.
    - Fuctions of two variables, you find the same things, but in matrix form:
      - Jacobian \(J = (\frac{\delta{}f}{\delta{}x_i}) = 0\)
      - Hessian \(H = [\frac{\delta^{2}f}{\delta{}x_{i}\delta{}x_j}] > 0\) must be
        positive definite.
    - Can't always do this, due to computational constrains, and due to weird or
      unknown function.

*** Gradient Ascent

    A way of maximizing/minimizing a function.  From your current position
    $\vec{x}$, go in the direction that maximizes the increase.

    \(\vec{x}_{new} = \vec{x}_{old} - \alpha \Delta f_{\vec{x}_old}(\vec{x})\)

    Here, \alpha is the step size, and \Delta f is the function gradient
    evaluated at x_{old}.

    Downside of this is that the convergence rate is not very good.  Also, this
    procedure assumes linearity, where a quadratic function may be a better
    approximation.

*** Newton-Raphson Method

    In this, we use a quadratic approximation of f.  Then, instead of taking a
    linear step, we take a "Newton step".

    \(f(\vec{x}_{old} + u) = f(\vec{x}_{old}) + u^T \Delta f_{\vec{x}_{old}}(\vec{x}) + \frac{1}{2} u^T \Delta^2f_{\vec{x}_{old}}(\vec{x})u = g(u)\)

    More math, see slides.

    Properties:
    - Fast convergence close to solution.
    - Not guaranteed to converge if started far from solution, may cycle or
      diverge in this case.

*** Quasi-Newton Methods

    - Often, constructing the Hessian for a multivariate function is
      computationally difficult, because it takes O(n^2) space and time and has
      to be done over and over.
    - So, a number of methods exist that approximate the Hessian by using the
      Jacobian at nearby points.

*** Local and Global Optima

    - A *global minimum* for a function is a point x where f(x) \leq f(x+u) for
      all u.
    - A *local minimum* is an x where f(x) \leq f(x+u) for all |u|<\epsilon, for
      some positive \epsilon.
    - Every global minimum is a local min, but not the other way around.
    - There is no algorithm that is guaranteed to find the global maximum of an
      arbitrary function.

*** Convex Sets

    Take two points x_1 and x_2.  A point on the line segment between them is
    defined by \lambda x_1 + (1-\lambda) x_2, for 0 \leq \lambda \leq 1.

    A Convex Set is a set of points such that for any two points in the set,
    \lambda x_1 + (1-\lambda) x_2 is also in the set (for 0 \leq \lambda \leq
    1).  Basically, you can visualize these sets on the plane as "shapes that
    don't have holes in them".

*** Convex Functions

    If you look at all the points that are "above" a function - {(x,y)|y \geq
    f(x)}, if that set is convex, then f is a convex function.

    JENSEN'S INEQUALITY (yaaaaaaas)!

    f(\lambda x_1 + (1-\lambda) x_2) \leq \lambda f(x_1) + (1-\lambda) f(x_2)

    Jensen's inequality seems to apply for any convex function.  It just says
    that the points on the segment between f(x_1) and f(x_2) have to be above
    the the function itself.  Pretty cool.

    For a convex function, every local optimum is also a global optimum!  That's
    a pretty nice property to have.

*** Constrained Optimization

    - Minimize a function of x such that some constraints on x are satisfied.
      The constraints define a feasible region on of in which the solution must
      lie.

*** Linear Programming

    Linear Programming is a *special case* of *constrained optimization*, in
    which both the objective function and the constraints are linear!
    Typically, we write all the constraints and objective function as functions
    of matrices and vectors, for compactness.

    When you apply all these linear constraints, you have a feasible region that
    is a "polyhedron" (because it is bounded by a bunch of "hyperplanes").  It's
    possible that one side of the feasible region is open, (so not completely
    bounded).

    If you have a linear objective function, you can say for certain that an
    optimal point is on one of the vertices.

*** Simplex Algorithm

    - Around the polyhedron we go.
    - From any feasible vertex, walk along the edges of the polyhedron,
      following the vertices.
    - Once you are at a vertex where the neighboring vertices have higher f
      values, stop.
    - You've found a local optimum, which happens to be a global optimum since
      the linear function is convex.

    Properties of this algorithm:

    - Very simple, and easy to implement, and works well in practice.
    - It works by traversing vertices, and there may be exponentially many
      vertices for n constraints.  So, in the worst case, runtime is
      exponential.
      - Average case under various distributions has been shown to be
        polynomial, which is useful.
    - Other algorithms exist, such as "interior point methods", which have
      polynomial bounds*

*** Duality in Linear Programming

    From any "primal" LP, we can derive a "dual" LP.  Say we have a primal LP:

    - min_x c^T x, such that
    - A x \geq b
    - x \geq 0

    We could create a dual like this:

    - max_u b^T u, such that
    - A^T u \leq c
    - u \geq 0

    The nice properties of this are:

    - The primal has a solution iff the dual has a solution.
    - Further, the dual LP is a lower bound on the primal LP.
      - That is, if we pick any feasible x and any feasible u, we always havve
        c^T x \geq b^T u.
    - From the relationship between primal and dual LPs, we can derive a set of
      conditions that characterize the solutions for a primal/dual pair, called
      the Karush-Kuhn-Tucker conditions.
    - Essentially, the conditions are that at the optimal solution, x and u are
      feasible and the objective functions c^T x and b^T u are equal (and some
      other stuff).
    - Soumya says if this doesn't make sense now, that's ok.  Which is good,
      because he lost me at the dual being a lower bound on the primal.

*** Summary of Optimization

    - Types of optimization problems.
    - Unconstrained optimization - gradient ascent/descent, Newton Raphson
      methods.
    - Convex sets and functions
    - Constrained optimization:
      - Linear programming
      - Simplex method
      - Duality
      - KKT conditions

** The Simplex Algorithm

   He says we should know how it works.

   Let us consider the following linear program:

   - minimize (with respect to x_1, x_2) f(x) = 3x_1 - 6x_2, such that
   - x_1 + 2x_2 \geq -1
   - 2x_1 + x_2 \geq 0
   - -x_2 + x_1 \geq -1
   - -4x_2 + x_1 \geq -15
   - -4x_1 + x_2 \geq -23
   - x1, x_2 \geq 0

   Steps:
   1. Standardize so everything is in [variables] \geq [constant] form.
   2. Introduce "slack variables".  Essentially, these are the gap in the
      conditions.  These have to be greater than or equal to 0:
      1. x_3 = x_1 + 2x_2 + 1
      2. x_4 = 2x_1 + x_2
      3. x_5 = -x2 + x_1 + 1
      4. x_6 = -4x_2 + x_1 + 15
      5. x_7 = -4x_1 + x_2 + 23
   3. We can put this stuff into tableu form:

      |     | x_1 | x_2 |    |
      | x_3 |   1 |   2 |  1 |
      | x_4 |   2 |   1 |  0 |
      | x_5 |   1 |  -1 |  1 |
      | x_6 |   1 |  -4 | 13 |
      | x_7 |  -4 |   1 | 23 |
      | 2   |   3 |  -6 |  0 |

   4. Assume that zero is feasible.  Pick the variable that will decrease the
      objective function (the most?), and change it accordingly.  In this case,
      we choose x_2.  Then, we write out the constraints, holding x_1 to be 0.
      We find the smallest positive constraint value for x_2, and choose that.
      Whatever variable caused that constraint, we swap it with x_2, and make a
      new tableau.

      In this case, x_5 is the blocking constraint, so we pick it.

      |     | x_1 | x_5 |  1 |
      | x_3 |   3 |  -2 |  3 |
      | x_4 |   3 |  -1 |  1 |
      | x_2 |   1 |  -1 |  1 |
      | x_6 |  -3 |   4 |  9 |
      | x_7 |  -3 |   1 | 24 |
      | z   |  -3 |   6 | -6 |

   5. The value of the function is now -6.  We can see that the right variable
      to decrease now is x_1.  So, we do the constraints again.  Here, the
      blocking constraint is x_6, so then we get this tableau:

      |     |  x_6 | x_5 |   1 |
      | x_3 |   -1 |   2 |  12 |
      | x_4 |   -1 |   3 |  10 |
      | x_2 |  1/3 | 1/3 |   4 |
      | x_1 | -1/3 | 1/3 |   3 |
      | x_7 |    1 |  -5 |  15 |
      | z   |    2 |   1 | -15 |

      The stopping condition is when both variables on top of the columns have
      coefficients that are positive, so you can't improve the function value.

   If you have more than one variable that will decrease the function, you can
   choose any variable to decrease, and you will always get to the correct
   solution.  However, some choices will be faster than others.
